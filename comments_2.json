[
    {
        "project": "apm-agent-java",
        "id": 1167628766,
        "body": "\n## :broken_heart: Build Failed\n\n\n<!-- BUILD BADGES-->\n> _the below badges are clickable and redirect to their specific view in the CI or DOCS_\n[![Pipeline View](https://img.shields.io/badge/pipeline-pipeline%20-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2691/detail/PR-2691/3//pipeline) [![Test View](https://img.shields.io/badge/test-test-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2691/detail/PR-2691/3//tests) [![Changes](https://img.shields.io/badge/changes-changes-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2691/detail/PR-2691/3//changes) [![Artifacts](https://img.shields.io/badge/artifacts-artifacts-yellow)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2691/detail/PR-2691/3//artifacts) [![preview](https://img.shields.io/badge/docs-preview-yellowgreen)](http://apm-agent-java_2691.docs-preview.app.elstc.co/diff)  [![preview](https://img.shields.io/badge/elastic-observability-blue)](https://ci-stats.elastic.co/app/apm/services/apm-ci/transactions/view?rangeFrom=2022-06-28T11:29:16.167Z&rangeTo=2022-06-28T11:49:16.167Z&transactionName=BUILD+apm-agent-java%2Fapm-agent-java-mbp%2FPR-%7Bnumber%7D&transactionType=job&latencyAggregationType=avg&traceId=e2d32f73d856e860b708e10bf9fb2dd3&transactionId=ce14be93a5c3bb4a)\n\n<!-- BUILD SUMMARY-->\n<details><summary>Expand to view the summary</summary>\n<p>\n\n#### Build stats\n\n\n\n\n* Start Time: 2022-06-28T11:39:16.167+0000\n\n\n* Duration: 9 min 46 sec\n\n\n\n\n\n</p>\n</details>\n\n<!-- TEST RESULTS IF ANY-->\n\n\n<!-- STEPS ERRORS IF ANY -->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Steps errors [![6](https://img.shields.io/badge/6%20-red)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2691/detail/PR-2691/3//pipeline)\n  <details><summary>Expand to view the steps failures</summary>\n  <p>\n\n\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 2 min 43 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2691/runs/3/steps/145/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 1 min 46 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2691/runs/3/steps/149/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 1 min 47 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2691/runs/3/steps/153/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 1 min 47 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2691/runs/3/steps/157/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 1 min 46 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2691/runs/3/steps/161/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `Error signal`\n<ul>\n<li>Took 0 min 0 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2691/runs/3/steps/92/log/?start=0\">here</a></li>\n<li>Description: <code>githubApiCall: The REST API call https://api.github.com/orgs/elastic/members/dependabot[bot] return the message : java.lang.Exception: httpRequest: Failure connecting to the service https://api.github.com/orgs/elastic/members/dependabot[bot] : httpRequest: Failure connecting to the service https://api.github.com/orgs/elastic/members/dependabot[bot] : Code: 404Error: {\"message\":\"User does not exist or is not a member of the organization\",\"documentation_url\":\"https://docs.github.com/rest/reference/orgs#check-organization-membership-for-a-user\"}</code></l1>\n</ul>\n  \n  </p>\n  </details>\n\n\n\n## :grey_exclamation: Flaky test report\nNo test was executed to be analysed.\n\n\n\n## :robot: GitHub comments\n\nTo re-run your PR in the CI, just comment with:\n\n\n\n- `/test` : Re-trigger the build.\n\n\n- `run benchmark tests` : Run the benchmark tests.\n\n\n- `run jdk compatibility tests` : Run the JDK Compatibility tests.\n\n\n- `run integration tests` : Run the Agent Integration tests.\n\n\n- `run end-to-end tests` : Run the APM-ITs.\n\n\n- `run windows tests` : Build & tests on windows.\n\n\n- `run` `elasticsearch-ci/docs` : Re-trigger the docs validation. (use unformatted text in the comment!)\n\n\n\n\n<!--COMMENT_GENERATED_WITH_ID_comment.id-->",
        "url": "https://github.com/elastic/apm-agent-java/pull/2691#issuecomment-1167628766"
    },
    {
        "project": "pulsar",
        "id": 1175880638,
        "body": "I have a question:\r\nwe use pulsar-2.8.0 in pro env, but this pr was merged into master, could you please merge this pr into pulsar-2.8.0's new tag, then I can use this new tag to replace our pro env pulsar-2.8.0 directly.",
        "url": "https://github.com/apache/pulsar/pull/16313#issuecomment-1175880638"
    },
    {
        "project": "hadoop",
        "id": 775367808,
        "body": "https://github.com/apache/hadoop/pull/2688 Seems the build failed, maybe because I uploaded the diff before your merge thus the base is different.  As of now, I don't see your merge of HADOOP-16767. I will rebase to the latest 2.10 branch once your merge shows up and recreate the PR. Thanks Steve.\r\n",
        "url": "https://github.com/apache/hadoop/pull/2685#issuecomment-775367808"
    },
    {
        "project": "hadoop",
        "id": 1207149152,
        "body": ":broken_heart: **-1 overall**\r\n\r\n\r\n\r\n\r\n\r\n\r\n| Vote | Subsystem | Runtime |  Logfile | Comment |\r\n|:----:|----------:|--------:|:--------:|:-------:|\r\n| +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n|||| _ Prechecks _ |\r\n| +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n| +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n| +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n| +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n|||| _ trunk Compile Tests _ |\r\n| +0 :ok: |  mvndep  |  15m 25s |  |  Maven dependency ordering for branch  |\r\n| +1 :green_heart: |  mvninstall  |  25m 49s |  |  trunk passed  |\r\n| +1 :green_heart: |  compile  |   4m  1s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  compile  |   3m 29s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  checkstyle  |   1m 32s |  |  trunk passed  |\r\n| +1 :green_heart: |  mvnsite  |   2m 19s |  |  trunk passed  |\r\n| +1 :green_heart: |  javadoc  |   2m  6s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 56s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 58s |  |  trunk passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n|||| _ Patch Compile Tests _ |\r\n| +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n| +1 :green_heart: |  mvninstall  |   1m 40s |  |  the patch passed  |\r\n| +1 :green_heart: |  compile  |   3m 49s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  cc  |   3m 49s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 49s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 generated 3 new + 446 unchanged - 0 fixed = 449 total (was 446)  |\r\n| +1 :green_heart: |  compile  |   3m 16s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  cc  |   3m 16s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 16s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 generated 3 new + 370 unchanged - 0 fixed = 373 total (was 370)  |\r\n| +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n| +1 :green_heart: |  checkstyle  |   1m 11s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server: The patch generated 0 new + 11 unchanged - 1 fixed = 11 total (was 12)  |\r\n| +1 :green_heart: |  mvnsite  |   1m 49s |  |  the patch passed  |\r\n| +1 :green_heart: |  javadoc  |   1m 31s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 27s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 43s |  |  the patch passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n|||| _ Other Tests _ |\r\n| +1 :green_heart: |  unit  |   3m  5s |  |  hadoop-yarn-server-common in the patch passed.  |\r\n| +1 :green_heart: |  unit  | 102m 57s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n| +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n|  |   | 231m 15s |  |  |\r\n\r\n\r\n| Subsystem | Report/Notes |\r\n|----------:|:-------------|\r\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/Dockerfile |\r\n| GITHUB PR | https://github.com/apache/hadoop/pull/4711 |\r\n| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat |\r\n| uname | Linux 0fd97407eebb 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | dev-support/bin/hadoop.sh |\r\n| git revision | trunk / 8e63615447d74945f2ec2f11287f1b5f2867a29c |\r\n| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n|  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/testReport/ |\r\n| Max. process+thread count | 949 (vs. ulimit of 5500) |\r\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server |\r\n| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/console |\r\n| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n| Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/4711#issuecomment-1207149152"
    },
    {
        "project": "spring-framework",
        "id": 1183308182,
        "body": "This has been merged into `5.3.x` and `main`.\r\n\r\nThanks for all of your effort recently in cleaning up typos! \ud83d\udc4d ",
        "url": "https://github.com/spring-projects/spring-framework/pull/28807#issuecomment-1183308182"
    },
    {
        "project": "spring-boot",
        "id": 360600978,
        "body": "@rajadilipkolli @philwebb thank you for attention to this issues! But this is false positive static code analysis warning.  Possible resource leak already fixed in [PR 11624](https://github.com/spring-projects/spring-boot/pull/11624) \r\n\r\nNeed to redesign this classes to avoid this sonarcloud warnings.\r\n\r\nLog4J2LoggingSystem.java is not applicable - stream should be in open state after return of ConfigurationSource \r\n\r\nJarWriter.java - output stream created in construnctor and used in another object methods\r\n\r\nSocketTargetServerConnection.java - network channel should be in open state after method return",
        "url": "https://github.com/spring-projects/spring-boot/pull/11779#issuecomment-360600978"
    },
    {
        "project": "flink",
        "id": 1196291153,
        "body": "@tweise Thank you so much for reviewing the PR! I just realized that I might have misread the JIRA issue as `HybridSource: Support dynamic stop position in HybridSource` instead of `HybridSource: Support dynamic stop position in FileSource`. So this PR actually aimed to design an _**generic interface to allow any sources to participate in dynamic source switch**_. With that in mind. Let me explain how I came up with the current design & implementation.\r\n\r\nAfter reviewing the current logic of the hybrid source (it's amazing work \ud83c\udf89 \ud83c\udf89 !!), I understand that the current implementation support transferring the end position to the next enumerator. However, it lacks the mechanism to know where is the end position (i.e. offset for a kafka partition). And these \"end positions\" are probably unknown beforehand, or it would be the same as [fixed start position](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/hybridsource/#fixed-start-position-at-graph-construction-time)?\r\nTherefore, I think the key is to transfer the \"end position\" to the enumerator from source reader during source switch. \r\nThere are a few points to consider:\r\n\r\n1. What would be the \"end position\" to transfer to the next source? \r\n\r\nI believe this varied by use cases. Some may find it enough to use [`split.path`](https://github.com/apache/flink/blob/4d6c2df74f45815cb8f682f56b5d199a562dcbc9/flink-connectors/flink-connector-files/src/main/java/org/apache/flink/connector/file/src/FileSourceSplit.java#L236), while some may require to derive the timestamp or offset from the content of the file (i.e. kafka archive, and the implementation can vary by companies.). Since we can't anticipate all use cases, passing all finished splits seem to be a reasonable solution here and let the developer to decide how to derive the position from them.\r\n\r\n2. Where to make the changes?\r\n\r\nI aimed to implement this s.t. most existing sources can benefit from it out of the box with minimal changes and no breaking changes to them.\r\n\r\n\r\n3. How to store the \"finished splits\" before source switch?\r\n\r\nPer FLIP-27, the enumerator only knows what splits to consume but not the actual progress - only source reader knows about it. So we need to store them and transfer them to the enumerator during source switch. However, most existing sources implements `SourceReaderBase` and it [purges them from state](https://github.com/apache/flink/blob/3e2620bc785b3b4d82f1f188eb7b1e0e129b14d3/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/SourceReaderBase.java#L193-L194) once it reaches the end position. One naive solution would be to adjust `SourceReaderBase` to also store finished splits into the state. However, this'll affect all sources running in production and is probably a big backward incompatible changes right away. Therefore, I decided to store them only in the `HybridSourceReader`, and the existing sources only need to implement one method (`DynamicHybridSourceReader::getFinishedSplits`) that allows the finished splits to be extracted during checkpoint and source switch. This process is transparent to all existing sources, and only happens when used with the `HybridSource`.\r\n\r\nWith the above points, the current implementation works as follow:\r\n- On each checkpoint,  `HybridSourceReader`  retrieve the finished states (marked with  `HybridSourceSplit.isFinished = true`) from the underlying reader and checkpoint them for persistent along with the unfinished states.\r\n- Upon source switch,  `HybridSourceReader`  will send all the finished splits in  `SourceReaderFinishedEvent`  to the enumerator.\r\n- Enumerator will pass along those finished splits in  `SourceSwitchContext`  to the next source. And the next source can use the splits to derive the start positions.\r\n\r\n#### Changes required on existing non hybrid sources:\r\n- Implements `DynamicHybridSourceReader::getFinishedSplits` on `SourceReaderBase`.\r\n\r\n#### API changes on `HybridSource`\r\n- Added `SourceSwitchContext::getPreviousSplits` which returns finished splits from the previous source.\r\n\r\nIt's a lot of words, so really appreciate your patience for reading this. Let me know if there are anything unclear, I'm happy to chat more about this! (Also, I'm happy to start discussion thread on the dev mail list if you think it's necessary.)",
        "url": "https://github.com/apache/flink/pull/20289#issuecomment-1196291153"
    },
    {
        "project": "spring-boot",
        "id": 1203405321,
        "body": "thanks @philwebb , \r\nIn my project,  I try to use the `ApplicationPreparedEvent ` to do some initialization work, but it seems not to prevent the web server start. And `ApplicationContextInitializedEvent` is not invoked.\r\n\r\n<img width=\"637\" alt=\"image\" src=\"https://user-images.githubusercontent.com/16216770/182509235-f30db7d1-32be-4f78-ac01-7094a25bd09c.png\">\r\n\r\n\r\n<img width=\"642\" alt=\"image\" src=\"https://user-images.githubusercontent.com/16216770/182509200-dd3d5103-5e44-4f53-b267-134bec299c47.png\">\r\n\r\n\r\n> it should extends ServletWebServerInitializedEvent since that even is for when a web server is ready.\r\n\r\nThis point is very correct, I will reconsider this question about the event.\r\n\r\nThanks for your replay\r\n",
        "url": "https://github.com/spring-projects/spring-boot/pull/31958#issuecomment-1203405321"
    },
    {
        "project": "druid",
        "id": 1195506326,
        "body": "Travis didn't trigger for some reason. I closed and re-opened the PR. changes LGTM. I will merge once CI passes. ",
        "url": "https://github.com/apache/druid/pull/12819#issuecomment-1195506326"
    },
    {
        "project": "spring-framework",
        "id": 33982138,
        "body": "@philwebb Update SPR please, I recreated the pull request to fix the target branch (Should be easier to merge now), it is the same branch, just fixed the pull request target branch to be 3.2.x\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/454#issuecomment-33982138"
    },
    {
        "project": "spring-security",
        "id": 994998140,
        "body": "> Thanks, @Drezir, for the updates!\r\n> \r\n> In addition to my inline feedback, will you please add some unit tests to `WebSecurityTests` for the new method and to a class called `CompositeRequestRejectedHandlerTests` for the new class? Remember to place those tests in their respective commits.\r\n\r\nI have added some tests, thanks to that I have discovered one problem so the tests have proven that they are useful :+1:\r\nI did not want to squash commits during code review, would you like me to do it and organize it like 1st commit for CompositeHandler and  2nd commit for WebSecurity setter?",
        "url": "https://github.com/spring-projects/spring-security/pull/10603#issuecomment-994998140"
    },
    {
        "project": "druid",
        "id": 953768873,
        "body": "The format of the Subquery ID has been changed in the later commits. Subquery ID is divided into 3 parts separated by `-`. (not all parts maybe present for a given subquery).\r\n1. First part indicates the sequence in which the subquery is run (in the collection of subqueries generated by the master query).\r\n2. The second part indicates the nesting level of that subquery datasource in the structure of original datasource. The nesting structure represents a hierarchy delimited by `.`.\r\n3. The third part is present if the subqueries are generated as a response to a union query, and depicts the original datasource and it's sequence number in the union subqueries.\r\n\r\nExample queries: \r\n```\r\nGroupByQuery{                                           \r\n  dataSource='JoinDataSource{                         \r\n    left=UnionDataSource{dataSources=[                  \r\n      foo,                                            \r\n      bar]},                                      \r\n    right=GroupByQuery{                     \r\n      dataSource='UnionDataSource{dataSources=[       \r\n        foo,                                            \r\n        bar]}                                         \r\n  }\r\n}     \r\n```\r\nThis will generate 4 subqueries. The one generated due to inlining of inner `GroupByQuery` would have sequence number as 1 (first of the subqueries which is run), the nesting level as `1.2` (nesting level of the subquery inside the parent datasource), and the third part a foo.1 and bar.2 respectively. Therefore the subqueries thus generated would have ids: `1-1.2-foo.1` and `1-1.2-bar.2`\r\nThe original query run would have the subquery ids of `foo.1` and `bar.2` (denoting the union datasources) since they don't contain the first and the second part respectively. ",
        "url": "https://github.com/apache/druid/pull/11809#issuecomment-953768873"
    },
    {
        "project": "hadoop",
        "id": 1006605173,
        "body": "## dependencies\r\n\r\ndependencies look good; nothing accidentally referred to.\r\n```\r\n[INFO] +- org.apache.hadoop:hadoop-common:test-jar:tests:3.4.0-SNAPSHOT:test\r\n[INFO] +- com.amazonaws:aws-java-sdk-bundle:jar:1.12.132:compile\r\n[INFO] +- org.assertj:assertj-core:jar:3.12.2:test\r\n```\r\n\r\nsize is now 264 MB, up from 226. that is huge, but it allows us to dodge all classpath problems we used to get about version incompatibilities of things like json parsing, http client etc.\r\n\r\n\r\n## Test run with `-Dparallel-tests -DtestsThreadCount=6  -Dmarkers=keep -Dscale`\r\n\r\n ITestS3AInputStreamPerformance output shows a bug in the throughput calculation; no new errors from the SDK though.\r\n```\r\n2022-01-05 17:32:29,352[JUnit-testReadWithNormalPolicy]INFO scale.ITestS3AInputStreamPerformance(ITestS3AInputStreamPerformance.java:executeSeekReadSequence(437))-Effective bandwidth-3.478002MB/S\r\n```\r\n\r\n## Test run with `-Dparallel-tests -DtestsThreadCount=6  -Dmarkers=delete -Ddynamo -Ds3guard`\r\n```\r\n[ERROR] Failures: \r\n[ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:268->Assert.assertTrue:42->Assert.fail:89 treewalk vs listFiles(/, true) mismatch: between \r\n  \"s3a://stevel-london/file.txt\"\r\n] and \r\n  \"s3a://stevel-london/dir-no-trailing/file2\"\r\n  \"s3a://stevel-london/file.txt\"\r\n  \"s3a://stevel-london/p1/p2/file\"\r\n]\r\n[INFO] \r\n[ERROR] Tests run: 151, Failures: 1, Errors: 0, Skipped: 82\r\n```\r\n## Test run with `-Dparallel-tests -DtestsThreadCount=6  -Dmarkers=delete -Ddynamo -Ds3guard -Dscale`\r\n\r\nThis time `ITestS3AContractRootDir` was happy.\r\n\r\n##  manual command line tests\r\n\r\nAll executed against AWS London with no problems in any of the commands. as S3Guard was disabled, I skipped some of the commands.\r\n\r\n```\r\n~/P/R/hadoop-3.4.0-SNAPSHOT bin/hadoop s3guard bucket-info -markers aware $BUCKET\r\n2022-01-06 13:37:48,612 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.s3a.server-side-encryption.key is deprecated. Instead, use fs.s3a.encryption.key\r\n2022-01-06 13:37:48,615 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.s3a.server-side-encryption-algorithm is deprecated. Instead, use fs.s3a.encryption.algorithm\r\n2022-01-06 13:37:49,759 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy(189)) - Directory markers will be kept\r\nFilesystem s3a://stevel-london\r\nLocation: eu-west-2\r\nFilesystem s3a://stevel-london is not using S3Guard\r\n\r\nS3A Client\r\n        Signing Algorithm: fs.s3a.signing-algorithm=(unset)\r\n        Endpoint: fs.s3a.endpoint=(unset)\r\n        Encryption: fs.s3a.encryption.algorithm=SSE-KMS\r\n        Input seek policy: fs.s3a.experimental.input.fadvise=normal\r\n        Change Detection Source: fs.s3a.change.detection.source=etag\r\n        Change Detection Mode: fs.s3a.change.detection.mode=server\r\n\r\nS3A Committers\r\n        The \"magic\" committer is supported in the filesystem\r\n        S3A Committer factory class: mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\r\n        S3A Committer name: fs.s3a.committer.name=magic\r\n        Store magic committer integration: fs.s3a.committer.magic.enabled=true\r\n\r\nSecurity\r\n        Delegation token support is disabled\r\n\r\nSecurity\r\n        The directory marker policy is \"keep\"\r\n        Available Policies: delete, keep, authoritative\r\n        Authoritative paths: fs.s3a.authoritative.path=\r\n        The S3A connector is compatible with buckets where directory markers are not deleted\r\n2022-01-06 13:37:50,622 [main] INFO  statistics.IOStatisticsLogging (IOStatisticsLogging.java:logIOStatisticsAtLevel(269)) - IOStatistics: counters=((audit_request_execution=1)\r\n(audit_span_creation=2)\r\n(store_exists_probe=1)\r\n(store_io_request=2));\r\n\r\ngauges=();\r\n\r\nminimums=((store_exists_probe.min=842));\r\n\r\nmaximums=((store_exists_probe.max=842));\r\n\r\nmeans=((store_exists_probe.mean=(samples=1, sum=842, mean=842.0000)));\r\n\r\n```\r\n\r\nI'm using the old properties for encryption because I still want tests against older builds to also be encrypted. Maybe I should set both properties just to get rid of the warning.\r\n\r\nand a storediag command\r\n\r\n```\r\n bin/hadoop jar $CLOUDSTORE storediag $BUCKET \r\n\r\nStore Diagnostics for stevel (auth:SIMPLE) on stevel-mbp1376/127.0.0.1\r\n======================================================================\r\n\r\n\r\nDiagnostics for filesystem s3a://stevel-london/\r\n===============================================\r\n\r\nS3A FileSystem Connector\r\nASF Filesystem Connector to Amazon S3 Storage and compatible stores\r\nhttps://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html\r\n\r\nHadoop information\r\n==================\r\n\r\n  Hadoop 3.4.0-SNAPSHOT\r\n  Compiled by stevel on 2022-01-05T18:07Z\r\n  Compiled with protoc 3.7.1\r\n  From source with checksum e249b256a31f772e4220f92668721349\r\n\r\nDetermining OS version\r\n======================\r\n\r\nDarwin stevel-mbp1376 21.2.0 Darwin Kernel Version 21.2.0: Sun Nov 28 20:28:54 PST 2021; root:xnu-8019.61.5~1/RELEASE_X86_64 x86_64\r\n\r\nSelected System Properties\r\n==========================\r\n\r\naws.accessKeyId = (unset)\r\naws.secretKey = (unset)\r\naws.sessionToken = (unset)\r\naws.region = (unset)\r\ncom.amazonaws.regions.RegionUtils.fileOverride = (unset)\r\ncom.amazonaws.regions.RegionUtils.disableRemote = (unset)\r\ncom.amazonaws.sdk.disableCertChecking = (unset)\r\ncom.amazonaws.sdk.ec2MetadataServiceEndpointOverride = (unset)\r\ncom.amazonaws.sdk.enableDefaultMetrics = (unset)\r\ncom.amazonaws.sdk.enableInRegionOptimizedMode = (unset)\r\ncom.amazonaws.sdk.enableThrottledRetry = (unset)\r\ncom.amazonaws.services.s3.disableImplicitGlobalClients = (unset)\r\ncom.amazonaws.services.s3.enableV4 = (unset)\r\ncom.amazonaws.services.s3.enforceV4 = (unset)\r\n\r\nEnvironment Variables\r\n=====================\r\n\r\nAWS_ACCESS_KEY_ID = (unset)\r\nAWS_ACCESS_KEY = (unset)\r\nAWS_SECRET_KEY = (unset)\r\nAWS_SECRET_ACCESS_KEY = (unset)\r\nAWS_SESSION_TOKEN = (unset)\r\nAWS_REGION = (unset)\r\nAWS_S3_US_EAST_1_REGIONAL_ENDPOINT = (unset)\r\nAWS_CBOR_DISABLE = (unset)\r\nAWS_CONTAINER_CREDENTIALS_RELATIVE_URI = (unset)\r\nAWS_CONTAINER_CREDENTIALS_FULL_URI = (unset)\r\nAWS_CONTAINER_AUTHORIZATION_TOKEN = (unset)\r\nAWS_EC2_METADATA_DISABLED = \"true\"\r\nAWS_EC2_METADATA_SERVICE_ENDPOINT = (unset)\r\nAWS_MAX_ATTEMPTS = (unset)\r\nAWS_RETRY_MODE = (unset)\r\nHADOOP_CONF_DIR = \"/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/etc/hadoop\"\r\nHADOOP_CREDSTORE_PASSWORD = (unset)\r\nHADOOP_HEAPSIZE = (unset)\r\nHADOOP_HEAPSIZE_MIN = (unset)\r\nHADOOP_HOME = \"/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT\"\r\nHADOOP_LOG_DIR = (unset)\r\nHADOOP_OPTIONAL_TOOLS = \"hadoop-azure,hadoop-aws,hadoop-openstack\"\r\nHADOOP_OPTS = \"-Djava.net.preferIPv4Stack=true  -Dyarn.log.dir=/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT -Dyarn.root.logger=INFO,console -Dhadoop.log.dir=/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT -Dhadoop.id.str=stevel -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender\"\r\nHADOOP_SHELL_SCRIPT_DEBUG = (unset)\r\nHADOOP_TOKEN = (unset)\r\nHADOOP_TOKEN_FILE_LOCATION = (unset)\r\nHADOOP_TOOLS_HOME = (unset)\r\nHADOOP_TOOLS_OPTIONS = (unset)\r\nHADOOP_YARN_HOME = \"/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT\"\r\nHDP_VERSION = (unset)\r\nJAVA_HOME = \"/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home\"\r\nLOCAL_DIRS = (unset)\r\nOPENSSL_ROOT_DIR = \"/usr/local/opt/openssl/\"\r\nPYSPARK_DRIVER_PYTHON = (unset)\r\nSPARK_HOME = (unset)\r\nSPARK_CONF_DIR = (unset)\r\nSPARK_SCALA_VERSION = (unset)\r\nYARN_CONF_DIR = (unset)\r\n\r\nSecurity\r\n========\r\n\r\nSecurity Enabled: false\r\nKeytab login: false\r\nTicket login: false\r\nCurrent user: stevel (auth:SIMPLE)\r\nToken count: 0\r\n\r\nHadoop Options\r\n==============\r\n\r\nfs.defaultFS = \"file:///\" [core-default.xml]\r\n2022-01-06 13:41:51,986 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.default.name is deprecated. Instead, use fs.defaultFS\r\nfs.default.name = \"file:///\" \r\nfs.permissions.umask-mode = \"022\" [core-default.xml]\r\nfs.trash.classname = (unset)\r\nfs.trash.interval = \"0\" [core-default.xml]\r\nfs.trash.checkpoint.interval = \"0\" [core-default.xml]\r\nhadoop.tmp.dir = \"/tmp/hadoop-stevel\" [core-default.xml]\r\nhdp.version = (unset)\r\nyarn.resourcemanager.address = \"0.0.0.0:8032\" [yarn-default.xml]\r\nyarn.resourcemanager.principal = (unset)\r\nyarn.resourcemanager.webapp.address = \"0.0.0.0:8088\" [yarn-default.xml]\r\nyarn.resourcemanager.webapp.https.address = \"0.0.0.0:8090\" [yarn-default.xml]\r\nmapreduce.input.fileinputformat.list-status.num-threads = \"1\" [mapred-default.xml]\r\nmapreduce.jobtracker.kerberos.principal = (unset)\r\nmapreduce.job.hdfs-servers.token-renewal.exclude = (unset)\r\nmapreduce.application.framework.path = (unset)\r\nfs.iostatistics.logging.level = \"info\" [core-site.xml]\r\n\r\nSecurity Options\r\n================\r\n\r\ndfs.data.transfer.protection = (unset)\r\nhadoop.http.authentication.simple.anonymous.allowed = \"true\" [core-default.xml]\r\nhadoop.http.authentication.type = \"simple\" [core-default.xml]\r\nhadoop.kerberos.min.seconds.before.relogin = \"60\" [core-default.xml]\r\nhadoop.kerberos.keytab.login.autorenewal.enabled = \"false\" [core-default.xml]\r\nhadoop.security.authentication = \"simple\" [core-default.xml]\r\nhadoop.security.authorization = \"false\" [core-default.xml]\r\nhadoop.security.credential.provider.path = (unset)\r\nhadoop.security.credstore.java-keystore-provider.password-file = (unset)\r\nhadoop.security.credential.clear-text-fallback = \"true\" [core-default.xml]\r\nhadoop.security.key.provider.path = (unset)\r\nhadoop.security.crypto.jceks.key.serialfilter = (unset)\r\nhadoop.rpc.protection = \"authentication\" [core-default.xml]\r\nhadoop.tokens = (unset)\r\nhadoop.token.files = (unset)\r\n\r\nSelected Configuration Options\r\n==============================\r\n\r\n\r\nfs.s3a.session.token = (unset)\r\nfs.s3a.server-side-encryption-algorithm = \"SSE-KMS\" [fs.s3a.bucket.stevel-london.server-side-encryption-algorithm via [core-site.xml]]\r\nfs.s3a.server-side-encryption.key = \"ar*********************************************************************3bf6\" [75] [fs.s3a.bucket.stevel-london.server-side-encryption.key via [core-site.xml]]\r\nfs.s3a.encryption-algorithm = (unset)\r\nfs.s3a.encryption.key = (unset)\r\nfs.s3a.aws.credentials.provider = \"\r\n    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,\r\n    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,\r\n    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,\r\n    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider\r\n  \" [core-default.xml]\r\nfs.s3a.endpoint = (unset)\r\nfs.s3a.endpoint.region = (unset)\r\nfs.s3a.signing-algorithm = (unset)\r\nfs.s3a.acl.default = (unset)\r\nfs.s3a.attempts.maximum = \"20\" [core-default.xml]\r\nfs.s3a.authoritative.path = (unset)\r\nfs.s3a.block.size = \"32M\" [core-default.xml]\r\nfs.s3a.bucket.probe = \"0\" [core-site.xml]\r\nfs.s3a.buffer.dir = \"/tmp/hadoop-stevel/s3a\" [core-default.xml]\r\nfs.s3a.bulk.delete.page.size = (unset)\r\nfs.s3a.change.detection.source = \"etag\" [core-default.xml]\r\nfs.s3a.change.detection.mode = \"server\" [core-default.xml]\r\nfs.s3a.change.detection.version.required = \"true\" [core-default.xml]\r\nfs.s3a.connection.ssl.enabled = \"true\" [core-default.xml]\r\nfs.s3a.connection.maximum = \"256\" [core-site.xml]\r\nfs.s3a.connection.establish.timeout = \"5000\" [core-site.xml]\r\nfs.s3a.connection.request.timeout = \"5s\" [core-site.xml]\r\nfs.s3a.connection.timeout = \"5000\" [core-site.xml]\r\nfs.s3a.custom.signers = (unset)\r\nfs.s3a.directory.marker.retention = \"keep\" [fs.s3a.bucket.stevel-london.directory.marker.retention via [core-site.xml]]\r\nfs.s3a.downgrade.syncable.exceptions = \"false\" [core-site.xml]\r\nfs.s3a.etag.checksum.enabled = \"false\" [core-default.xml]\r\nfs.s3a.experimental.input.fadvise = (unset)\r\nfs.s3a.experimental.aws.s3.throttling = (unset)\r\nfs.s3a.experimental.optimized.directory.operations = (unset)\r\nfs.s3a.fast.buffer.size = (unset)\r\nfs.s3a.fast.upload.buffer = \"disk\" [core-default.xml]\r\nfs.s3a.fast.upload.active.blocks = \"8\" [core-site.xml]\r\nfs.s3a.impl.disable.cache = (unset)\r\nfs.s3a.list.version = \"2\" [core-default.xml]\r\nfs.s3a.max.total.tasks = \"128\" [core-site.xml]\r\nfs.s3a.multipart.size = \"32M\" [core-site.xml]\r\nfs.s3a.multiobjectdelete.enable = \"true\" [core-default.xml]\r\nfs.s3a.multipart.purge = \"false\" [core-site.xml]\r\nfs.s3a.multipart.purge.age = \"3600000\" [core-site.xml]\r\nfs.s3a.paging.maximum = \"5000\" [core-default.xml]\r\nfs.s3a.path.style.access = \"false\" [core-site.xml]\r\nfs.s3a.proxy.host = (unset)\r\nfs.s3a.proxy.port = (unset)\r\nfs.s3a.proxy.username = (unset)\r\nfs.s3a.proxy.password = (unset)\r\nfs.s3a.proxy.domain = (unset)\r\nfs.s3a.proxy.workstation = (unset)\r\nfs.s3a.rename.raises.exceptions = \"true\" [core-site.xml]\r\nfs.s3a.readahead.range = \"524288\" [core-site.xml]\r\nfs.s3a.retry.limit = \"7\" [core-default.xml]\r\nfs.s3a.retry.interval = \"500ms\" [core-default.xml]\r\nfs.s3a.retry.throttle.limit = \"20\" [core-default.xml]\r\nfs.s3a.retry.throttle.interval = \"100ms\" [core-default.xml]\r\nfs.s3a.ssl.channel.mode = \"default_jsse\" [core-default.xml]\r\nfs.s3a.s3.client.factory.impl = (unset)\r\nfs.s3a.threads.max = \"128\" [core-site.xml]\r\nfs.s3a.threads.keepalivetime = \"60\" [core-default.xml]\r\nfs.s3a.user.agent.prefix = (unset)\r\nfs.s3a.assumed.role.arn = \"arn:aws:iam::152813717728:role/stevel-assumed-role\" [core-site.xml]\r\nfs.s3a.assumed.role.sts.endpoint = \"sts.eu-west-2.amazonaws.com\" [core-site.xml]\r\nfs.s3a.assumed.role.sts.endpoint.region = \"eu-west-2\" [core-site.xml]\r\nfs.s3a.assumed.role.session.name = (unset)\r\nfs.s3a.assumed.role.session.duration = \"12h\" [core-site.xml]\r\nfs.s3a.assumed.role.credentials.provider = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\" [core-default.xml]\r\nfs.s3a.assumed.role.policy = (unset)\r\nfs.s3a.metadatastore.impl = \"org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore\" [core-default.xml]\r\nfs.s3a.metadatastore.authoritative = \"false\" [core-default.xml]\r\nfs.s3a.metadatastore.authoritative.dir.ttl = (unset)\r\nfs.s3a.metadatastore.fail.on.write.error = \"true\" [core-default.xml]\r\nfs.s3a.metadatastore.metadata.ttl = \"15m\" [core-default.xml]\r\nfs.s3a.s3guard.consistency.retry.interval = \"2s\" [core-default.xml]\r\nfs.s3a.s3guard.consistency.retry.limit = \"7\" [core-default.xml]\r\nfs.s3a.s3guard.ddb.table = (unset)\r\nfs.s3a.s3guard.ddb.region = \"eu-west-2\" [fs.s3a.bucket.stevel-london.s3guard.ddb.region via [core-site.xml]]\r\nfs.s3a.s3guard.ddb.background.sleep = \"25ms\" [core-default.xml]\r\nfs.s3a.s3guard.ddb.max.retries = \"9\" [core-default.xml]\r\nfs.s3a.s3guard.ddb.table.capacity.read = \"0\" [core-default.xml]\r\nfs.s3a.s3guard.ddb.table.capacity.write = \"0\" [core-default.xml]\r\nfs.s3a.s3guard.ddb.table.create = \"false\" [core-default.xml]\r\nfs.s3a.s3guard.ddb.throttle.retry.interval = \"100ms\" [core-default.xml]\r\nfs.s3a.s3guard.local.max_records = (unset)\r\nfs.s3a.s3guard.local.ttl = (unset)\r\nfs.s3a.committer.name = \"magic\" [core-site.xml]\r\nfs.s3a.committer.magic.enabled = \"true\" [core-default.xml]\r\nfs.s3a.committer.staging.abort.pending.uploads = (unset)\r\nfs.s3a.committer.staging.conflict-mode = \"append\" [core-default.xml]\r\nfs.s3a.committer.staging.tmp.path = \"tmp/staging\" [core-default.xml]\r\nfs.s3a.committer.threads = \"8\" [core-default.xml]\r\nfs.s3a.committer.staging.unique-filenames = \"false\" [core-site.xml]\r\nmapreduce.outputcommitter.factory.scheme.s3a = \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\" [mapred-default.xml]\r\nmapreduce.fileoutputcommitter.marksuccessfuljobs = (unset)\r\nfs.s3a.delegation.token.binding = (unset)\r\nfs.s3a.delegation.token.secondary.bindings = (unset)\r\nfs.s3a.audit.referrer.enabled = (unset)\r\nfs.s3a.audit.referrer.filter = (unset)\r\nfs.s3a.audit.reject.out.of.span.operations = \"true\" [core-site.xml]\r\nfs.s3a.audit.request.handlers = (unset)\r\nfs.s3a.audit.service.classname = (unset)\r\n\r\nRequired Classes\r\n================\r\n\r\nAll these classes must be on the classpath\r\n\r\nclass: org.apache.hadoop.fs.s3a.S3AFileSystem\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: com.amazonaws.services.s3.AmazonS3\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.132.jar\r\nclass: com.amazonaws.ClientConfiguration\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.132.jar\r\nclass: java.lang.System\r\n\r\nOptional Classes\r\n================\r\n\r\nThese classes are needed in some versions of Hadoop.\r\nAnd/or for optional features to work.\r\n\r\nclass: com.amazonaws.services.dynamodbv2.AmazonDynamoDB\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.132.jar\r\nclass: com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClient\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.132.jar\r\nclass: com.fasterxml.jackson.annotation.JacksonAnnotation\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/common/lib/jackson-annotations-2.13.0.jar\r\nclass: com.fasterxml.jackson.core.JsonParseException\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/common/lib/jackson-core-2.13.0.jar\r\nclass: com.fasterxml.jackson.databind.ObjectMapper\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/common/lib/jackson-databind-2.13.0.jar\r\nclass: org.joda.time.Interval\r\n       Not found on classpath: org.joda.time.Interval\r\nclass: org.apache.hadoop.fs.s3a.s3guard.S3Guard\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.Invoker\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: com.amazonaws.services.s3.model.SelectObjectContentRequest\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/aws-java-sdk-bundle-1.12.132.jar\r\nclass: org.apache.hadoop.fs.s3a.select.SelectInputStream\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.impl.RenameOperation\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.impl.NetworkBinding\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.impl.DirectoryPolicy\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.hadoop.fs.s3a.audit.AuditManagerS3A\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\nclass: org.apache.knox.gateway.cloud.idbroker.s3a.IDBDelegationTokenBinding\r\n       Not found on classpath: org.apache.knox.gateway.cloud.idbroker.s3a.IDBDelegationTokenBinding\r\nclass: org.wildfly.openssl.OpenSSLProvider\r\n       file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/wildfly-openssl-1.0.7.Final.jar\r\n\r\nAt least one optional class was missing -the filesystem client *may* still work\r\n\r\nS3A Config validation\r\n=====================\r\n\r\nBuffer configuration option fs.s3a.buffer.dir = /tmp/hadoop-stevel/s3a\r\nTemporary files created in /tmp/hadoop-stevel/s3a\r\n\r\nConfiguration options with prefix fs.s3a.ext.\r\n=============================================\r\n\r\n\r\nLocating implementation class for Filesystem scheme s3a://\r\n==========================================================\r\n\r\nFileSystem for s3a:// is: org.apache.hadoop.fs.s3a.S3AFileSystem\r\nLoaded from: file:/Users/stevel/Projects/Releases/hadoop-3.4.0-SNAPSHOT/share/hadoop/tools/lib/hadoop-aws-3.4.0-SNAPSHOT.jar\r\n\r\nEndpoints\r\n=========\r\n\r\nAttempting to list and connect to public service endpoints,\r\nwithout any authentication credentials. \r\nThis is just testing the reachability of the URLs.\r\nIf the request fails with any network error it is likely\r\nto be configuration problem with address, proxy, etc%n\r\nIf it is some authentication error, then don't worry so much%n-look for the results of the filesystem operations\r\n\r\nEndpoint: https://stevel-london.s3.amazonaws.com/\r\n=================================================\r\n\r\nCanonical hostname s3-w.eu-west-2.amazonaws.com\r\n  IP address 52.95.148.69\r\nProxy: none\r\n\r\nConnecting to https://stevel-london.s3.amazonaws.com/\r\n\r\nResponse: 403 : Forbidden\r\nHTTP response 403 from https://stevel-london.s3.amazonaws.com/: Forbidden\r\nUsing proxy: false \r\nTransfer-Encoding: chunked\r\nnull: HTTP/1.1 403 Forbidden\r\nServer: AmazonS3\r\nx-amz-request-id: 8YZH01Y5KCVP5TM9\r\nx-amz-id-2: 0XTVOvJ5Z7p1FiMmGomfymm3sqqNp7Qbf0n8dcshA8npKgA72d5M4mY90P1Quvv/+jGojQw7KcU=\r\nDate: Thu, 06 Jan 2022 13:41:52 GMT\r\nx-amz-bucket-region: eu-west-2\r\nContent-Type: application/xml\r\n\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>8YZH01Y5KCVP5TM9</RequestId><HostId>0XTVOvJ5Z7p1FiMmGomfymm3sqqNp7Qbf0n8dcshA8npKgA72d5M4mY90P1Quvv/+jGojQw7KcU=</HostId></Error>\r\n\r\n\r\nEndpoint: https://sts.eu-west-2.amazonaws.com/\r\n==============================================\r\n\r\nCanonical hostname 52.94.48.43\r\n  IP address 52.94.48.43\r\nProxy: none\r\n\r\nConnecting to https://sts.eu-west-2.amazonaws.com/\r\n\r\nResponse: 200 : OK\r\nHTTP response 200 from https://sts.eu-west-2.amazonaws.com/: OK\r\nUsing proxy: false \r\nTransfer-Encoding: chunked\r\nnull: HTTP/1.1 200 OK\r\nX-Cache: Miss from cloudfront\r\nServer: Server\r\nX-Content-Type-Options: nosniff\r\nX-Amz-Cf-Pop: IAD79-C1\r\nPermissions-Policy: interest-cohort=()\r\nConnection: keep-alive\r\nLast-Modified: Mon, 03 Jan 2022 20:29:37 GMT\r\nx-amz-rid: NFFCXAKR69RKGD9S6BWE\r\nDate: Thu, 06 Jan 2022 13:41:55 GMT\r\nVia: 1.1 39174a6a452e175e6e614ff396a4ca4f.cloudfront.net (CloudFront)\r\nX-Frame-Options: SAMEORIGIN\r\nStrict-Transport-Security: max-age=300\r\nX-Amz-Cf-Id: 0aTRgu2cx13Puh4L0m8tfbZ68KFe821nDGOxRGXpcu1VTyFiXE30DA==\r\nVary: accept-encoding,Content-Type,Accept-Encoding,X-Amzn-CDN-Cache,X-Amzn-AX-Treatment,User-Agent\r\nSet-Cookie: aws_lang=en; Domain=.amazon.com; Path=/,aws-priv=eyJ2IjoxLCJldSI6MSwic3QiOjB9; Version=1; Comment=\"Anonymous cookie for privacy regulations\"; Domain=.aws.amazon.com; Max-Age=31536000; Expires=Fri, 06-Jan-2023 13:41:55 GMT; Path=/\r\nx-amz-id-1: NFFCXAKR69RKGD9S6BWE\r\nX-XSS-Protection: 1; mode=block\r\nContent-Security-Policy-Report-Only: default-src *; connect-src *; font-src * data:; frame-src *; img-src * data:; media-src *; object-src *; script-src 'unsafe-eval' 'unsafe-inline' *; style-src 'unsafe-inline' *; report-uri https://prod-us-west-2.csp-report.marketing.aws.dev/submit\r\nContent-Type: text/html;charset=UTF-8\r\n\r\n<!doctype html>\r\n<html class=\"no-js aws-lng-en_US aws-with-target\" lang=\"en-US\" data-static-assets=\"https://a0.awsstatic.com\" data-js-version=\"1.0.413\" data-css-version=\"1.0.400\">\r\n <head> \r\n  <meta http-equiv=\"Content-Security-Policy\" content=\"default-src 'self' data: https://a0.awsstatic.com; connect-src 'self' https://112-tzm-766.mktoresp.com https://112-tzm-766.mktoutil.com https://a0.awsstatic.com https://a0.p.awsstatic.com https://a1.awsstatic.com https://amazonwebservices.d2.sc.omtrdc.net https://amazonwebservicesinc.tt.omtrdc.net https://api.regional-table.region-services.aws.a2z.com https://api.us-west-2.prod.pricing.aws.a2z.com https://aws.amazon.com https://aws.demdex.net https://b0.p.awsstatic.com https://c0.b0.p.awsstatic.com https://calculator.aws https://cm.everesttech.net https://d0.awsstatic.com https://d1.awsstatic.com https://d1fgizr415o1r6.cloudfront.net https://d3borx6sfvnesb.cloudfront.net https://dc.ads.linkedin.com https://dftu77xade0tc.cloudfront.net https://dpm.demdex.net https://fls-na\r\n\r\nWARNING: this unauthenticated operation was not rejected.\r\n This may mean the store is world-readable.\r\n Check this by pasting https://sts.eu-west-2.amazonaws.com/ into your browser\r\n\r\nTest filesystem s3a://stevel-london/\r\n====================================\r\n\r\nTrying some operations against the filesystem\r\nStarting with some read operations, then trying to write\r\n2022-01-06 13:41:55,180 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: Creating filesystem for s3a://stevel-london/\r\n2022-01-06 13:41:55,201 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.s3a.server-side-encryption.key is deprecated. Instead, use fs.s3a.encryption.key\r\n2022-01-06 13:41:55,201 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1459)) - fs.s3a.server-side-encryption-algorithm is deprecated. Instead, use fs.s3a.encryption.algorithm\r\n2022-01-06 13:41:55,983 [main] INFO  impl.DirectoryPolicyImpl (DirectoryPolicyImpl.java:getDirectoryPolicy(189)) - Directory markers will be kept\r\n2022-01-06 13:41:55,985 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - Creating filesystem for s3a://stevel-london/: duration 0:00:807\r\nS3AFileSystem{uri=s3a://stevel-london, workingDir=s3a://stevel-london/user/stevel, inputPolicy=normal, partSize=33554432, enableMultiObjectsDelete=true, maxKeys=5000, readAhead=524288, blockSize=33554432, multiPartThreshold=134217728, s3EncryptionAlgorithm='SSE_KMS', blockFactory=org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory@2db2cd5, auditManager=Service ActiveAuditManagerS3A in state ActiveAuditManagerS3A: STARTED, auditor=LoggingAuditor{ID='b09ad642-5323-4ebd-b67a-3870d6efbeed', headerEnabled=true, rejectOutOfSpan=true}}, metastore=NullMetadataStore, authoritativeStore=false, authoritativePath=[], useListV1=false, magicCommitter=true, boundedExecutor=BlockingThreadPoolExecutorService{SemaphoredDelegatingExecutor{permitCount=384, available=384, waiting=0}, activeCount=0}, unboundedExecutor=java.util.concurrent.ThreadPoolExecutor@70e659aa[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0], credentials=AWSCredentialProviderList[refcount= 1: [TemporaryAWSCredentialsProvider, SimpleAWSCredentialsProvider, EnvironmentVariableCredentialsProvider, org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider@615f972], delegation tokens=disabled, DirectoryMarkerRetention{policy='keep'}, instrumentation {S3AInstrumentation{}}, ClientSideEncryption=false}\r\nImplementation class class org.apache.hadoop.fs.s3a.S3AFileSystem\r\n\r\nPath Capabilities\r\n=================\r\n\r\nfs.capability.etags.available   true\r\nfs.capability.etags.preserved.in.rename false\r\nfs.capability.paths.checksums   false\r\nfs.capability.multipart.uploader        true\r\nfs.capability.outputstream.abortable    true\r\nfs.s3a.capability.magic.committer       true\r\nfs.s3a.capability.select.sql    true\r\nfs.s3a.capability.directory.marker.aware        true\r\nfs.s3a.capability.directory.marker.policy.keep  true\r\nfs.s3a.capability.directory.marker.policy.delete        false\r\nfs.s3a.capability.directory.marker.policy.authoritative false\r\nfs.s3a.capability.directory.marker.action.keep  true\r\nfs.s3a.capability.directory.marker.action.delete        false\r\n2022-01-06 13:41:55,993 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: GetFileStatus s3a://stevel-london/\r\nroot entry S3AFileStatus{path=s3a://stevel-london/; isDirectory=true; modification_time=0; access_time=0; owner=stevel; group=stevel; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=UNKNOWN eTag=null versionId=null\r\n2022-01-06 13:41:56,027 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - GetFileStatus s3a://stevel-london/: duration 0:00:034\r\n2022-01-06 13:41:56,027 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: First 25 entries of listStatus(s3a://stevel-london/)\r\ns3a://stevel-london/ : scanned 0 entries\r\n2022-01-06 13:41:56,601 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - First 25 entries of listStatus(s3a://stevel-london/): duration 0:00:574\r\n2022-01-06 13:41:56,602 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: First 25 entries of listFiles(s3a://stevel-london/)\r\nFiles listing provided by: FunctionRemoteIterator{FileStatusListingIterator[Object listing iterator against s3a://stevel-london/; listing count 1; isTruncated=false; counters=((object_continue_list_request=0) (object_continue_list_request.failures=0) (object_list_request.failures=0) (object_list_request=1));\r\ngauges=();\r\nminimums=((object_continue_list_request.min=-1) (object_list_request.failures.min=-1) (object_continue_list_request.failures.min=-1) (object_list_request.min=50));\r\nmaximums=((object_list_request.max=50) (object_continue_list_request.max=-1) (object_continue_list_request.failures.max=-1) (object_list_request.failures.max=-1));\r\nmeans=((object_continue_list_request.mean=(samples=0, sum=0, mean=0.0000)) (object_list_request.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_continue_list_request.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_list_request.mean=(samples=1, sum=50, mean=50.0000)));\r\n]}\r\n2022-01-06 13:41:56,663 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - First 25 entries of listFiles(s3a://stevel-london/): duration 0:00:061\r\n\r\nSecurity and Delegation Tokens\r\n==============================\r\n\r\nSecurity is disabled\r\nFilesystem s3a://stevel-london does not/is not configured to issue delegation tokens (at least while security is disabled)\r\n2022-01-06 13:41:56,663 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: probe for a directory which does not yet exist s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921\r\n2022-01-06 13:41:56,738 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - probe for a directory which does not yet exist s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921: duration 0:00:075\r\n\r\nFilesystem Write Operations\r\n===========================\r\n\r\n2022-01-06 13:41:56,738 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: creating a directory s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921\r\n2022-01-06 13:41:56,925 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - creating a directory s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921: duration 0:00:187\r\n2022-01-06 13:41:56,926 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: create directory s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921\r\n2022-01-06 13:41:56,994 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - create directory s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921: duration 0:00:067\r\n2022-01-06 13:41:56,994 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: probing path s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file\r\n2022-01-06 13:41:57,060 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - probing path s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file: duration 0:00:066\r\n2022-01-06 13:41:57,060 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: creating a file s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file\r\nOutput stream summary: FSDataOutputStream{wrappedStream=S3ABlockOutputStream{WriteOperationHelper {bucket=stevel-london}, blockSize=33554432 Statistics=counters=((object_multipart_aborted=0) (stream_write_total_time=0) (op_hsync=0) (stream_write_exceptions_completing_upload=0) (stream_write_total_data=7) (action_executor_acquired=1) (op_abort.failures=0) (stream_write_exceptions=0) (op_hflush=0) (op_abort=0) (action_executor_acquired.failures=0) (stream_write_queue_duration=0) (multipart_upload_completed=0) (stream_write_block_uploads=1) (object_multipart_aborted.failures=0) (multipart_upload_completed.failures=0) (stream_write_bytes=7));\r\ngauges=((stream_write_block_uploads_pending=1) (stream_write_block_uploads_data_pending=0));\r\nminimums=((object_multipart_aborted.failures.min=-1) (multipart_upload_completed.failures.min=-1) (op_abort.min=-1) (action_executor_acquired.min=0) (op_abort.failures.min=-1) (action_executor_acquired.failures.min=-1) (object_multipart_aborted.min=-1) (multipart_upload_completed.min=-1));\r\nmaximums=((object_multipart_aborted.max=-1) (multipart_upload_completed.failures.max=-1) (op_abort.max=-1) (action_executor_acquired.failures.max=-1) (multipart_upload_completed.max=-1) (op_abort.failures.max=-1) (object_multipart_aborted.failures.max=-1) (action_executor_acquired.max=0));\r\nmeans=((op_abort.failures.mean=(samples=0, sum=0, mean=0.0000)) (action_executor_acquired.mean=(samples=1, sum=0, mean=0.0000)) (action_executor_acquired.failures.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.failures.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.mean=(samples=0, sum=0, mean=0.0000)));\r\n}}\r\n2022-01-06 13:41:57,291 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - creating a file s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file: duration 0:00:230\r\n2022-01-06 13:41:57,291 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: Listing  s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921\r\n2022-01-06 13:41:57,331 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - Listing  s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921: duration 0:00:040\r\n2022-01-06 13:41:57,331 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: Reading a file s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file\r\ninput stream summary: org.apache.hadoop.fs.FSDataInputStream@26f3d90c: S3AInputStream{s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file wrappedStream=closed read policy=normal pos=7 nextReadPos=7 contentLength=7 contentRangeStart=0 contentRangeFinish=7 remainingInCurrentRequest=0 ChangeTracker{ETagChangeDetectionPolicy mode=Server, revisionId='c96643be2404fdc0054404530b198c21'}\r\nStreamStatistics{counters=((stream_read_bytes_discarded_in_close=0) (stream_read_operations_incomplete=0) (stream_read_close_operations=1) (stream_read_fully_operations=0) (stream_read_opened=1) (stream_read_seek_backward_operations=0) (stream_read_closed=1) (stream_read_operations=1) (stream_read_bytes_backwards_on_seek=0) (stream_read_seek_bytes_discarded=0) (action_http_get_request.failures=0) (stream_read_seek_policy_changed=1) (stream_aborted=0) (stream_read_unbuffered=0) (stream_read_seek_operations=0) (stream_read_total_bytes=7) (stream_read_exceptions=0) (stream_read_version_mismatches=0) (stream_read_seek_forward_operations=0) (stream_read_bytes_discarded_in_abort=0) (stream_read_bytes=7) (action_http_get_request=1) (stream_read_seek_bytes_skipped=0));\r\ngauges=((stream_read_gauge_input_policy=0));\r\nminimums=((action_http_get_request.min=45) (action_http_get_request.failures.min=-1));\r\nmaximums=((action_http_get_request.max=45) (action_http_get_request.failures.max=-1));\r\nmeans=((action_http_get_request.mean=(samples=1, sum=45, mean=45.0000)) (action_http_get_request.failures.mean=(samples=0, sum=0, mean=0.0000)));\r\n}}\r\n2022-01-06 13:41:57,432 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - Reading a file s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file: duration 0:00:101\r\n2022-01-06 13:41:57,432 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: Renaming file s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file under s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir\r\n2022-01-06 13:41:58,446 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - Renaming file s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/file under s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir: duration 0:01:014\r\n2022-01-06 13:41:58,447 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: probing path s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir/subfile\r\n2022-01-06 13:41:58,595 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - probing path s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir/subfile: duration 0:00:148\r\n2022-01-06 13:41:58,595 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: delete dir s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir2\r\n2022-01-06 13:41:58,854 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - delete dir s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir2: duration 0:00:259\r\n2022-01-06 13:41:58,854 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: probing path s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir2\r\n2022-01-06 13:41:58,921 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - probing path s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921/subdir2: duration 0:00:067\r\n2022-01-06 13:41:58,921 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:<init>(56)) - Starting: delete directory s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921\r\n2022-01-06 13:41:59,019 [main] INFO  diag.StoreDiag (StoreDurationInfo.java:close(115)) - delete directory s3a://stevel-london/dir-893a3615-18ac-4f2d-8ec7-d9501e019921: duration 0:00:098\r\n2022-01-06 13:41:59,023 [main] INFO  statistics.IOStatisticsLogging (IOStatisticsLogging.java:logIOStatisticsAtLevel(269)) - IOStatistics: counters=((action_executor_acquired=1)\r\n(action_http_get_request=1)\r\n(action_http_head_request=17)\r\n(audit_request_execution=50)\r\n(audit_span_creation=18)\r\n(directories_created=2)\r\n(directories_deleted=1)\r\n(files_copied=2)\r\n(files_copied_bytes=14)\r\n(files_created=1)\r\n(files_deleted=4)\r\n(object_bulk_delete_request=2)\r\n(object_copy_requests=2)\r\n(object_delete_objects=5)\r\n(object_delete_request=2)\r\n(object_list_request=21)\r\n(object_metadata_request=17)\r\n(object_put_bytes=7)\r\n(object_put_request=3)\r\n(object_put_request_completed=3)\r\n(op_create=1)\r\n(op_delete=2)\r\n(op_get_file_status=6)\r\n(op_get_file_status.failures=4)\r\n(op_list_files=2)\r\n(op_list_status=1)\r\n(op_mkdirs=2)\r\n(op_open=1)\r\n(op_rename=2)\r\n(store_io_request=51)\r\n(stream_read_bytes=7)\r\n(stream_read_close_operations=1)\r\n(stream_read_closed=1)\r\n(stream_read_opened=1)\r\n(stream_read_operations=1)\r\n(stream_read_seek_policy_changed=1)\r\n(stream_read_total_bytes=7)\r\n(stream_write_block_uploads=1)\r\n(stream_write_bytes=7)\r\n(stream_write_total_data=14));\r\n\r\ngauges=((stream_write_block_uploads_pending=1));\r\n\r\nminimums=((action_executor_acquired.min=0)\r\n(action_http_get_request.min=45)\r\n(action_http_head_request.min=27)\r\n(object_bulk_delete_request.min=44)\r\n(object_delete_request.min=38)\r\n(object_list_request.min=32)\r\n(object_put_request.min=73)\r\n(op_create.min=41)\r\n(op_delete.min=38)\r\n(op_get_file_status.failures.min=66)\r\n(op_get_file_status.min=3)\r\n(op_list_files.min=40)\r\n(op_list_status.min=571)\r\n(op_mkdirs.min=185)\r\n(op_rename.min=399));\r\n\r\nmaximums=((action_executor_acquired.max=0)\r\n(action_http_get_request.max=45)\r\n(action_http_head_request.max=110)\r\n(object_bulk_delete_request.max=61)\r\n(object_delete_request.max=41)\r\n(object_list_request.max=558)\r\n(object_put_request.max=167)\r\n(op_create.max=41)\r\n(op_delete.max=87)\r\n(op_get_file_status.failures.max=148)\r\n(op_get_file_status.max=67)\r\n(op_list_files.max=53)\r\n(op_list_status.max=571)\r\n(op_mkdirs.max=186)\r\n(op_rename.max=428));\r\n\r\nmeans=((action_executor_acquired.mean=(samples=1, sum=0, mean=0.0000))\r\n(action_http_get_request.mean=(samples=1, sum=45, mean=45.0000))\r\n(action_http_head_request.mean=(samples=17, sum=629, mean=37.0000))\r\n(object_bulk_delete_request.mean=(samples=2, sum=105, mean=52.5000))\r\n(object_delete_request.mean=(samples=2, sum=79, mean=39.5000))\r\n(object_list_request.mean=(samples=21, sum=1344, mean=64.0000))\r\n(object_put_request.mean=(samples=3, sum=360, mean=120.0000))\r\n(op_create.mean=(samples=1, sum=41, mean=41.0000))\r\n(op_delete.mean=(samples=2, sum=125, mean=62.5000))\r\n(op_get_file_status.failures.mean=(samples=4, sum=355, mean=88.7500))\r\n(op_get_file_status.mean=(samples=2, sum=70, mean=35.0000))\r\n(op_list_files.mean=(samples=2, sum=93, mean=46.5000))\r\n(op_list_status.mean=(samples=1, sum=571, mean=571.0000))\r\n(op_mkdirs.mean=(samples=2, sum=371, mean=185.5000))\r\n(op_rename.mean=(samples=2, sum=827, mean=413.5000)));\r\n\r\nJVM: memory=89262960\r\n```\r\n\r\nI'm actually going to list that on the commands to run during validation because it could potentially find a problem on the command line.\r\n",
        "url": "https://github.com/apache/hadoop/pull/3864#issuecomment-1006605173"
    },
    {
        "project": "apm-agent-java",
        "id": 1113408659,
        "body": "I am a bit confused with this PR state, could anyone help me understand/confirm the following:\r\n\r\n- this PR currently targets the `1.x` branch, not `main` is that intended ? We usually force-update `1.x` branch during release thus the changes will be lost on the next release.\r\n- the only changes that were added where in https://github.com/elastic/apm-agent-java/pull/2526/commits/2da94afdb0aae5d9ed396768e1a45c2d65aa5f2e commit.\r\n\r\nSo, if the target branch choice is not intended and the only changes are the ones in the last commit, the next steps I plan to execute are:\r\n- [x] change the target PR branch to `main` + rebase the pr on `main`.\r\n- [x] cherry-pick/rewrite the last commit to include the changes in https://github.com/elastic/apm-agent-java/pull/2526/commits/2da94afdb0aae5d9ed396768e1a45c2d65aa5f2e\r\n- [x] change `server_urls` to `server_url` for simplicity, even if we support both.\r\n- [x] force-push this PR branch to keep a simple history and avoid having `1.x` merged into `main`.\r\n",
        "url": "https://github.com/elastic/apm-agent-java/pull/2526#issuecomment-1113408659"
    },
    {
        "project": "flink",
        "id": 1041402840,
        "body": "> Are you also planning a blog post for this? It would be a good opportunity to enhance this with some high level pictures that could be then reused.\r\n\r\nYes, blog post is in our plan, maybe shortly after 1.15 release.\r\n",
        "url": "https://github.com/apache/flink/pull/18757#issuecomment-1041402840"
    },
    {
        "project": "pulsar",
        "id": 1126255563,
        "body": "@nicoloboschi @mattisonchao \r\nI have to rework this patch, actually there is no way to pass values containing the \"comma\" (,) or the equals (=) characters, so you are very limited.\r\n\r\nI will update it early next week",
        "url": "https://github.com/apache/pulsar/pull/15503#issuecomment-1126255563"
    },
    {
        "project": "pulsar",
        "id": 1201512995,
        "body": "Well. I fixed this issue locally. Here is a patch you can make use of:\r\n\r\n```patch\r\ndiff --git a/pom.xml b/pom.xml\r\nindex a6ce94fe734..2d093f1f1eb 100644\r\n--- a/pom.xml\r\n+++ b/pom.xml\r\n@@ -95,7 +95,8 @@ flexible messaging model and an intuitive client API.</description>\r\n     <test.additional.args>\r\n       --add-opens java.base/jdk.internal.loader=ALL-UNNAMED\r\n       --add-opens java.base/java.lang=ALL-UNNAMED <!--Mockito-->\r\n-      --add-opens java.base/java.io=ALL-UNNAMED <!--Bookkeeper NativeIO -->\r\n+      --add-opens java.base/java.io=ALL-UNNAMED <!--Bookkeeper NativeIO-->\r\n+      --add-opens java.base/java.util=ALL-UNNAMED <!--System Lambda-->\r\n       --add-opens java.base/sun.net=ALL-UNNAMED <!--netty.DnsResolverUtil-->\r\n       --add-opens java.management/sun.management=ALL-UNNAMED <!--JvmDefaultGCMetricsLogger-->\r\n     </test.additional.args>\r\n@@ -1294,6 +1295,13 @@ flexible messaging model and an intuitive client API.</description>\r\n   </dependencyManagement>\r\n \r\n   <dependencies>\r\n+    <dependency>\r\n+      <groupId>com.github.stefanbirkner</groupId>\r\n+      <artifactId>system-lambda</artifactId>\r\n+      <version>1.2.1</version>\r\n+      <scope>test</scope>\r\n+    </dependency>\r\n+\r\n     <!-- These dependencies are common to all the submodules -->\r\n     <dependency>\r\n       <groupId>org.apache.pulsar</groupId>\r\ndiff --git a/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java b/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java\r\nindex 8dbc880fa16..22ef2dd9e60 100644\r\n--- a/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java\r\n+++ b/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java\r\n@@ -21,11 +21,7 @@ package org.apache.pulsar.functions.secretsprovider;\r\n \r\n import static org.testng.Assert.assertEquals;\r\n import static org.testng.Assert.assertNull;\r\n-\r\n-import java.util.HashMap;\r\n-import java.util.Map;\r\n-\r\n-import org.powermock.reflect.Whitebox;\r\n+import com.github.stefanbirkner.systemlambda.SystemLambda;\r\n import org.testng.annotations.Test;\r\n \r\n public class EnvironmentBasedSecretsProviderTest {\r\n@@ -33,22 +29,8 @@ public class EnvironmentBasedSecretsProviderTest {\r\n     public void testConfigValidation() throws Exception {\r\n         EnvironmentBasedSecretsProvider provider = new EnvironmentBasedSecretsProvider();\r\n         assertNull(provider.provideSecret(\"mySecretName\", \"Ignored\"));\r\n-        injectEnvironmentVariable(\"mySecretName\", \"SecretValue\");\r\n-        assertEquals(provider.provideSecret(\"mySecretName\", \"Ignored\"), \"SecretValue\");\r\n-    }\r\n-\r\n-    private static void injectEnvironmentVariable(String key, String value)\r\n-            throws Exception {\r\n-\r\n-        Class<?> processEnvironment = Class.forName(\"java.lang.ProcessEnvironment\");\r\n-        Map<String,String> unmodifiableMap = new HashMap<>(Whitebox\r\n-                .getInternalState(processEnvironment, \"theUnmodifiableEnvironment\"));\r\n-        unmodifiableMap.put(key, value);\r\n-        Whitebox.setInternalState(processEnvironment, \"theUnmodifiableEnvironment\", unmodifiableMap);\r\n-\r\n-        Map<String,String> envMap = new HashMap<>(Whitebox\r\n-                .getInternalState(processEnvironment, \"theEnvironment\"));\r\n-        envMap.put(key, value);\r\n-        Whitebox.setInternalState(processEnvironment, \"theEnvironment\", envMap);\r\n+        SystemLambda.withEnvironmentVariable(\"mySecretName\", \"SecretValue\").execute(() -> {\r\n+            assertEquals(provider.provideSecret(\"mySecretName\", \"Ignored\"), \"SecretValue\");\r\n+        });\r\n     }\r\n }\r\n```\r\n\r\nI guess the reason is that `Whitebox` failed to set the unmodifiableMap with log4j2 2.18.0 while I don't have an idea how it happens.\r\n\r\nI don't like powermock as it's lack of maintenance. It seems all usage of powermock is `Whitebox`. We can get rid of it with simple reflections. This can be a separated issue, though.\r\n\r\nDebugging details:\r\n\r\n```\r\nBefore\r\n2022-08-02T01:00:16,796+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@46 - 1 System.getenv(): 120752794 {}\r\n2022-08-02T01:00:17,932+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@52 - 2 unmodifiableMap: 901581632 {}\r\n2022-08-02T01:00:17,932+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@53 - 2 System.getenv(): 901581632 {}\r\n\r\nAfter\r\n2022-08-02T01:00:47,197+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@46 - 1 System.getenv(): 120750803 {}\r\n2022-08-02T01:00:47,203+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@52 - 2 unmodifiableMap: 901579641 {}\r\n2022-08-02T01:00:47,203+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@53 - 2 System.getenv(): 120750803 {}\r\n```\r\n\r\nAs you can see, \"After\" we failed to set the env map. But I don't know how it happens exactly.",
        "url": "https://github.com/apache/pulsar/pull/16884#issuecomment-1201512995"
    },
    {
        "project": "apm-agent-java",
        "id": 1202568890,
        "body": "If we manage to update the `JulMdc` of the application, it means that we can make ECS logging output ((of the application) contain the `trace.id` `error.id` and `transaction.id` fields that are populated by the agent.\r\n\r\nHowever, if we limit ourselves to the MDC attributes, the other ECS logging attributes `service.name`, `service.version`, ... will remain with the values configured at the application level.\r\n\r\nIn the case of overriding logs, I *think* it would be simpler to just replace the `EcsFormatter` of the application with the internal one of the agent, in other words it could be as simple as allowing `EcsFormatter` for Jul to allow delegating to another instance that would be set by the agent through instrumentation.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2724#issuecomment-1202568890"
    },
    {
        "project": "spring-security",
        "id": 1149984644,
        "body": "Because of javax->jakarta, the merge isn't clean so it needs to be done manually. Since you've done that work already, I figure we can finish this PR, too. That said, if you are out of time, I can take care of it by cherry-picking the other and changing the package names.\r\n\r\nUp to you -- if you close the PR, I'll do the cherry-picking. If not, we can merge this one as well once it's ready.",
        "url": "https://github.com/spring-projects/spring-security/pull/11319#issuecomment-1149984644"
    },
    {
        "project": "apm-agent-java",
        "id": 774639596,
        "body": "recreated here https://github.com/elastic/apm-agent-java/pull/1650",
        "url": "https://github.com/elastic/apm-agent-java/pull/1510#issuecomment-774639596"
    },
    {
        "project": "apm-agent-java",
        "id": 1082837588,
        "body": "@bmorelli25 The build docs are failing now because I added a back link to the Lambda extension `{apm-guide-ref}/aws-lambda-arch.html`. Will this resolve when https://github.com/elastic/apm-aws-lambda/pull/158 is merged or is something in addition needed (e.g. cherry picking like you did for `{apm-guide-ref}/aws-lambda-config-options.html`) ?\r\n\r\n",
        "url": "https://github.com/elastic/apm-agent-java/pull/2556#issuecomment-1082837588"
    },
    {
        "project": "pulsar",
        "id": 1205237650,
        "body": "@coderzc `PartitionedConsumerImpl` and `MultiTopicsConsumerImpl` may be also can improve",
        "url": "https://github.com/apache/pulsar/pull/16940#issuecomment-1205237650"
    },
    {
        "project": "skywalking",
        "id": 1200628485,
        "body": "> Please update changes.md in the PR template. You missed one step. And remove <> around the issue number.\r\n\r\nplease check again.",
        "url": "https://github.com/apache/skywalking/pull/9414#issuecomment-1200628485"
    },
    {
        "project": "hadoop",
        "id": 918179680,
        "body": "Thanks for the review fixes. +1 from my side.\r\n\r\nThe hasEmptyPart could be simplified with the newly added iterator, and there are some checkStyle warnings (longer than 100 lines).",
        "url": "https://github.com/apache/hadoop/pull/3342#issuecomment-918179680"
    },
    {
        "project": "spring-boot",
        "id": 466665545,
        "body": "@snicoll I've restructured the section and added more general instructions on how to use JAXB.",
        "url": "https://github.com/spring-projects/spring-boot/pull/16005#issuecomment-466665545"
    },
    {
        "project": "hadoop",
        "id": 1179822427,
        "body": "Hi, @aajisaka I have seen the suggestions you gave, I will modify and refactor the relevant code according to your suggestions, thank you very much.\r\n\r\nRefer to the discussion of this jira\r\nhttps://issues.apache.org/jira/browse/HADOOP-18277?focusedCommentId=17564659&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17564659",
        "url": "https://github.com/apache/hadoop/pull/4463#issuecomment-1179822427"
    },
    {
        "project": "druid",
        "id": 472613171,
        "body": "> I don't especially like the abstract method that returns a TypeReference to tell it how to serialize the things, but I guess it's ok compared to the alternatives I can think of like custom json deserializer or creating ParameterizedType with reflection. I do agree that it's nice that pulling out SequenceMetadata does make it a bit more clear the coupling between it and SeekableStreamIndexTaskRunner, I think there is probably some room for refactoring. I can't help but wonder if maybe SeekableStreamIndexTaskRunner should not have been abstract, and rather all of it's abstract methods usage instead be delegating to stream specific types that it encapsulates to do all the things like handle offset comparisons, serde, control points etc, but I haven't quite nailed down what this should look like.\r\n\r\nAh. I think the TypeReference based thing you did is pretty reasonable compared to the alternatives. TaskAction does something similar. So that makes me feel better :)",
        "url": "https://github.com/apache/druid/pull/7256#issuecomment-472613171"
    },
    {
        "project": "pulsar",
        "id": 903373632,
        "body": "Hi @bytestreme we're redesigning the website and may have a new design for the drop-down list, so how about reverting your changes?",
        "url": "https://github.com/apache/pulsar/pull/11201#issuecomment-903373632"
    },
    {
        "project": "hadoop",
        "id": 1160394168,
        "body": "> @ashutoshcipher Shall we assign it to some variable in try and use it from both try and catch clause.\r\n> \r\n> ```\r\n>     String hostString = urlString.replace(HOST_TOKEN, ip);\r\n>     try {\r\n>       URL url = new URL(hostString);\r\n> ```\r\n\r\nThanks for the suggestion @PrabhuJoseph , I have made the required changes. \r\n",
        "url": "https://github.com/apache/hadoop/pull/4436#issuecomment-1160394168"
    },
    {
        "project": "skywalking",
        "id": 502590697,
        "body": "> inline\r\n\r\nSorry, @IanCao . I don't really understand what you mean.",
        "url": "https://github.com/apache/skywalking/pull/2888#issuecomment-502590697"
    },
    {
        "project": "flink",
        "id": 1202302082,
        "body": "Hi @Myasuka, this test has always been unstable due to the way we obtained checkpoints, I refactored it. Could you please take a look?",
        "url": "https://github.com/apache/flink/pull/20420#issuecomment-1202302082"
    },
    {
        "project": "hadoop",
        "id": 1180734620,
        "body": "> So it looks like the new dependency on rs-api needs to be removed. This may not require jackson to be downgraded too.\r\n\r\njavax.ws.rs-api was introduced only to support Jackson 2.13 upgrade. Without rs-api, (on Jackson 2.13) majority of Yarn and ATS tests fail as jackson 2.13 does have dependency on JAX-RS 2 based rs-api (PR #3749)",
        "url": "https://github.com/apache/hadoop/pull/4460#issuecomment-1180734620"
    },
    {
        "project": "hadoop",
        "id": 1197975586,
        "body": "merged manually",
        "url": "https://github.com/apache/hadoop/pull/4645#issuecomment-1197975586"
    },
    {
        "project": "pulsar",
        "id": 1185112645,
        "body": "> > > @lordcheng10 Could you please provide more details about the BUG? so that the reviewers can easily understand what the issue is that the PR wants to fix.\r\n> > \r\n> > \r\n> > Sorry, I didn't describe clearly. @codelipenghui\r\n> > I'll give an example to illustrate:\r\n> > \r\n> > 1. Configure loadBalancerNamespaceMaximumBundles=128\r\n> > 2. The current number of bundles in the namespace is 120\r\n> > 3. According to the current logic, after executing the method findBundlesToSplit, 40 bundles may be found that need to be split;\r\n> > 4. After these bundles, the number of bundles in the namespace will become 120+40=160 > loadBalancerNamespaceMaximumBundles=128\r\n> \r\n> Make sense.\r\n\r\nWhen judging whether the number of bundles exceeds the configuration, it is necessary to add the already selected bundles. @codelipenghui @Technoboy- ",
        "url": "https://github.com/apache/pulsar/pull/16552#issuecomment-1185112645"
    },
    {
        "project": "hadoop",
        "id": 1207149152,
        "body": ":broken_heart: **-1 overall**\r\n\r\n\r\n\r\n\r\n\r\n\r\n| Vote | Subsystem | Runtime |  Logfile | Comment |\r\n|:----:|----------:|--------:|:--------:|:-------:|\r\n| +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n|||| _ Prechecks _ |\r\n| +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n| +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n| +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n| +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n|||| _ trunk Compile Tests _ |\r\n| +0 :ok: |  mvndep  |  15m 25s |  |  Maven dependency ordering for branch  |\r\n| +1 :green_heart: |  mvninstall  |  25m 49s |  |  trunk passed  |\r\n| +1 :green_heart: |  compile  |   4m  1s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  compile  |   3m 29s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  checkstyle  |   1m 32s |  |  trunk passed  |\r\n| +1 :green_heart: |  mvnsite  |   2m 19s |  |  trunk passed  |\r\n| +1 :green_heart: |  javadoc  |   2m  6s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 56s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 58s |  |  trunk passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n|||| _ Patch Compile Tests _ |\r\n| +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n| +1 :green_heart: |  mvninstall  |   1m 40s |  |  the patch passed  |\r\n| +1 :green_heart: |  compile  |   3m 49s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  cc  |   3m 49s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 49s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 generated 3 new + 446 unchanged - 0 fixed = 449 total (was 446)  |\r\n| +1 :green_heart: |  compile  |   3m 16s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  cc  |   3m 16s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 16s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 generated 3 new + 370 unchanged - 0 fixed = 373 total (was 370)  |\r\n| +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n| +1 :green_heart: |  checkstyle  |   1m 11s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server: The patch generated 0 new + 11 unchanged - 1 fixed = 11 total (was 12)  |\r\n| +1 :green_heart: |  mvnsite  |   1m 49s |  |  the patch passed  |\r\n| +1 :green_heart: |  javadoc  |   1m 31s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 27s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 43s |  |  the patch passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n|||| _ Other Tests _ |\r\n| +1 :green_heart: |  unit  |   3m  5s |  |  hadoop-yarn-server-common in the patch passed.  |\r\n| +1 :green_heart: |  unit  | 102m 57s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n| +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n|  |   | 231m 15s |  |  |\r\n\r\n\r\n| Subsystem | Report/Notes |\r\n|----------:|:-------------|\r\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/Dockerfile |\r\n| GITHUB PR | https://github.com/apache/hadoop/pull/4711 |\r\n| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat |\r\n| uname | Linux 0fd97407eebb 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | dev-support/bin/hadoop.sh |\r\n| git revision | trunk / 8e63615447d74945f2ec2f11287f1b5f2867a29c |\r\n| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n|  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/testReport/ |\r\n| Max. process+thread count | 949 (vs. ulimit of 5500) |\r\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server |\r\n| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/console |\r\n| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n| Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/4711#issuecomment-1207149152"
    },
    {
        "project": "spring-boot",
        "id": 1049061041,
        "body": "Thanks, Lars. Unfortunately, these changes seem to expose a bug (or some misconfiguration in our build) where [a `ConcurrentModificationException` is thrown](https://ge.spring.io/s/siwrtgds7keac/failure?expanded-stacktrace=WyIwLTEtMiIsIjAtMSIsIjAiXQ#1). Some experimentation suggests that the problem's been fixed in Gradle 7.x but I am not 100% sure as I don't fully understand the failure.\r\n\r\nWe haven't really had any significant problems with the time taken to configure tasks so I don't think we can really justify potentially destabilising the build at this time. Thanks anyway.\r\n\r\nIf it's any consolation, looking at this did make me realise that https://github.com/spring-io/spring-javaformat/issues/319 would be a good thing to do. In Spring Boot's build, the Java Format Plugin's eagerly creating over 1000 tasks so we should see a significant reduction once 0.0.32 is released and Boot's build has been upgraded.",
        "url": "https://github.com/spring-projects/spring-boot/pull/29827#issuecomment-1049061041"
    },
    {
        "project": "spring-boot",
        "id": 1059491828,
        "body": "@pivotal-cla This is an Obvious Fix\r\n\r\nThis reinstates previously existing code and introduces no new IP.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30002#issuecomment-1059491828"
    },
    {
        "project": "skywalking",
        "id": 1090255480,
        "body": "\r\n\r\n> So, generally, you want an OAP server having multiple consumers?\r\n\r\nIt depends on the speed of producers and consumers. If MQ accumulates, increase the number of consumers and reduce delay",
        "url": "https://github.com/apache/skywalking/pull/8828#issuecomment-1090255480"
    },
    {
        "project": "flink",
        "id": 1205448171,
        "body": "@xintongsong Thanks for the review, I have address all of comments, please take a look.",
        "url": "https://github.com/apache/flink/pull/20426#issuecomment-1205448171"
    },
    {
        "project": "skywalking",
        "id": 1153963524,
        "body": "About this issue, `NamingControl` is used to control the length of original data. If in some cases, the length gets breaking, we should fix it from there. By following your way, it is most likely you would break the encoded ID, which could break the further decoding.\r\n\r\n> By the way, how to become skywaling commiter, I want to contribute my part to skywalking\uff01\r\n\r\nFirst of all, we welcome contributions no matter whether you are a committer or not. The official process to invite a new committer is through this, https://skywalking.apache.org/docs/main/latest/en/guides/asf/committer/\r\n\r\nFor a code contributor, like you, we usually check whether you have spent a long time on a specific part(s) of SkyWalking, continuously fixing things, leading new features to land, and taking part in release votes to understand how to vote a release.\r\n\r\nThe official committer team is listed here, you could follow some of them to see what they did/are doing for the community. https://skywalking.apache.org/team/. \r\n\r\nThe key point of a committer from my personal perspective(not mean the same to all other PMC members), is that SkyWalking's other committers agree, that you could be trusted, and responsible for the project. ",
        "url": "https://github.com/apache/skywalking/pull/9209#issuecomment-1153963524"
    },
    {
        "project": "spring-framework",
        "id": 488623499,
        "body": "This is in master now, including a first pass with a revision of mine that removes all legacy configuration options from `AbstractReactiveTransactionManager` plus a few further things. The intent is to keep the reactive transaction abstraction rather streamlined, not burdening it with JTA/JPA-oriented options that `AbstractPlatformTransactionManager` historically had to deal with. Let me know if anything important is missing now for the Mongo/R2DBC implementations; we can of course consider reintroducing relevant options through template methods or the like (ideally not through boolean setters again).\r\n\r\nA second pass is coming in just a bit, renaming some of the types in `transaction.reactive` through dropping the \"Reactive\" prefix: `TransactionSynchronizationManager`, `TransactionCallback` etc read much nicer in the corresponding API declarations and are locally prefixed by \"reactive\" through their containing package anyway. Also, they don't immediately conflict with the same-named types in `transaction.support`, whereas we have to use specific prefixes for new types in the `transaction` base package (e.g. `ReactiveTransactionStatus` vs `TransactionStatus`). We applied the same reasoning to our reactive HTTP abstractions, using same-named types in quite a few cases there as well.\r\n\r\n@mp911de Please wait for that renaming commit before integrating the Spring Data bits with it.",
        "url": "https://github.com/spring-projects/spring-framework/pull/22646#issuecomment-488623499"
    },
    {
        "project": "skywalking",
        "id": 1202174383,
        "body": "@mrproliu Please check this PR. We would need profiling(record data type) to support this kind of merging to reduce ElasticSearch load.",
        "url": "https://github.com/apache/skywalking/pull/9416#issuecomment-1202174383"
    },
    {
        "project": "spring-security",
        "id": 962142897,
        "body": "Thanks, @nor-ek!\r\n\r\nWill you please adjust your commit to include the ticket in the message instead of the title, like so?\r\n\r\n```\r\nCommit Title\r\n\r\nCloses gh-10364\r\n```\r\n\r\nAlso, you might consider squashing your commits as this will simplify backporting.",
        "url": "https://github.com/spring-projects/spring-security/pull/10463#issuecomment-962142897"
    },
    {
        "project": "hadoop",
        "id": 1193994496,
        "body": "## RetryPolicies - RedundantModifier\r\n\r\n```\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java:602:    public OtherThanRemoteAndSaslExceptionDependentRetry(RetryPolicy defaultPolicy,:5: Redundant 'public' modifier. [RedundantModifier]\r\n```\r\n\r\nCorrecting this checkstyle issue because I am touching this line anyway\r\n\r\n\r\n## RetryPolicies - EmptyBlock\r\n\r\n```\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/retry/RetryPolicies.java:614:      if (e instanceof RemoteException || isSaslFailure(e)) {:61: Must have at least one statement. [EmptyBlock]\r\n```\r\n\r\nThe code had the empty block before the change:\r\n\r\n```\r\n      if (e instanceof RemoteException || isSaslFailure(e)) {\r\n        // do nothing\r\n      } else {\r\n```\r\n\r\nWill refactor this because required change is minimal\r\n\r\n\r\n## SaslRpcClient - Indentation\r\n\r\n```\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java:240:        final String serverPrincipal;: 'block' child has incorrect indentation level 8, expected level should be 6. [Indentation]\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java:241:        try {: 'try' has incorrect indentation level 8, expected level should be 6. [Indentation]\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java:242:          serverPrincipal = getServerPrincipal(authType);: 'try' child has incorrect indentation level 10, expected level should be 8. [Indentation]\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java:243:        } catch (IllegalArgumentException ex) {: 'try rcurly' has incorrect indentation level 8, expected level should be 6. [Indentation]\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java:246:          throw new SaslException(\"Bad Kerberos server principal configuration\", ex);: 'catch' child has incorrect indentation level 10, expected level should be 8. [Indentation]\r\n./hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java:247:        }: 'catch rcurly' has incorrect indentation level 8, expected level should be 6. [Indentation]\r\n```\r\n\r\nThis is what the code looks like after applying this checkstyle rule:\r\n\r\n```\r\n      case KERBEROS: {\r\n        if (ugi.getRealAuthenticationMethod().getAuthMethod() !=\r\n            AuthMethod.KERBEROS) {\r\n          LOG.debug(\"client isn't using kerberos\");\r\n          return null;\r\n        }\r\n      final String serverPrincipal;\r\n      try {\r\n        serverPrincipal = getServerPrincipal(authType);\r\n      } catch (IllegalArgumentException ex) {\r\n        // YARN-11210: getServerPrincipal can throw IllegalArgumentException if Kerberos\r\n        // configuration is bad, this is surfaced as a non-retryable SaslException\r\n        throw new SaslException(\"Bad Kerberos server principal configuration\", ex);\r\n      }\r\n        if (serverPrincipal == null) {\r\n          LOG.debug(\"protocol doesn't use kerberos\");\r\n          return null;\r\n        }\r\n```\r\n\r\nNot sure why its recommending this indentation level, but it doesn't align with the way this code is currently structured. Going to recommend leaving this\r\n",
        "url": "https://github.com/apache/hadoop/pull/4563#issuecomment-1193994496"
    },
    {
        "project": "spring-framework",
        "id": 1184308290,
        "body": "Note that this is only possible in inline code blocks because indentation and line breaks are lost with `{@code]`.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28821#issuecomment-1184308290"
    },
    {
        "project": "skywalking",
        "id": 887378117,
        "body": "# [Codecov](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report\n> Merging [#7377](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (f9e6745) into [master](https://codecov.io/gh/apache/skywalking/commit/ffc69dc6f369730819a9a1420ccdb69e369d6d28?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (ffc69dc) will **increase** coverage by `4.05%`.\n> The diff coverage is `n/a`.\n\n> :exclamation: Current head f9e6745 differs from pull request most recent head 28b79b4. Consider uploading reports for the commit 28b79b4 to get more accurate results\n[![Impacted file tree graph](https://codecov.io/gh/apache/skywalking/pull/7377/graphs/tree.svg?width=650&height=150&src=pr&token=qrILxY5yA8&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)\n\n```diff\n@@             Coverage Diff              @@\n##             master    #7377      +/-   ##\n============================================\n+ Coverage     54.04%   58.09%   +4.05%     \n+ Complexity     4370      864    -3506     \n============================================\n  Files          1893      179    -1714     \n  Lines         41032     4489   -36543     \n  Branches       4612      521    -4091     \n============================================\n- Hits          22176     2608   -19568     \n+ Misses        17729     1556   -16173     \n+ Partials       1127      325     -802     \n```\n\n\n| [Impacted Files](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage \u0394 | |\n|---|---|---|\n| [...t/status/HierarchyMatchExceptionCheckStrategy.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvY29udGV4dC9zdGF0dXMvSGllcmFyY2h5TWF0Y2hFeGNlcHRpb25DaGVja1N0cmF0ZWd5LmphdmE=) | `9.09% <0.00%> (-90.91%)` | :arrow_down: |\n| [...g/apm/agent/core/context/status/StatusChecker.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvY29udGV4dC9zdGF0dXMvU3RhdHVzQ2hlY2tlci5qYXZh) | `23.80% <0.00%> (-76.20%)` | :arrow_down: |\n| [...ywalking/apm/agent/core/plugin/PluginSelector.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvcGx1Z2luL1BsdWdpblNlbGVjdG9yLmphdmE=) | `28.57% <0.00%> (-71.43%)` | :arrow_down: |\n| [...enhance/ClassStaticMethodsEnhancePluginDefine.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvcGx1Z2luL2ludGVyY2VwdG9yL2VuaGFuY2UvQ2xhc3NTdGF0aWNNZXRob2RzRW5oYW5jZVBsdWdpbkRlZmluZS5qYXZh) | `33.33% <0.00%> (-66.67%)` | :arrow_down: |\n| [...re/logging/core/converters/AgentNameConverter.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvbG9nZ2luZy9jb3JlL2NvbnZlcnRlcnMvQWdlbnROYW1lQ29udmVydGVyLmphdmE=) | `0.00% <0.00%> (-66.67%)` | :arrow_down: |\n| [...re/logging/core/converters/ThrowableConverter.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvbG9nZ2luZy9jb3JlL2NvbnZlcnRlcnMvVGhyb3dhYmxlQ29udmVydGVyLmphdmE=) | `16.66% <0.00%> (-66.67%)` | :arrow_down: |\n| [...he/skywalking/apm/commons/datacarrier/EnvUtil.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLWNvbW1vbnMvYXBtLWRhdGFjYXJyaWVyL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9za3l3YWxraW5nL2FwbS9jb21tb25zL2RhdGFjYXJyaWVyL0VudlV0aWwuamF2YQ==) | `20.00% <0.00%> (-60.00%)` | :arrow_down: |\n| [...lking/apm/agent/core/context/ExtensionContext.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvY29udGV4dC9FeHRlbnNpb25Db250ZXh0LmphdmE=) | `23.25% <0.00%> (-51.17%)` | :arrow_down: |\n| [...core/context/status/OffExceptionCheckStrategy.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvY29udGV4dC9zdGF0dXMvT2ZmRXhjZXB0aW9uQ2hlY2tTdHJhdGVneS5qYXZh) | `50.00% <0.00%> (-50.00%)` | :arrow_down: |\n| [...gin/interceptor/enhance/MethodInterceptResult.java](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-YXBtLXNuaWZmZXIvYXBtLWFnZW50LWNvcmUvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYXBtL2FnZW50L2NvcmUvcGx1Z2luL2ludGVyY2VwdG9yL2VuaGFuY2UvTWV0aG9kSW50ZXJjZXB0UmVzdWx0LmphdmE=) | `50.00% <0.00%> (-50.00%)` | :arrow_down: |\n| ... and [1808 more](https://codecov.io/gh/apache/skywalking/pull/7377/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)\n> `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation). Last update [65e8ab3...28b79b4](https://codecov.io/gh/apache/skywalking/pull/7377?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation).\n",
        "url": "https://github.com/apache/skywalking/pull/7377#issuecomment-887378117"
    },
    {
        "project": "pulsar",
        "id": 1198020631,
        "body": "Hi @codelipenghui\r\n\r\n> We have synchronized (pendingMarkDeleteOps) for both create cursor failure and `internalAsyncMarkDelete`, if triggered the ledger creation means the left thread got the lock and the right thread can only get the lock after the `mdEntry` is added to the `pendingMarkDeleteOps `, no?\r\n\r\nAccording to your tips, I found a new problem: see Flow-2 above. And I have modified the `Motivation`. \r\n\r\n\r\n@codelipenghui @Technoboy- @mattisonchao  Could you review this PR again?\r\n\r\n\r\n\r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/16841#issuecomment-1198020631"
    },
    {
        "project": "apm-agent-java",
        "id": 1027238517,
        "body": "@felixbarny ,\r\n\r\nThis issue gave a lot of context on the issue. In my org our security scanners have been constantly creating new bugs for me to resolve in 30 days all because of elastic-apm, log4j keeps coming up over and over again.\r\n\r\n> > could we have a version without java7 support, otherwise we need to either disable security scanning or compiling ourself\r\n> \r\n> Seconding this. Elastic libs have had a Low vulnerability that has shown up in security scanners (Twistlock, etc...) for over a year an a half. Many orgs have a 180 day compliance policy to fix these issues. Now there is a CRITICAL vulnerability that will show up and blow up many scan results. Often orgs have 15-30 days to fix these critical/high vulnerabilities.\r\n\r\n+1\r\n\r\nI don't think it's wise to continue a course where you KNOW that so many organizations are going to get false-positive results and come back to you (like I'm doing right now) by doing hacks on hacks just because for some reason Java 7 is important to support.\r\n\r\nHere's a list of things I think you can do here:\r\n\r\n1. Get rid of log4j (preferred) replace it with slf4j/logback?\r\n2. Stop supporting Java 7, for gods sake upgrade log4j\r\n3. Do nothing, and continue to add work for everyone else to work with their security scanning vendors, and continue to have `elastic-apm-agent` to be on the minds of thousands of organizations who need to emergency patch their stuff and wish to get rid of log4j but can't because a transitive of a transitive of a transitive dependency chose to support Java 7\r\n\r\nHere's a list of all the vulnerabilities that I had to field so far:\r\n\r\n1. CVE-2020-9488\r\n2. CVE-2021-44832\r\n3. CVE-2021-45105\r\n4. CVE-2021-45046 \r\n\r\nEach time the version changed, it started at log4j 2.11.1, then new bugs created for 2.11.2, 2.11.3, and now 2.11.4",
        "url": "https://github.com/elastic/apm-agent-java/pull/2332#issuecomment-1027238517"
    },
    {
        "project": "apm-agent-java",
        "id": 900210548,
        "body": "> In addition to the minor inlined comments, can we eliminate the duplication of `runAsXXX` and `executeAsXXX` by migrating everything to work with `executeAs` and the `CommandOutput`?\r\n\r\nchecking if this makes things clearer or not",
        "url": "https://github.com/elastic/apm-agent-java/pull/2065#issuecomment-900210548"
    },
    {
        "project": "apm-agent-java",
        "id": 1163055663,
        "body": "> Congrats for pushing this through @SylvainJuge. Apologies for airing dirty laundry here, notwithstanding how badly designed, badly implemented, badly timed, or badly intended my own contribution and work is; suffice to say due to the pushiness and entitlement of @fern-we roping in @kananindzya its unfortunate i never want to contribute to elastic/apm-agent ever again.\r\n\r\nI am really sorry that you've felt offended by what happened here @abdulazizali77 .\r\n\r\nYou initiated this plugin in https://github.com/elastic/apm-agent-java/pull/1932 PR and your commits served as the basis of this PR as we can see in the [commit log](https://github.com/elastic/apm-agent-java/pull/2229/commits), so you still deserve most of the credit for this.\r\n\r\nWhat @kananindzya did (or what I assume the intent is) was just trying to move this forward through another PR, because he is not an Elastic employee he could not directly push nor directly contribute to your PR, if we had to do it again I would have suggested him opening a PR against your own fork/branch instead of opening another one based on your work.\r\n\r\nThere is nothing wrong with your contribution here, even if the code you contributed was later removed or modified, it helped building the current form which we are about to ship and we should all feel grateful for that. Unfortunately, working on agents means that we often have to iterate a lot and often rewrite code often more than once with different approaches before being able to ship something.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2229#issuecomment-1163055663"
    },
    {
        "project": "spring-boot",
        "id": 1087467968,
        "body": "Hey, ignore my comment. Just learned that we split the commits if we need to.\r\n\r\nI think the doc changes are applicable to earlier versions of Spring Boot (2.5.x), too. I'll merge the PR.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30498#issuecomment-1087467968"
    },
    {
        "project": "pulsar",
        "id": 1206016126,
        "body": "@momo-jun Your request has been fixed. Could you help review it again?",
        "url": "https://github.com/apache/pulsar/pull/16941#issuecomment-1206016126"
    },
    {
        "project": "spring-boot",
        "id": 1116157282,
        "body": "@marcwrobel Thanks very much for making your first contribution to Spring Boot. I have applied the changes to 2.5.x and merged them forwards through 2.6.x and 2.7.x into main, picking up changes that were specific to those branches along the way. I don't think any of your improvements were lost along the way, but please let me know if I have missed something.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30773#issuecomment-1116157282"
    },
    {
        "project": "spring-boot",
        "id": 965569351,
        "body": "Thanks a lot @filiphr for all your insights.\r\n\r\nWe've just discussed this as a team and we've decided to:\r\n1. Bring back `RestClient` as a bean; we initially decided that it had low value for actual applications, but with the latest changes in the elasticsearch ecosystem, it's better to have it.\r\n2. Adapt the health contributor to only depend on the low level `RestClient`\r\n3. Deprecate the `RestHighLevelClient` in 2.7, see #28598\r\n4. Support the new `ElasticsearchClient` in 2.7 as a replacement, see #28597\r\n\r\nCould you adapt this PR to address 1) and 2)?\r\nNote that we've scheduled that for 2.7, so there's no rush on this as we're going to focus on getting 2.6 out and critical bug fixes first.",
        "url": "https://github.com/spring-projects/spring-boot/pull/28496#issuecomment-965569351"
    },
    {
        "project": "spring-security",
        "id": 851407829,
        "body": "Signing keys are often managed externally to the application and alg may change when rolling out new keys.\r\nAs caller code, I don't want to set headers related to signing key (alg, kid), so that I don't have to fiddle with the content of the keys.\r\n\r\nActually, the Jwt given to encoder must provide a alg header. Either the caller code reads the alg from a configured property (often it is hard coded) or it has access to the same JwkSource and extracts the alg by itself (as seen [here](https://github.com/spring-projects/spring-security/blob/68f91edbb8a223ee8505893697d85cbe51890ea2/oauth2/oauth2-client/src/main/java/org/springframework/security/oauth2/client/endpoint/NimbusJwtClientAuthenticationParametersConverter.java#L116)). When there is only one signing key in the JwkSource, kid and alg should be guess from this signing key.\r\n\r\nNote that this require alg and kid members in JWK but JWK RFC 7517 set them as optional. This shouldn't be a new requirement as currently, the encoder seems to require them both already.\r\n\r\n",
        "url": "https://github.com/spring-projects/spring-security/pull/9208#issuecomment-851407829"
    },
    {
        "project": "apm-agent-java",
        "id": 1149565231,
        "body": "Gotcha, I created a method that lets you remove suffixes. I think that's the best of both worlds. We can ignore suffixes in the element matcher where we know we have to deal with unorthodox `.redhat` versions.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2665#issuecomment-1149565231"
    },
    {
        "project": "spring-security",
        "id": 716680837,
        "body": "I modified the tests to ensure that the \"default\" instance and the custom salt length instance pass same kind of tests with success. I think that simply adding `encodedLengthSuccess` test is not enough (for example, `matches` method could potentially fail if the salt extraction from the encoded password is not \"custom salt length\" aware).\r\n\r\nBut if you want, i can revert changes on `matches` test. Are there some other tests you want me to revert ?",
        "url": "https://github.com/spring-projects/spring-security/pull/9147#issuecomment-716680837"
    },
    {
        "project": "hadoop",
        "id": 1204196987,
        "body": "@goiri Can you help me merge this pr into trunk branch? Thank you very much!",
        "url": "https://github.com/apache/hadoop/pull/4650#issuecomment-1204196987"
    },
    {
        "project": "hadoop",
        "id": 1204496373,
        "body": "I am ignoring the 2 checkstyle violations for the following reasons:\r\n\r\n------\r\n\r\n```\r\n./hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java:285:  /**: First sentence should end with a period. [JavadocStyle]\r\n```\r\n\r\nThis is an existing comment: https://github.com/apache/hadoop/blob/60433bffc3a7fbb2153a78782d257534f8c7e34f/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java#L285\r\n\r\nSince I am not modifying this comment (i.e. lines of code) in any way, I think its better that I don't touch it\r\n\r\n-------\r\n\r\n```\r\n./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java:1932:  public void testDeleteCorruptReplicaForUnderReplicatedBlockInternal() throws Exception {:3: Method length is 233 lines (max allowed is 150). [MethodLength]\r\n```\r\n\r\n\"testDeleteCorruptReplicaForUnderReplicatedBlockInternal\" is an existing method \"testDeleteCorruptReplicaForUnderReplicatedBlock\" which was renamed. Since the method was already merged I don't think its necessary that I reduce the number of lines in the method.\r\n",
        "url": "https://github.com/apache/hadoop/pull/4568#issuecomment-1204496373"
    },
    {
        "project": "apm-agent-java",
        "id": 1196311992,
        "body": "> But since it is a snapshot version of the agent, would that mean that it is a modified build?\r\n\r\nIt is, but it's not how an older ASM was build into it. It is the snapshot built from this PR in order to properly print this stack trace.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2720#issuecomment-1196311992"
    },
    {
        "project": "spring-boot",
        "id": 1109875843,
        "body": "@garyrussell Ok. I'll add the commits for the classes split to this PR in the following days",
        "url": "https://github.com/spring-projects/spring-boot/pull/30567#issuecomment-1109875843"
    },
    {
        "project": "druid",
        "id": 1162659924,
        "body": "@clintropolis, thanks for the history of parameters. I reread the links that you generously provided: very helpful. @gianm your advice to look into the JDBC (Avatica) code path was wise: doing so was insightful.\r\n\r\nIt turns out that Druid's implementation of the JDBC protocol is a bit off, which may have led to some of the confusion around when to prepare and when to expect parameters. Issue #12682 spells out the gaps. In particular, the implementation combines the ideas of JDBC `Statement`, `PreparedStatement` and `ResultSet` into one `DruidStatement`. In addition to being non-compliant with the standard, it cause ambiguity around parameters. So, PR #12709 fixes that issue.\r\n\r\nPR #12708 handles the Calcite planner change. The prior `SqlLifecycle` refactoring was backed out of this PR so that this one is a bit more bite-sized. Once this one is done, we can move onto the \"handler\" concept from PR #12637 and the lifecycle refactoring previously in this PR.",
        "url": "https://github.com/apache/druid/pull/12636#issuecomment-1162659924"
    },
    {
        "project": "apm-agent-java",
        "id": 996771985,
        "body": "What is really weird here is that this byte buddy artifact should have been ignored thanks to the changes introduced in https://github.com/elastic/apm-agent-java/pull/2252.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2339#issuecomment-996771985"
    },
    {
        "project": "spring-security",
        "id": 870835927,
        "body": "I've also split this change into two commits so we can cherry-pick it in other branches easier, like #9846 ",
        "url": "https://github.com/spring-projects/spring-security/pull/10011#issuecomment-870835927"
    },
    {
        "project": "hadoop",
        "id": 541394645,
        "body": "\nThank you very much to open this pull request.\n\nDuring the weekend the Ozone source code has been moved out from [apache/hadoop](https://github.com/apache/hadoop) repository to [apache/hadoop-ozone](https://github.com/apache/hadoop-ozone) repository.\n\nThis git commits are rewritten, but the branch of this pull request is also transformed (state of Saturday morning), you can use the new, migrated branch to recreate this pull request.\n\nYour pull request is important for us: **Can you please re-create your pull request in the new repository?**\n\n**1**. Create a new fork of https://github.com/apache/hadoop-ozone\n\n**2**. Clone it and have both your fork and the apache repo as remotes:\n\n```\ngit clone git@github.com:bharatviswa504/hadoop-ozone.git\ncd hadoop-ozone\ngit remote add apache git@github.com:apache/hadoop-ozone.git\ngit fetch apache\n```\n\n**3**. Fetch your migrated branch and push it to your fork.\n\n```\ngit checkout -b HDDS-2278 apache/HDDS-2278\ngit push origin HDDS-2278\n```\n\n**4**. And create the new pull request on the new repository.\n\nhttps://github.com/apache/hadoop-ozone/compare/master...bharatviswa504:HDDS-2278?expand=1\n\nIf you need more information, please check [this](https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Ozone+source+tree+split) wiki page or contact with me (my github user name + apache.org).\n\nThank you, and sorry for the inconvenience.",
        "url": "https://github.com/apache/hadoop/pull/1643#issuecomment-541394645"
    },
    {
        "project": "spring-boot",
        "id": 1121675393,
        "body": "> Thanks for the PR, @almogtavor. A few general comments:\r\n> \r\n> * I'm a bit confused by the apparent overlap or at least similarity between some of the existing properties and those that are being proposed here. For example `spring.kafka.consumer.auto-commit-interval` and `spring.reactor.kafka.receiver.commit-interval`, `spring.kafka.properties.*` and `spring.reactor.kafka.properties.*`\r\n> * Given that there is no `spring.reactor.kafka.consumer.properties.*`, there seems to be an imbalance between `spring.reactor.kafka.properties.*` and `spring.reactor.kafka.sender.properties.*`\r\n> * Given the reuse of the `spring.kafka.producer.*` and `spring.kafka.consumer.*` properties to configure Reactor Kafka, would `spring.kafka.reactor` be a better prefix as it brings the property names closer together?\r\n> \r\n> I think things would benefit from taking a small step back and considering exactly how we want the receiver and sender options to be configured. Once that's been established, we can then map that into properties classes and auto-configuration.\r\n\r\n@wilkinsona I will reply w/ what I see in regards to the overlap of the properties and also stepping back and thinking about how we want to configure this thing....\r\n\r\n> \u2139\ufe0f  I talk mostly of Receiver but the Sender follows the same vein of thought...\r\n\r\nSo, the pattern is that all of the properties in the \"ReceiverOptions.properties\" map and the key and value serializer property\r\nare passed into the KafkaConsumer. \r\n\r\n```java\r\npublic <K, V> Consumer<K, V> createConsumer(ReceiverOptions<K, V> config) {\r\n    return new KafkaConsumer<>(config.consumerProperties(),\r\n                               config.keyDeserializer(),\r\n                               config.valueDeserializer());\r\n}\r\n```\r\nThese are the only \"true\" overlapping ones. All of the other options are used outside of the `KafkaConsumer` in the reactive control layer. You can see the usage in [ConsumerEventLoop](https://github.com/reactor/reactor-kafka/blob/main/src/main/java/reactor/kafka/receiver/internals/ConsumerEventLoop.java#L117).\r\n\r\nI am guessing they chose not to keep the key/value consumer property map entry and the 1st class key/value serde properties in sync since they are both passed into the KafkaConsumer constructor. We could do the same, or sync them if we want. \r\n\r\nI remember when we I used `ReactorKafka` in the past, it is confusing as to what \"levers\" to configure (the native Kafka consumer ones or the ReactorKafka ones) to get the behavior you want. The ref docs for ReactorKafka do talk about this IIRC. I think we should stay out of that business and simply let the 1st class options properties exist where they do and not try to consolidate/reconcile for example w/ the \r\n`spring.kafka.consumer.auto-commit-interval and spring.reactor.kafka.receiver.commit-interval`.\r\n\r\n### Possible direction\r\n\r\nTo summarize what I think I am hearing and also what I think would be a good direction: \r\n\r\n- Prefix the RKP as 'spring.kafka.reactor'\r\n- Do not leverage the KP class in the RKP any longer\r\n- Break out Consumer/Producer properties into own class (leverage `NestedConfigurationProperty`) in KP and RKP\r\n- Add buildConsumer/ProducerProeprties in RKP to get common props such as bootstrap etc..\r\n- Mirror the consumer/producer properties directly as done in the receiver/sender options (aka don't try to reconcile similiar properties)\r\n\r\nTradeoff of the split approach is that the consumer properties need to be mapped multiple times if someone is\r\nusing both normal and reactive Kafka. I would think the \"levers\" would be slightly different between them anyways though.\r\n\r\nAn example yaml would look like:\r\n```yaml\r\nspring:\r\n  kafka:\r\n    bootstrap-servers: localhost:9092   \r\n    consumer:\r\n      auto-offset-reset: earliest\r\n      auto-commit-interval: 5s\r\n    reactive:\r\n      consumer:\r\n        auto-offset-reset: latest\r\n        auto-commit-interval: 3s\r\n```\r\n\r\nThoughts? ",
        "url": "https://github.com/spring-projects/spring-boot/pull/30567#issuecomment-1121675393"
    },
    {
        "project": "pulsar",
        "id": 1201512995,
        "body": "Well. I fixed this issue locally. Here is a patch you can make use of:\r\n\r\n```patch\r\ndiff --git a/pom.xml b/pom.xml\r\nindex a6ce94fe734..2d093f1f1eb 100644\r\n--- a/pom.xml\r\n+++ b/pom.xml\r\n@@ -95,7 +95,8 @@ flexible messaging model and an intuitive client API.</description>\r\n     <test.additional.args>\r\n       --add-opens java.base/jdk.internal.loader=ALL-UNNAMED\r\n       --add-opens java.base/java.lang=ALL-UNNAMED <!--Mockito-->\r\n-      --add-opens java.base/java.io=ALL-UNNAMED <!--Bookkeeper NativeIO -->\r\n+      --add-opens java.base/java.io=ALL-UNNAMED <!--Bookkeeper NativeIO-->\r\n+      --add-opens java.base/java.util=ALL-UNNAMED <!--System Lambda-->\r\n       --add-opens java.base/sun.net=ALL-UNNAMED <!--netty.DnsResolverUtil-->\r\n       --add-opens java.management/sun.management=ALL-UNNAMED <!--JvmDefaultGCMetricsLogger-->\r\n     </test.additional.args>\r\n@@ -1294,6 +1295,13 @@ flexible messaging model and an intuitive client API.</description>\r\n   </dependencyManagement>\r\n \r\n   <dependencies>\r\n+    <dependency>\r\n+      <groupId>com.github.stefanbirkner</groupId>\r\n+      <artifactId>system-lambda</artifactId>\r\n+      <version>1.2.1</version>\r\n+      <scope>test</scope>\r\n+    </dependency>\r\n+\r\n     <!-- These dependencies are common to all the submodules -->\r\n     <dependency>\r\n       <groupId>org.apache.pulsar</groupId>\r\ndiff --git a/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java b/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java\r\nindex 8dbc880fa16..22ef2dd9e60 100644\r\n--- a/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java\r\n+++ b/pulsar-functions/secrets/src/test/java/org/apache/pulsar/functions/secretsprovider/EnvironmentBasedSecretsProviderTest.java\r\n@@ -21,11 +21,7 @@ package org.apache.pulsar.functions.secretsprovider;\r\n \r\n import static org.testng.Assert.assertEquals;\r\n import static org.testng.Assert.assertNull;\r\n-\r\n-import java.util.HashMap;\r\n-import java.util.Map;\r\n-\r\n-import org.powermock.reflect.Whitebox;\r\n+import com.github.stefanbirkner.systemlambda.SystemLambda;\r\n import org.testng.annotations.Test;\r\n \r\n public class EnvironmentBasedSecretsProviderTest {\r\n@@ -33,22 +29,8 @@ public class EnvironmentBasedSecretsProviderTest {\r\n     public void testConfigValidation() throws Exception {\r\n         EnvironmentBasedSecretsProvider provider = new EnvironmentBasedSecretsProvider();\r\n         assertNull(provider.provideSecret(\"mySecretName\", \"Ignored\"));\r\n-        injectEnvironmentVariable(\"mySecretName\", \"SecretValue\");\r\n-        assertEquals(provider.provideSecret(\"mySecretName\", \"Ignored\"), \"SecretValue\");\r\n-    }\r\n-\r\n-    private static void injectEnvironmentVariable(String key, String value)\r\n-            throws Exception {\r\n-\r\n-        Class<?> processEnvironment = Class.forName(\"java.lang.ProcessEnvironment\");\r\n-        Map<String,String> unmodifiableMap = new HashMap<>(Whitebox\r\n-                .getInternalState(processEnvironment, \"theUnmodifiableEnvironment\"));\r\n-        unmodifiableMap.put(key, value);\r\n-        Whitebox.setInternalState(processEnvironment, \"theUnmodifiableEnvironment\", unmodifiableMap);\r\n-\r\n-        Map<String,String> envMap = new HashMap<>(Whitebox\r\n-                .getInternalState(processEnvironment, \"theEnvironment\"));\r\n-        envMap.put(key, value);\r\n-        Whitebox.setInternalState(processEnvironment, \"theEnvironment\", envMap);\r\n+        SystemLambda.withEnvironmentVariable(\"mySecretName\", \"SecretValue\").execute(() -> {\r\n+            assertEquals(provider.provideSecret(\"mySecretName\", \"Ignored\"), \"SecretValue\");\r\n+        });\r\n     }\r\n }\r\n```\r\n\r\nI guess the reason is that `Whitebox` failed to set the unmodifiableMap with log4j2 2.18.0 while I don't have an idea how it happens.\r\n\r\nI don't like powermock as it's lack of maintenance. It seems all usage of powermock is `Whitebox`. We can get rid of it with simple reflections. This can be a separated issue, though.\r\n\r\nDebugging details:\r\n\r\n```\r\nBefore\r\n2022-08-02T01:00:16,796+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@46 - 1 System.getenv(): 120752794 {}\r\n2022-08-02T01:00:17,932+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@52 - 2 unmodifiableMap: 901581632 {}\r\n2022-08-02T01:00:17,932+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@53 - 2 System.getenv(): 901581632 {}\r\n\r\nAfter\r\n2022-08-02T01:00:47,197+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@46 - 1 System.getenv(): 120750803 {}\r\n2022-08-02T01:00:47,203+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@52 - 2 unmodifiableMap: 901579641 {}\r\n2022-08-02T01:00:47,203+0800 ERROR [main] o.a.p.f.s.EnvironmentBasedSecretsProviderTest@53 - 2 System.getenv(): 120750803 {}\r\n```\r\n\r\nAs you can see, \"After\" we failed to set the env map. But I don't know how it happens exactly.",
        "url": "https://github.com/apache/pulsar/pull/16884#issuecomment-1201512995"
    },
    {
        "project": "flink",
        "id": 1195637892,
        "body": "> It's very common for deserializers to include an implementation of getProducedType. [...] why don't we need that here?\r\n\r\nWe extend the `AbstractDeserializationSchema` which takes care of that based on the class/typeinfo passed to the constructor.",
        "url": "https://github.com/apache/flink/pull/20355#issuecomment-1195637892"
    },
    {
        "project": "flink",
        "id": 1204559117,
        "body": "Thanks a lot for the fix @fredia \r\n\r\nCould you please briefly describe the root cause of the failure in the ticket?\r\n\r\nBesides of changing the source, this PR refactors the test setup.\r\nIdeally, refactoring should be a separate commit (with [hotfix] or [refactor] label). If it's a part of the fix, the message shouldn't be \"refactor\".\r\nIn either case, could you please explain why the refactoring is necessary (e.g. in the commit message)?\r\n\r\nI'm also wondering whether `PeriodicMaterialization` part in test name is still relevant; if not, probably this PR is a good place to rename it. WDYT?",
        "url": "https://github.com/apache/flink/pull/20404#issuecomment-1204559117"
    },
    {
        "project": "hadoop",
        "id": 780921454,
        "body": "I am hardly able to follow your thoughts.\r\nI am not familiar with the module hierarchy - so please feel free to rewrite the source code yourself.\r\n\r\nThere is no specific JNI stuff, for os-maven-plugin, which would depend on your environment. Properly identified architecture allows to pick up the latest version of the artifact - but that should be enough.",
        "url": "https://github.com/apache/hadoop/pull/2700#issuecomment-780921454"
    },
    {
        "project": "spring-framework",
        "id": 725438912,
        "body": "Good catch, the javadoc is definitely out of date now. However, with respect to the implementation, the use of the empty list constant is intentional since it reduces the memory footprint for the common empty case... the next best thing we can do after using the `InjectionMetadata.EMPTY` constant itself.",
        "url": "https://github.com/spring-projects/spring-framework/pull/26070#issuecomment-725438912"
    },
    {
        "project": "hadoop",
        "id": 1201827298,
        "body": "Thanks @goiri @slfan1989 for your review.\r\n\r\n- First, I will remove some code that has nothing to do with lambdas\r\n- Second, I will split this PR at the subPackage Level, such as: `org.apache.hadoop.hdfs.server.namenode`, `org.apache.hadoop.hdfs.server.namenode.ha`",
        "url": "https://github.com/apache/hadoop/pull/4668#issuecomment-1201827298"
    },
    {
        "project": "apm-agent-java",
        "id": 1117089988,
        "body": "> tag builds are removed on the main pipeline\r\n\r\nThis wont' be immediately done, the automation takes a few minutes to apply the new changes. In any case, tags will be gone\r\n ",
        "url": "https://github.com/elastic/apm-agent-java/pull/2612#issuecomment-1117089988"
    },
    {
        "project": "druid",
        "id": 1204489700,
        "body": "@petermarshallio, thanks for the improvements!\r\n\r\nLGTM (non-binding)",
        "url": "https://github.com/apache/druid/pull/12843#issuecomment-1204489700"
    },
    {
        "project": "hadoop",
        "id": 1200156797,
        "body": "@slfan1989 Thanks for your review.\r\n\r\n> I feel that this change is a bit risky, will this lead to instability of the service? Sometimes it is reasonable to configure without timeout\r\n\r\nSorry, I didn't get your idea. Can you share more detailed information or cases?\r\n\r\n> The main question is is it reasonable to use the same timeout configuration for different protocols?\r\n\r\nAccording to my practical experience, using one configuration `ipc.client.rpc-timeout.ms` is enough. Of course, if some masters feel it is necessary to distinguish this configuration, I will modify this patch.\r\n",
        "url": "https://github.com/apache/hadoop/pull/4660#issuecomment-1200156797"
    },
    {
        "project": "apm-agent-java",
        "id": 861339262,
        "body": "I thought that having the \"log once\" logger disable all log levels once it has already been used would have helped reuse it.\r\n\r\nHowever, when trying to use it with `co.elastic.apm.agent.servlet.ServletVersionInstrumentation` that does not work as two log statements are issued, and that might introduce a small concurrency bug as \"checking the logger state\" and \"issuing log statement\" are not done in an atomic operation.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1863#issuecomment-861339262"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1024725584,
        "body": "Do we usually put major deprecation information (#1845) into the release notes? Or is the breaking changes section enough?",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1892#issuecomment-1024725584"
    },
    {
        "project": "spring-framework",
        "id": 436157144,
        "body": "Thanks for the feedback. I am not keen to copy/paste the setup instruction there so I've added a link and restructured that section a bit  3232825. Thanks for the PR in any case.",
        "url": "https://github.com/spring-projects/spring-framework/pull/2009#issuecomment-436157144"
    },
    {
        "project": "apm-agent-java",
        "id": 636855487,
        "body": "@felixbarny I've tried to create a test case reproducing the issue on my test-application but until now, unsuccessfully.\r\n\r\nI was able to narrow it down to the exact place where it loses context: https://github.com/milanvdm/scala-elastic-apm/blob/check-edge-case/src/main/scala/me/milan/main/future/DummyHttpBackend.scala#L76\r\n\r\nI've tried to recreate that logic using only Future, but without the AsyncHttpClient it seems to be tricky.\r\n\r\nI'll spend some time trying to make the setup as small as possible to make it easier to track.\r\n\r\nFrom my initial testing, it seems directly connected to the usage of the AsyncHttpClient.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1048#issuecomment-636855487"
    },
    {
        "project": "skywalking",
        "id": 1108648966,
        "body": "> Ww discussed about this before, 2 things\r\n> \r\n> 1. Use this setup means you have known what to do, and what does this setup mean\r\n> 2. Use the first IP address doesn't change the fact it still could be wrong. A little approvement, but not much\r\n> \r\n> And from the code perspective, if this is going to be accepted, we have to change all cluster manager implementations, besides k8s as it is automatically working.\r\n\r\nThe automatic setting solves the problem in most cases, in a few cases it can be set manually, you are correct, need to change all cluster manager implementations, I didn't realize this at first. It is recommended that the official make a unified improvement on this issue.",
        "url": "https://github.com/apache/skywalking/pull/8945#issuecomment-1108648966"
    },
    {
        "project": "apm-agent-java",
        "id": 982448801,
        "body": "So, after a discussion with @eyalkoren, merging this PR does not seem the right thing to do for the following reasons:\r\n- the `host.hostname` field that was used by Kibana is deprecated and should not be relied on\r\n- the impact on users is quite minimal, it only shows `N/A` for hostname in one place, and it does not breaks any product feature\r\n- changing this behavior at agents level implies to modify the spec + ensure that all agents comply to it (thus quite some overhead)\r\n- apm server implementation relies on `detected_hostname` to be optional, there is no usage of sending both `configured_hostname` and `detected_hostname` as the detected one is ignored when one is explicitly configured.\r\n- `detected_hostname` relies on external commands execution, which make them depend on OS and platform, we have some heuristics to avoid FQDN, both of which are likely to be unreliable and require a simple work-around if they fail. With current implementation setting `configured_hostname` allows to provide this work-around in case hostname detection does not behave as expected.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2295#issuecomment-982448801"
    },
    {
        "project": "spring-boot",
        "id": 964389122,
        "body": "Thanks for the PR, but I don't think we should replace all instances with though. Some of the replacements don't read as well.",
        "url": "https://github.com/spring-projects/spring-boot/pull/28584#issuecomment-964389122"
    },
    {
        "project": "apm-agent-java",
        "id": 957873166,
        "body": "> Please rename the tests extending `AbstractRedisInstrumentationTest` so that they end with `IT` (integration test). This makes them not run when doing a `mvn test`, only a `mvn verify`.\r\n> \r\n> I don't think we need to test against a specific Redis server version.\r\n\r\nDone",
        "url": "https://github.com/elastic/apm-agent-java/pull/2221#issuecomment-957873166"
    },
    {
        "project": "flink",
        "id": 1178478120,
        "body": "This PR includes UI changes regarding the [Completed Jobs Information Enhancement FLIP](https://cwiki.apache.org/confluence/display/FLINK/FLIP-241%3A+Completed+Jobs+Information+Enhancement). Could @simplejason help review this? Thanks!",
        "url": "https://github.com/apache/flink/pull/20209#issuecomment-1178478120"
    },
    {
        "project": "skywalking",
        "id": 1129675439,
        "body": "Your call, I could merge this first if you like.",
        "url": "https://github.com/apache/skywalking/pull/9064#issuecomment-1129675439"
    },
    {
        "project": "hadoop",
        "id": 934619131,
        "body": "> On the other hand,Pushing those changes after a major milestone would mean only one Jira and one PR (i.e., HADOOP-XYZ: migrating banned imports to use restrict-imports-enforcer-rule).\r\n\r\nIMHO the problem here is that if someone has already pushed a couple of commits with illegal imports by the time we are done with all modules, then we will have to take care of such individual modules violating the rule (by replacing the respective imports), it's kind of rework right?",
        "url": "https://github.com/apache/hadoop/pull/3503#issuecomment-934619131"
    },
    {
        "project": "flink",
        "id": 1089823724,
        "body": "> @mayuehappy, thanks for your contribution to reduce the frequency of the timer registration. I left the minor comments for your changes. BTW, @mayuehappy, could you fix the typo in the tile, which is `impove`->`improve`? cc @dawidwys, could you please take a look at this improvement?\r\n\r\n@SteNicholas Sorry for the late reply , i've your comment and made some replies. PTAL, thx.",
        "url": "https://github.com/apache/flink/pull/19259#issuecomment-1089823724"
    },
    {
        "project": "spring-security",
        "id": 537201250,
        "body": "> Thanks for the PR @AndreasKl.\r\n> \r\n> There is quite a bit of coupling in the `OAuth2AuthorizationRequestResolver` and `AuthorizationRequestRepository` and it's implementations (and Reactive). I feel we should rework things to remove this coupling and potentially allow for reuse in other areas within the OAuth support, eg. expiring PK from a JWK Cache.\r\n> \r\n> The expiring capability is really a cross-cutting concern. Can you try the following solution?\r\n> \r\n>     * Intercept calls to `loadAuthorizationRequest()` and store the information necessary\r\n> \r\n>     * At a scheduled time (configurable with skew), the component will attempt to `loadAuthorizationRequest()` and if it returns than determine if time has expired and than `removeAuthorizationRequest`\r\n> \r\n> \r\n> Please take a look at the [Cache abstraction](https://docs.spring.io/spring/docs/current/spring-framework-reference/integration.html#cache) to see if it would meet our needs.\r\n\r\nIf you have time could you have a look at: https://github.com/spring-projects/spring-security/issues/5145#issuecomment-528597139 . I'm now even more convinced that a size based solution could be a better match; we see a few `OAuth2AuthorizationRequest` that exists for over 10 minutes on our production system.\r\n\r\nOff the top of my head, I think the cache abstraction would impose a quite large runtime requirement to the user of spring security. The user than has to deploy a distributed cache for be able to prevent what is effectively a way to cause an OOM in an application.\r\n\r\nI definitely see the need for reducing the duplication and fully understand that `OAuth2AuthorizationRequest` should not contain a `createdAt` or a `expiresAt`, I'll look into other options to store the data and generalize the clean up.",
        "url": "https://github.com/spring-projects/spring-security/pull/7381#issuecomment-537201250"
    },
    {
        "project": "spring-security",
        "id": 668693648,
        "body": "> Can you expand on what you believe the problem is? This PR breaks the build.\r\n\r\nWhen working with legacy code it is common to have the salt and hash in separate columns of the database. The salt used to decode is randomly generated and does not include the forced curly brackets which the MessageDigestPasswordEncoder enforces.\r\n\r\nThe expected behaviour would be to pass a string like \"{salt}hash\" and have the MessageDigestPasswordEncoder use \"salt\" as the salt and not \"{salt}\" since the hash was generated with \"salt\".\r\n\r\nThis is the same problem encountered in #6594 \r\n\r\nThe questioning the issue is: what is the point in having a backwards compatible algorithm if it cannot be applied to old data.\r\n\r\nI marked this as draft since I decided to rewrite the class in my project and didn't get around finding a general solution. \r\n",
        "url": "https://github.com/spring-projects/spring-security/pull/8878#issuecomment-668693648"
    },
    {
        "project": "apm-agent-java",
        "id": 1030853364,
        "body": "I actually intend to get rid of `TraceContext#applicationClassLoader` as part of #2428 ...\r\nWe don't need this reference anymore and it's always a good idea to store one less weak reference (and any CL reference).\r\nSince you are actually only interested in the implicit side-effect of `TraceContext#setApplicationClassLoader`, which is to override the service info for the transaction, the API seems inappropriate to me. Calling it `overrideServiceInfoFor(ClassLoader)` would be better (the javadoc would how the provided `ClassLoader` is used).\r\n\r\nGenerally speaking, I would prefer a cleaner and more generic API such as `overrideServiceInfo(String serviceName, String serviceVersion)`. If extracting the service version and name are simple enough- just implement that instead of relying on the automatic discovery. If relying on the automatic discovery is important, I assume this is mostly relevant for external plugins (not for other usages of the API), so we can expose `ElasticApmTracer#getServiceInfo(ClassLoader)` through the sdk by making it part of the `co.elastic.apm.agent.impl.Tracer` interface. WDYT?",
        "url": "https://github.com/elastic/apm-agent-java/pull/2444#issuecomment-1030853364"
    },
    {
        "project": "spring-framework",
        "id": 1023200095,
        "body": "Hi @ttddyy,\r\n\r\nThat example was intended to be a proof of concept.\r\n\r\nHere's a fully working test class for your `DisabledOnNativeCondition`.\r\n\r\n```java\r\nimport java.lang.reflect.AnnotatedElement;\r\nimport java.lang.reflect.Constructor;\r\nimport java.util.Optional;\r\n\r\nimport org.junit.jupiter.api.AfterEach;\r\nimport org.junit.jupiter.api.BeforeEach;\r\nimport org.junit.jupiter.api.Test;\r\nimport org.junit.jupiter.api.extension.ConditionEvaluationResult;\r\nimport org.junit.jupiter.api.extension.ExecutionCondition;\r\nimport org.junit.jupiter.api.extension.ExtensionContext;\r\n\r\nimport static org.assertj.core.api.Assertions.assertThat;\r\nimport static org.assertj.core.api.InstanceOfAssertFactories.STRING;\r\nimport static org.mockito.Mockito.mock;\r\nimport static org.mockito.Mockito.when;\r\n\r\n/**\r\n * Unit tests for {@link DisabledOnNativeCondition}.\r\n *\r\n * @author Sam Brannen\r\n */\r\nclass DisabledOnNativeConditionTests {\r\n\r\n\tprivate static final String IMAGECODE_PROPERTY_NAME = \"org.graalvm.nativeimage.imagecode\";\r\n\tprivate static final String NATIVE_DETECTOR_CLASS_NAME = \"org.springframework.core.NativeDetector\";\r\n\tprivate static final String DISABLED_ON_NATIVE_CONDITION_CLASS_NAME = \"org.springframework.core.DisabledOnNativeCondition\";\r\n\r\n\r\n\t@BeforeEach\r\n\tvoid preconditions() {\r\n\t\tassertThat(NativeDetector.inNativeImage()).isFalse();\r\n\t}\r\n\r\n\t@AfterEach\r\n\tvoid postconditions() {\r\n\t\tSystem.clearProperty(IMAGECODE_PROPERTY_NAME);\r\n\t\tassertThat(NativeDetector.inNativeImage()).isFalse();\r\n\t}\r\n\r\n\t@Test\r\n\tvoid enabledOnNativeImageWhenImageCodePropertyIsNotSet() throws Exception {\r\n\t\tSystem.clearProperty(IMAGECODE_PROPERTY_NAME);\r\n\r\n\t\tConditionEvaluationResult result = getConditionEvaluationResult();\r\n\t\tassertThat(result).isNotNull();\r\n\t\tassertThat(result.isDisabled()).isFalse();\r\n\t\tassertThat(result.getReason()).get().asInstanceOf(STRING).matches(\"^.+ is enabled on non native image$\");\r\n\t}\r\n\r\n\t@Test\r\n\tvoid disabledOnNativeImageWhenImageCodePropertyIsSet() throws Exception {\r\n\t\tSystem.setProperty(IMAGECODE_PROPERTY_NAME, \"test\");\r\n\r\n\t\tConditionEvaluationResult result = getConditionEvaluationResult();\r\n\t\tassertThat(result).isNotNull();\r\n\t\tassertThat(result.isDisabled()).isTrue();\r\n\t\tassertThat(result.getReason()).get().asInstanceOf(STRING).matches(\"^.+ is disabled on native image$\");\r\n\t}\r\n\r\n\tprivate ConditionEvaluationResult getConditionEvaluationResult() throws Exception {\r\n\t\tTestClassLoader testClassLoader = new TestClassLoader(getClass().getClassLoader());\r\n\t\tClass<?> disbledOnNativeConditionClass = testClassLoader.loadClass(DISABLED_ON_NATIVE_CONDITION_CLASS_NAME, false);\r\n\t\tConstructor<?> constructor = disbledOnNativeConditionClass.getDeclaredConstructor();\r\n\t\tconstructor.setAccessible(true);\r\n\t\tExecutionCondition condition = (ExecutionCondition) constructor.newInstance();\r\n\r\n\t\tExtensionContext context = mock(ExtensionContext.class);\r\n\t\twhen(context.getElement()).thenReturn(Optional.of(mock(AnnotatedElement.class)));\r\n\r\n\t\treturn condition.evaluateExecutionCondition(context);\r\n\t}\r\n\r\n\r\n\tprivate static class TestClassLoader extends OverridingClassLoader {\r\n\r\n\t\tTestClassLoader(ClassLoader parent) {\r\n\t\t\tsuper(parent);\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tprotected boolean isEligibleForOverriding(String className) {\r\n\t\t\treturn NATIVE_DETECTOR_CLASS_NAME.equals(className) ||\r\n\t\t\t\t\tDISABLED_ON_NATIVE_CONDITION_CLASS_NAME.equals(className);\r\n\t\t}\r\n\r\n\t}\r\n\r\n}\r\n```\r\n\r\nFeel free to use that (or something based on that) in your PR for Spring Native.",
        "url": "https://github.com/spring-projects/spring-framework/pull/27979#issuecomment-1023200095"
    },
    {
        "project": "spring-security",
        "id": 558142673,
        "body": "Hi,\r\n\r\n(not sure where i should post this comment... so i put it here)\r\n\r\nI had to change some code of the class to make it work with my use case.\r\n\r\n# How i tested this PR (maybe wrongly?)\r\n\r\n(in my code, i already renamed `ReactiveOAuth2AuthorizedClientServiceReactiveOAuth2AuthorizedClientManager` to `OAuth2AuthorizedClientServiceReactiveOAuth2AuthorizedClientManager`)\r\n```java\r\n    ReactiveClientRegistrationRepository clientRegistrationRepository = new InMemoryReactiveClientRegistrationRepository(clientRegistration);\r\n    ReactiveOAuth2AuthorizedClientService authorizedClientService = new InMemoryReactiveOAuth2AuthorizedClientService(clientRegistrationRepository);\r\n\r\n    OAuth2AuthorizedClientServiceReactiveOAuth2AuthorizedClientManager authorizedClientManager =\r\n        new OAuth2AuthorizedClientServiceReactiveOAuth2AuthorizedClientManager(\r\n                clientRegistrationRepository,\r\n                authorizedClientService\r\n        );\r\n    authorizedClientManager.setReactiveOAuth2AuthorizedClientProvider(new SpecificPartnerReactiveOAuth2AuthorizedClientProvider());\r\n\r\n```\r\n\r\nThe current implementation does not save the authorized client into the repository.\r\n\r\n# The code i changed to make it work\r\n\r\nI changed\r\n```java\r\n.doOnNext(x -> reactiveAuthorizedClientService.saveAuthorizedClient(x, principal))\r\n```\r\nto\r\n```java\r\n.flatMap(authClient -> reactiveAuthorizedClientService.saveAuthorizedClient(authClient, principal).thenReturn(authClient))\r\n```\r\n\r\nRegards",
        "url": "https://github.com/spring-projects/spring-security/pull/7589#issuecomment-558142673"
    },
    {
        "project": "apm-agent-java",
        "id": 1083632407,
        "body": "> @bmorelli25 The build docs are failing now because I added a back link to the Lambda extension {apm-guide-ref}/aws-lambda-arch.html. Will this resolve when https://github.com/elastic/apm-aws-lambda/pull/158 is merged or is something in addition needed (e.g. cherry picking like you did for {apm-guide-ref}/aws-lambda-config-options.html) ?\r\n\r\nThat's correct, a cherry-pick to 8.1 is needed. I'll update.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2556#issuecomment-1083632407"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 34330232,
        "body": "Officially closing the issue. Master and 1.3 M2 supports `fields` for 0.90 branch and the newly introduced `_source` for 1.x.\n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/54#issuecomment-34330232"
    },
    {
        "project": "spring-boot",
        "id": 1162982520,
        "body": "Wouldn't it make more sense to modify the [`StreamUtils.copy`](NoHandlerFoundException) method instead? And update the `BUFFER_SIZE` to the same default as used inside the JDK (which is `8192` double of what `StreamUtils` is using)?",
        "url": "https://github.com/spring-projects/spring-boot/pull/31494#issuecomment-1162982520"
    },
    {
        "project": "spring-boot",
        "id": 1161823058,
        "body": "Hi @mhalbritter thanks for review, I replaced the` CompletableFuture` implementation with a normal one. I will wait for you to check about the WebClient initialization. My logic was that `@ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.REACTIVE)` in `ZipkinConfigurations` should mean that Netty should be on the classpath when true. I could be wrong though.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30792#issuecomment-1161823058"
    },
    {
        "project": "pulsar",
        "id": 766915396,
        "body": "@eolivelli \r\n\r\nI think the root cause is a bit different. Seems like your workflow/job pattern is different than many other ASF projects, you are simply launching MANY workflows and jobs.\r\n\r\nHave  you considered merging those workflows into smaller number of them ? I think actually the many, many workflows launched by pulsar are the reason for many ASF projects to get blocked queue.\r\n\r\nSee this chart:\r\n\r\n![06a6b973-fee9-4651-a197-a275a39b6f55](https://user-images.githubusercontent.com/595491/105729417-7cfc2b00-5f2d-11eb-9da6-69c7b9d5904f.png)\r\n\r\nI am happy to help Pulsar to redesign their CI to be a bit more friendlier towards other projects? I have quite an experience with that - in Apache Airflow I introduced changes that brought the usage of Github Actions by 70% maintaining it's usefulness and I am pretty sure the way you are using GitHub Actions can be optimised. Super Happy to help with that if needed.\r\n\r\nYou can also see the discussions we had about it at the ASF discussion lists recently. I thik this thread from the last weekend describes the current state of affairs rather well: https://lists.apache.org/thread.html/r6130d6d111217b8a90b3b90bbb3029b75b5e2a4b5463f8d2c8f130f6%40%3Cusers.infra.apache.org%3E  (you need to sign-up to users@infra.apache.org to view it).\r\n\r\n\r\nDo you think this can be done ?\r\n\r\n\r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/9159#issuecomment-766915396"
    },
    {
        "project": "spring-framework",
        "id": 575222675,
        "body": "Thanks for the pull request. I've updated it a bit, renaming the property from `proxyPing` to `handlePing`.",
        "url": "https://github.com/spring-projects/spring-framework/pull/24367#issuecomment-575222675"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 531973276,
        "body": "The change isn't completely noop for `master` and other development branches.\r\n\r\nThe docs currently append `-SNAPSHOT` (e.g. 8.0.0-SNAPSHOT) to the version for development branches. While I can use an ifeval to recreate this, I feel that using only the version number (e.g. 8.0.0) is more consistent with other Elastic docs.\r\n\r\nThis change would remove the appended `-SNAPSHOT`, except for in the Development Builds section of the Installation docs:\r\nhttps://www.elastic.co/guide/en/elasticsearch/hadoop/master/install.html#download-dev\r\n\r\nI've included the output diffs below.\r\n\r\n### HTML Output Diffs\r\n\r\nhttps://www.elastic.co/guide/en/elasticsearch/hadoop/master/requirements.html\r\n\r\n```diff\r\n@@ -14 +14 @@\r\n-java version \"1.8.0_45\"</pre></div></div><div class=\"section\"><div class=\"titlepage\"><div><div><h2 class=\"title\"><a id=\"requirements-es\"></a>Elasticsearch<a href=\"https://github.com/elastic/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/intro/requirements.adoc\" class=\"edit_me\" title=\"Edit this page on GitHub\" rel=\"nofollow\">edit</a></h2></div></div></div><p>We <span class=\"strong strong\"><strong>highly</strong></span> recommend using the latest Elasticsearch (currently 8.0.0-SNAPSHOT). While elasticsearch-hadoop maintains backwards compatibility\r\n+java version \"1.8.0_45\"</pre></div></div><div class=\"section\"><div class=\"titlepage\"><div><div><h2 class=\"title\"><a id=\"requirements-es\"></a>Elasticsearch<a href=\"https://github.com/elastic/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/intro/requirements.adoc\" class=\"edit_me\" title=\"Edit this page on GitHub\" rel=\"nofollow\">edit</a></h2></div></div></div><p>We <span class=\"strong strong\"><strong>highly</strong></span> recommend using the latest Elasticsearch (currently 8.0.0). While elasticsearch-hadoop maintains backwards compatibility\r\n```\r\n\r\n```diff\r\n@@ -17 +17 @@ find a matrix of supported versions <a class=\"ulink\" href=\"https://www.elastic.c\r\n-elasticsearch-8.0.0-SNAPSHOT</pre></div><p>If Elasticsearch is running (locally or remotely), one can find out its version through REST:</p><div class=\"pre_wrapper lang-js\"><pre class=\"programlisting prettyprint lang-js\">$ curl -XGET http://localhost:9200\r\n+elasticsearch-8.0.0</pre></div><p>If Elasticsearch is running (locally or remotely), one can find out its version through REST:</p><div class=\"pre_wrapper lang-js\"><pre class=\"programlisting prettyprint lang-js\">$ curl -XGET http://localhost:9200\r\n```\r\n\r\n```diff\r\n@@ -22 +22 @@ elasticsearch-8.0.0</pre></div><p>If Elasticsearch is running (locally or remote\r\n-    \"number\" : \"8.0.0-SNAPSHOT\",\r\n+    \"number\" : \"8.0.0\",\r\n```\r\n\r\n```diff\r\n@@ -108 +108 @@\r\n-java version \"1.8.0_45\"</pre></div></div><div class=\"section\"><div class=\"titlepage\"><div><div><h2 class=\"title\"><a id=\"requirements-es\"></a>Elasticsearch<a href=\"https://github.com/elastic/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/intro/requirements.adoc\" class=\"edit_me\" title=\"Edit this page on GitHub\" rel=\"nofollow\">edit</a></h2></div></div></div><p>We <span class=\"strong strong\"><strong>highly</strong></span> recommend using the latest Elasticsearch (currently 8.0.0-SNAPSHOT). While elasticsearch-hadoop maintains backwards compatibility\r\n+java version \"1.8.0_45\"</pre></div></div><div class=\"section\"><div class=\"titlepage\"><div><div><h2 class=\"title\"><a id=\"requirements-es\"></a>Elasticsearch<a href=\"https://github.com/elastic/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/intro/requirements.adoc\" class=\"edit_me\" title=\"Edit this page on GitHub\" rel=\"nofollow\">edit</a></h2></div></div></div><p>We <span class=\"strong strong\"><strong>highly</strong></span> recommend using the latest Elasticsearch (currently 8.0.0). While elasticsearch-hadoop maintains backwards compatibility\r\n```\r\n```diff\r\n@@ -111 +111 @@ find a matrix of supported versions <a class=\"ulink\" href=\"https://www.elastic.c\r\n-elasticsearch-8.0.0-SNAPSHOT</pre></div><p>If Elasticsearch is running (locally or remotely), one can find out its version through REST:</p><div class=\"pre_wrapper lang-js\"><pre class=\"programlisting prettyprint lang-js\">$ curl -XGET http://localhost:9200\r\n+elasticsearch-8.0.0</pre></div><p>If Elasticsearch is running (locally or remotely), one can find out its version through REST:</p><div class=\"pre_wrapper lang-js\"><pre class=\"programlisting prettyprint lang-js\">$ curl -XGET http://localhost:9200\r\n```\r\n\r\n```diff\r\n@@ -116 +116 @@ elasticsearch-8.0.0</pre></div><p>If Elasticsearch is running (locally or remote\r\n-    \"number\" : \"8.0.0-SNAPSHOT\",\r\n+    \"number\" : \"8.0.0\",\r\n```\r\n\r\nhttps://www.elastic.co/guide/en/elasticsearch/hadoop/master/install.html\r\n```diff\r\n@@ -56 +56 @@ Spark itself.</p></td></tr></table></div><p>The Spark connector framework is the\r\n-  &lt;version&gt;{version}-SNAPSHOT&lt;/version&gt; <a id=\"CO6-1\"></a><i class=\"conum\" data-value=\"1\"></i>\r\n+  &lt;version&gt;8.0.0-SNAPSHOT&lt;/version&gt; <a id=\"CO6-1\"></a><i class=\"conum\" data-value=\"1\"></i>\r\n```\r\n\r\n```diff\r\n@@ -150 +150 @@ Spark itself.</p></td></tr></table></div><p>The Spark connector framework is the\r\n-  &lt;version&gt;{version}-SNAPSHOT&lt;/version&gt; <a id=\"CO6-1\"></a><i class=\"conum\" data-value=\"1\"></i>\r\n+  &lt;version&gt;8.0.0-SNAPSHOT&lt;/version&gt; <a id=\"CO6-1\"></a><i class=\"conum\" data-value=\"1\"></i>\r\n\r\n```\r\n\r\n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1355#issuecomment-531973276"
    },
    {
        "project": "flink",
        "id": 1205087894,
        "body": "> The current PR tries to fix the issue by replacing job cancellation with ControlledSource + requestJobResult, right?\r\n\r\nRight.\r\n\r\n> However, if the refactoring is part of a fix as you mentioned, then this renaming is probably out of scope of this PR.\r\n\r\nYes, how about adding another commit? like:\r\n- 1f1480af49fb6d70895ecb51a4b74a0cf4ee9afd [refactor][state/changelog] Complete upload tasks in StateChangeUploadScheduler",
        "url": "https://github.com/apache/flink/pull/20404#issuecomment-1205087894"
    },
    {
        "project": "spring-security",
        "id": 930064412,
        "body": "Sorry, @jzheaux ,I messed up the squash. I deleted my branch and will  recreate the PR from the fresh branch",
        "url": "https://github.com/spring-projects/spring-security/pull/10300#issuecomment-930064412"
    },
    {
        "project": "druid",
        "id": 810975092,
        "body": ">Sorry, I don't understand what you said. Do you mean an integration test is developing for Avro stream and when it finished, I can add a new json about this test? Or I need create this integration test by myself?\r\n\r\nAh sorry, let me try to explain a bit more. So in the case of avro inline schema and avro schema registry, you should be able to just add the JSON files with the `InputFormat` template and get the tests for free. The integration test is [`ITKafkaIndexingServiceDataFormatTest`](https://github.com/apache/druid/blob/master/integration-tests/src/test/java/org/apache/druid/tests/parallelized/ITKafkaIndexingServiceDataFormatTest.java) is used to test the same data with kafka streaming using a variety of different data formats which are supported by kafka ingestion. It works by iterating over the JSON templates in https://github.com/apache/druid/tree/master/integration-tests/src/test/resources/stream/data to test each data-format present in that directory with the same set of data. For each of these data formats in the integration tests, there is implemented a corresponding [`EventSerializer`](https://github.com/apache/druid/blob/master/integration-tests/src/main/java/org/apache/druid/testing/utils/EventSerializer.java), which writes data for the tests to the Kafka stream for the format, and the parser or inputFormat templates are then used to construct a supervisor to spawn indexing tasks to read the data and run the actual tests to verify stuff is working correctly.\r\n\r\n[`AvroEventSerializer`](https://github.com/apache/druid/blob/master/integration-tests/src/main/java/org/apache/druid/testing/utils/AvroEventSerializer.java), and [`AvroSchemaRegistryEventSerializer`](https://github.com/apache/druid/blob/master/integration-tests/src/main/java/org/apache/druid/testing/utils/AvroSchemaRegistryEventSerializer.java) are already present because there are integration tests using the Avro stream parsers ([avro inline](https://github.com/apache/druid/blob/master/integration-tests/src/test/resources/stream/data/avro/parser/input_row_parser.json) and [avro schema registry](https://github.com/apache/druid/blob/master/integration-tests/src/test/resources/stream/data/avro_schema_registry/parser/input_row_parser.json)), they are just missing the input format templates because it didn't exist until this PR (JSON, CSV, and TSV do have input format templates, [which might be useful as a reference](https://github.com/apache/druid/tree/master/integration-tests/src/test/resources/stream/data)). You should just be able to adapt those parser templates into the equivalent input format template.",
        "url": "https://github.com/apache/druid/pull/11040#issuecomment-810975092"
    },
    {
        "project": "spring-boot",
        "id": 1195353873,
        "body": "I'm not sure why I added `merge-with-amendements`, perhaps it was because I thought some changes would be required to apply the fixes to 2.6.x. Turns out it applied pretty cleanly.\r\n\r\nThanks for the PR @marcwrobel! Much appreciated.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31734#issuecomment-1195353873"
    },
    {
        "project": "hadoop",
        "id": 986568780,
        "body": "@tasanuma Thank you for your review and comments. \r\n\r\nI run the UT `doTestBalancerWithStripedFile` in current trunk branch and sometimes the errors occur .\r\n![\u4f01\u4e1a\u5fae\u4fe1\u622a\u56fe_fa59a4ad-e149-4259-b587-31468bb77e69](https://user-images.githubusercontent.com/2844826/144813625-e15c5bea-ec9f-48e9-8aff-c0d14449ffe1.png)\r\n\r\nTherefore, it is not good to use `assertEquals(0, nnc.getBlocksFailed().get())` to check the result in this new UT.\r\nI will redesign a UT to test this scenario as soon as possible.\r\nIf you have any suggestions, please let me know, thank you.",
        "url": "https://github.com/apache/hadoop/pull/3679#issuecomment-986568780"
    },
    {
        "project": "pulsar",
        "id": 559631006,
        "body": "@sijie Got it. Thanks for your reminder. We should consider reorganizing it. Currently, there are two \"tiered storage\" sections in sidebar under \"Concepts and architecture\" and \"Cookbooks\".",
        "url": "https://github.com/apache/pulsar/pull/5762#issuecomment-559631006"
    },
    {
        "project": "flink",
        "id": 1179279118,
        "body": "hi @afedulov , Parameter name has been modified, thank you for your suggestion",
        "url": "https://github.com/apache/flink/pull/20127#issuecomment-1179279118"
    },
    {
        "project": "hadoop",
        "id": 1084475215,
        "body": "> You mean [HDFS-16327](https://issues.apache.org/jira/browse/HDFS-16327)(#3716) needs follow-up?\r\n\r\nI mean HDFS-16396 (#3827) can be extended for namenode as well (reconfig of `dfs.datanode.peer.stats.enabled`). This work is w.r.t what @jojochuang has mentioned reg the ability to enable slownode tracking without cluster restart.",
        "url": "https://github.com/apache/hadoop/pull/4107#issuecomment-1084475215"
    },
    {
        "project": "pulsar",
        "id": 1207374478,
        "body": "> Remove the `release/2.7.5` label because memory limiter was introduced since 2.8, see #8965.\r\n\r\n@BewareMyPower @Jason918 Created a new PR #16972 to branch-2.7 which only fix the semaphore release duplicated problem in ProducerImpl, PTAL.",
        "url": "https://github.com/apache/pulsar/pull/16837#issuecomment-1207374478"
    },
    {
        "project": "spring-security",
        "id": 937257984,
        "body": "Hello, @jzheaux! Thank you for a fast response.\r\n\r\nVery sorry for tests. I though I run `./gradlew build` as mentioned in `README`, but I must messed something up, when I was trying to import whole project to Eclipse. My everyday work is with Eclipse and Maven, so I'm not very good with Gradle... Long story short I was trying to import project into eclipse and ended up with broken repo... So I cloned it once again, installed Intellij and now I can see I cannot split my change in two separated changes. I made change in `InMemoryXmlApplicationContext` on my first try, then I split commit in 2 and uploaded just one of them for the try. I thought I run `gradle` correctly, but I looks I was wrong. Once again sorry for that.\r\n\r\nNow about additional tests... Well that is why I created an issue as my first try to this problem. So I wouldn't try to write tests in the project I don't know (from a development site) and wouldn't bother with the stuff like parsing highest version number etc. :-) But OK, I could try. Let say I read `spring.schemas` and find out that `spring-security-X.Y.Z.xsd` is the lastest XSD. What you want me to test from `SecurityNamespaceHandler`? Method `matchesVersionInternal` is private and part of much bigger `parse` method.\r\n\r\nIn the nearest feture I'll at least try to provide correct PR (I gotta read how to correct a PR now :-)).",
        "url": "https://github.com/spring-projects/spring-security/pull/10348#issuecomment-937257984"
    },
    {
        "project": "spring-security",
        "id": 1146818094,
        "body": "> Thanks, @houssemba! I've left some feedback inline.\r\n\r\nThanks @jzheaux for your review. I have taken your feedback into account. Don't hesitate if I forget something. ",
        "url": "https://github.com/spring-projects/spring-security/pull/11316#issuecomment-1146818094"
    },
    {
        "project": "spring-security",
        "id": 1067941653,
        "body": "refactor UsernamePasswordAuthenticationFilter method",
        "url": "https://github.com/spring-projects/spring-security/pull/10970#issuecomment-1067941653"
    },
    {
        "project": "druid",
        "id": 982054041,
        "body": "@yuanlihan, cool feature! Here is one question you might want to explain: what is that use case that will be improved by this change?\r\n\r\nThe feature would seem to optimize queries that hit exactly the same data every time. How common is this? It might solve a use case that you have, say, 100 dashboards, each of which will issue the same query. One of them runs the real query, 99 fetch from the cache.\r\n\r\nHere's a similar use case I've seen many times, but would *not* benefit from a post-merge cache: a dashboard wants to show the \"last 10 minutes\" of data, and we have minute-grain cubes (segments). Every query shifts the time range by one minute which will add a bit more new data, and exclude a bit of old data. With this design, we cache the post-merge data, so each shift in the time window will result in a new result set to cache.\r\n\r\nCan this feature instead cache pre-merge results? That way, we cache each minute slice once. The merge is still needed to gather up the slices required by the query. But, that merge is in-memory and should be pretty fast. The result would be, in this example, caching 1/10 the amount of data compared to caching post-merge data. And, less load on the historicals since we don't hit them each time the query time range shifts forward one minute.\r\n\r\nIn short, please explain your use case a bit more so we an understand the goal of this enhancement.\r\n\r\nEdit: I'm told we may have some of the above. Historicals can already cache per-segment results. In a system with large numbers of segments, and where the query hits a good percentage of those, I'm told the merge cost can be high. Is it the merge cost that this PR seeks to avoid?",
        "url": "https://github.com/apache/druid/pull/11989#issuecomment-982054041"
    },
    {
        "project": "spring-security",
        "id": 624694502,
        "body": "Hi,\r\n\r\nAs I mentioned in the original ticket, I'm interested in helping out with this PR. We are library developers and our goal is to enable JVM developers in our organization to use spring-security with OAuth 2.0. Our Authorization Server is going to be PingFederate and our security engineers decided to require private_key_jwt in all cases when it's possible.\r\n\r\nBecause we don't know how our users are going to want to use it, we're looking for a fairly complete implementation.\r\n\r\nThis is the initial design I came up with (some of it is already addressed in this PR, some of it is new or slightly different, I don't insist on the structure and naming, only the functionality):\r\n\r\n- Create `Encoder` counterparts to `Decoder`s in `org.springframework.security.oauth2.jwt` (in the spring-security-oauth2-jose project)\r\n  - `NimbusJwtDecoder` currently only supports RSA, we should add support for EC as well\r\n- Add `JWT` or a `CLIENT_SECRET_JWT` and a `PRIVATE_KEY_JWT` field in `org.springframework.security.oauth2.core.ClientAuthenticationMethod`\r\n- Add any necessary pieces of information for creating and signing JWTs with either a client secret or private key to `org.springframework.security.oauth2.client.registration.ClientRegistration`, specifically:\r\n  - the `clientSecret` field might be usable as a string representation of the private key as well, otherwise a separate one is needed and the unused one of these 2 properties must be` null`, which can be used to determine if the actual method is `client_secret_jwt` or `private_key_jwt`\r\n  - `org.springframework.security.oauth2.jose.jws.JwsAlgorithm` for the encoding, which can be validated to be one of `HS*` for `client_secret_jwt` and one of `RS*` or `ES*` for `private_key_jwt`\r\n  - Optionally a `org.springframework.security.oauth2.jwt.JwtClaimAccessor` (could be a `org.springframework.security.oauth2.jwt.Jwt`, although it needs some modifications to be convenient for this purpose*) which can define optional overrides for the default claims to include in the transmitted JWTs\r\n    - *: currently the `org.springframework.security.oauth2.jwt.Jwt` class contains both the final token generated from the claims and the headers, so it more accurately represents a JWS rather than just a JWT (although it doesn\u2019t contain the signature, only the full JWS in its serialized form). This is OK for what it\u2019s used now (decoding JWS-es) but not convenient for storing user-provided JWT claims. It implements `org.springframework.security.oauth2.jwt.JwtClaimAccessor` which is a perfect interface for this purpose, but currently doesn\u2019t have any other implementor. Consider renaming `org.springframework.security.oauth2.jwt.Jwt` to `Jws` or something more accurate and creating a separate `Jwt` class, or alternatively create a `JwtClaimsSet` or similar.\r\n- Fix verifications in `org.springframework.security.oauth2.client.registration.ClientRegistration.Builder` and its `create()` method to account for the new configurations\r\n  - In the `create()` method, if the client secret is missing, the authentication method is set to `NONE`, but it\u2019ll be missing for private key authentication as well if we use a separate field for it\r\n  - More verification might be necessary for the new additions\r\n- Add conversions for new method(s) in `org.springframework.security.oauth2.core.ClientAuthenticationMethod` to\r\n  - `org.springframework.security.oauth2.client.registration.ClientRegistrations#getClientAuthenticationMethod`\r\n  - `org.springframework.security.oauth2.client.jackson2.StdConverters.ClientAuthenticationMethodConverter#convert`\r\n- Add support for the new properties of `org.springframework.security.oauth2.client.registration.ClientRegistration` to\r\n  - `org.springframework.security.config.oauth2.client.ClientRegistrationsBeanDefinitionParser`\r\n  - `org.springframework.security.oauth2.client.jackson2.ClientRegistrationDeserializer`\r\n  - `org.springframework.security.oauth2.client.registration.ClientRegistrations`\r\n- Implement creating `client_secret_jwt` and `private_key_jwt` requests from `org.springframework.security.oauth2.client.registration.ClientRegistration`s at the OAuth2 Client Endpoints in `org.springframework.security.oauth2.client.endpoint`, specifically:\r\n  - Create a `org.springframework.security.oauth2.jwt.JwtEncoderFactory<org.springframework.security.oauth2.client.registration.ClientRegistration>` (new interface) that creates a `org.springframework.security.oauth2.jwt.NimbusJwtEncoder` configured with a `com.nimbusds.jwt.proc.JWTProcessor` from a `org.springframework.security.oauth2.client.registration.ClientRegistration`\r\n  - Create a utility that creates a JWT claims set from a `org.springframework.security.oauth2.client.registration.ClientRegistration`, using the default claims specified in it and falling back to:\r\n    - The `clientId` for `iss` and `sub`\r\n    - The token URI of the Authorization Server for `aud`\r\n    - A random UUID for `jti`\r\n    - The current timestamp for `iat` and that plus a configured amount of time (e.g. 5 minutes) for `exp` - this might need a configuration property as well for the default lifetime\r\n  - Add the `urn:ietf:params:oauth:client-assertion-type:jwt-bearer client_assertion_type` and the `client_assertion` generated by the JWT encoder built by the above factory and JWT claims factory from the `ClientRegistration` in the form body at the following locations, if the authentication method is one of the new JWT-related ones:\r\n    - `org.springframework.security.oauth2.client.endpoint.AbstractWebClientReactiveOAuth2AccessTokenResponseClient#populateTokenRequestBody`\r\n    - `org.springframework.security.oauth2.client.endpoint.NimbusAuthorizationCodeTokenResponseClient#getTokenResponse`\r\n    - `org.springframework.security.oauth2.client.endpoint.OAuth2AuthorizationCodeGrantRequestEntityConverter#buildFormParameters`\r\n    - `org.springframework.security.oauth2.client.endpoint.OAuth2ClientCredentialsGrantRequestEntityConverter#buildFormParameters`\r\n    - `org.springframework.security.oauth2.client.endpoint.OAuth2PasswordGrantRequestEntityConverter#buildFormParameters`\r\n    - `org.springframework.security.oauth2.client.endpoint.OAuth2RefreshTokenGrantRequestEntityConverter#buildFormParameters`\r\n- Testing:\r\n  - Unit tests for the new fields and properties in `org.springframework.security.oauth2.core.ClientAuthenticationMethod`, `org.springframework.security.oauth2.client.registration.ClientRegistration` and the new converters/factories/deserializers/etc. where other similar tests currently exist\r\n  - Copy of tests for decoders in `org.springframework.security.oauth2.jwt` (in the spring-security-oauth2-jose project) for the new encoders\r\n  - Adjust existing tests in spring-security-oauth2-jose project for any changes to `org.springframework.security.oauth2.jwt.Jwt` and the decoders for EC support\r\n  - Unit and integration tests for the new utilities in `org.springframework.security.oauth2.client.endpoint`\r\n  - Copy of existing tests for the `POST` authentication method in `org.springframework.security.oauth2.client.endpoint` for `client_secret_jwt` and `private_key_jwt` for each class in the package that implements them\r\n\r\nOpen questions:\r\n1. Should we have a single `JWT` or a separate `CLIENT_SECRET_JWT` and a `PRIVATE_KEY_JWT` field in `org.springframework.security.oauth2.core.ClientAuthenticationMethod`? The implementation could use the fields in `org.springframework.security.oauth2.client.registration.ClientRegistration` to determine which method to use, but it might not even be necessary to distinguish between them, since the only change is how to sign the JWT, and a generic signer that takes a signing algorithm and a String key could deal with that.\r\n1. Should we store the private key as a String in the `clientSecret` field of the `ClientRegistration`, or have a separate typed field for it? For full disclosure, I haven't looked into the full implications of this (i.e. is there a convenient String serialization of all private key types we plan to support and how should we get the key from the keystore to the `ClientRegistration`).\r\n1. `org.springframework.security.oauth2.jwt.Jwt` doesn't represent a JWT and we probably need something that does. (It requires to contain the JOSE headers and the entire JWS token (headers+JWT+signature) in its serialized form.) Could/should we rename the current class to something more accurate and create a new `Jwt` class which is just a JWT, or leave it as is and create something like a `JwtClaimsSet`? If we choose to rename (e.g. to `Jws`), \"`Jws`\" could extend the new `Jwt` class which could keep the existing extra contents as deprecated for a while, and then the change would be backwards-compatible besides the deprecation warning.\r\n\r\n@jgrandja could you please have a look at this and say if you agree / give your opinion on the above \"open questions\"?\r\n@forgo let me know if/how I can help once we agree on a design\r\n\r\nThanks a lot to both of you for working on this!",
        "url": "https://github.com/spring-projects/spring-security/pull/8445#issuecomment-624694502"
    },
    {
        "project": "druid",
        "id": 985804768,
        "body": "I'm a bit curious, [Avro OCF is a file format](https://avro.apache.org/docs/current/spec.html#Object+Container+Files), is it common to put these files in streaming ingest messages? There is no technical reason this wouldn't work if the files were small enough to fit in the messages since it is all just binary blobs in the end, but was mostly wondering if this is a common use case compared to the streaming oriented avro formats we support (inline schema, multi-inline-schema, schema repo, schema registry).",
        "url": "https://github.com/apache/druid/pull/11865#issuecomment-985804768"
    },
    {
        "project": "pulsar",
        "id": 767427730,
        "body": "> I am happy to help Pulsar to redesign their CI to be a bit more friendlier towards other projects? I have quite an experience with that - in Apache Airflow I introduced changes that brought the usage of Github Actions by 70% maintaining it's usefulness and I am pretty sure the way you are using GitHub Actions can be optimised. Super Happy to help with that if needed.\r\n\r\n@potiuk any help is very appreciated\r\nas @zymap says the problem is that we have lots of tests and they take much time.\r\n\r\nWe are working on fixing all of the flakes, but this will take time. This will help because we won't need to rerun PR validation more times than needed.\r\n\r\nProbably it would be interesting to try to merge the workflows: all java tests, all integration tests, all cpp....in some test GitHub repository and work on a new setup.\r\nWe also should remove the 'retry' at build script level.\r\n\r\nif we let the build continue in case of errors we should have some tool to gather all of the failures and we must be sure that we are not missing any of them\r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/9159#issuecomment-767427730"
    },
    {
        "project": "druid",
        "id": 1080945084,
        "body": "This is a big PR. It is all-in-one so folks can see the whole picture. If it helps, this can be broken into smaller chunks, at loss of overall context.\r\n\r\nHere's a quick summary of what's new vs. what's refactored:\r\n\r\n* `docker-tests/docs` is all new and is meant to capture all the learnings from this exercise, along with information needed to move froward. This is the main resource for understanding this PR.\r\n* `docker-tests` project is new so it does not conflict with the existing `integration-tests` project: both can exist.\r\n* `docker-tests/base-test` is mostly new. It contains the revised test config code and a new cluster client.\r\n  * `ClusterConfig` is the YAML-based config mechanism.\r\n  * `Initializer` has a bunch of complexity to force the server-minded Guice config to work in a client. Loads the DB. Etc.\r\n* `docker-tests/test-image` is a greatly refactored set of Docker build scripts. The Dockerfile is heavily refactored to remove the third-party dependencies and rearrange how Druid is laid out (unpacked from the distribution tarball). DB setup is removed.\r\n* `docker-tests/testing-tools` is mostly a copy/paste of `extensions-core/testing-tools` with the custom node role from `integration-tests` added.\r\n* `docker-tests/high-availability` is a refactor of one test from `integration-tests`. The Docker Compose script is specific to this one test, refactored from those in `integration-tests`. The idea is that this test contains just the files for this \"group\". Other groups will follow this pattern.\r\n* Other files are mostly clean-up uncovered while debugging. In some cases, code was refactored so the test \"clients\" could use code that was previously tightly coupled with the server.\r\n* `yaml` files: refactor of Docker Compose with new test config.\r\n\r\nAlso, as noted before, this PR moves authorization aside into separate files. Authorization is not yet enabled.",
        "url": "https://github.com/apache/druid/pull/12368#issuecomment-1080945084"
    },
    {
        "project": "pulsar",
        "id": 983228833,
        "body": "\r\n> @Anonymitaet\r\n> \r\n> Please don't block the PR due to the documentation format. yes, I agree, the better format helps but the reorganization of the content can be handled in separate PR as well because every individual has a different idea about the formation.\r\n\r\nHi @rdhabalia, \r\n- I did not mean to block anything, I just want to improve the user experience since no one would like to read \"a mountain of\" information at a glance. Technical information should be easy to find/understand/use.\r\n- Agree that documentation can be submitted in a separate PR. \r\n- Everyone has a different opinion on everything but there are some general rules/standards accepted by the majority. In technical writing, often, text that is difficult to understand in paragraph form becomes clear in a table format. Tables help highlight relationships among similar pieces of information and are easily scanned for quick reference.\r\n",
        "url": "https://github.com/apache/pulsar/pull/12902#issuecomment-983228833"
    },
    {
        "project": "druid",
        "id": 281633922,
        "body": "@jon-wei I simply added a new ```takeBatch()``` method to BlockingPool which is ```checkAndDrain()``` you mentioned. After thinking about this for a while, I decided to add it because this issue focuses on guaranteeing the atomicity when acquiring merge buffers, and it cannot be achieved without ```takeBatch()```. But I still believe that we need to improve and maybe redesign BlockingPool.\r\n\r\nSorry for going back and forth. Would you and other reviewers @himanshug @leventov mind reviewing again please? ",
        "url": "https://github.com/apache/druid/pull/3939#issuecomment-281633922"
    },
    {
        "project": "apm-agent-java",
        "id": 724699700,
        "body": "OTOH, when a PR is created by dependabot, if that particular dependency should be excluded, there is a way to say in the PR with a comment, for instance https://github.com/elastic/apm-pipeline-library/pull/829\r\n\r\n```\r\n@dependabot close will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\r\n@dependabot ignore this major version will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\r\n@dependabot ignore this minor version will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\r\n@dependabot ignore this dependency will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\r\n```",
        "url": "https://github.com/elastic/apm-agent-java/pull/1486#issuecomment-724699700"
    },
    {
        "project": "druid",
        "id": 967746282,
        "body": "Contrarian opinion: I don't see how deferring the breaking change for 1 release helps anyone. People still need to do the work before they can upgrade to _some_ version, whether it's 0.23 or 0.24. And they won't be that far apart. So why not do the breaking change now?\r\n\r\n(On the other hand, it _would_ be helpful to defer the breaking change for a longer period, and use that time to batch it up with other breaking changes. Breaking a bunch of things all at once from time to time, and having all other releases be non-breaking, is easier for people to deal with than breaking a little bit each release. But I don't advocate this plan for reasons I get into in the rest of the comment.)\r\n\r\nThe rest of this comment is a bit of soapboxing, so feel free to ignore it if you aren't into that sort of thing.\r\n\r\nThe soapboxy statement is that I'm sympathetic to the Linux kernel driver school of thought when it comes to query extensions like new query types, filters, aggregators, etc. You can read about that school of thought here: https://www.kernel.org/doc/html/latest/process/stable-api-nonsense.html. There's a very abbreviated TLDR right there in the URL.\r\n\r\nIn our situation, maintaining stability for query extensions has upsides that affect a relatively small number of sites where there are site-specific query extensions in play, but has downsides that affect all sites.\r\n\r\nThe main downside of maintaining stable interfaces is that it creates extra work. For example: it's common for us to add a method that gets some new information that query engines will use to make better decisions. Supporting a stable interface means we need to include handling for that information being unknown. On the other hand, if we break the interface then we can also get rid of the code that handles the \"unknown\" case. It simplifies the core, reduces the surface area that needs to be tested and maintained, and makes the user experience more uniform, all of which are good things.\r\n\r\nBeing more free with changing these interfaces would help us evolve them faster, and would benefit the majority of sites that _don't_ have their own site-specific extensions. And just like in Linux, if people with site-specific extensions get tired of maintaining them, that problem can be solved by contributing them to mainline in extensions-contrib.\r\n\r\nBtw, a coda. Our equivalent of the Linux kernel _syscall_ API (which is famously stable) is our HTTP APIs and query API. I am sympathetic to the Linux view there too, and I kind of wish they were _more_ stable than they are.",
        "url": "https://github.com/apache/druid/pull/11917#issuecomment-967746282"
    },
    {
        "project": "spring-boot",
        "id": 360600978,
        "body": "@rajadilipkolli @philwebb thank you for attention to this issues! But this is false positive static code analysis warning.  Possible resource leak already fixed in [PR 11624](https://github.com/spring-projects/spring-boot/pull/11624) \r\n\r\nNeed to redesign this classes to avoid this sonarcloud warnings.\r\n\r\nLog4J2LoggingSystem.java is not applicable - stream should be in open state after return of ConfigurationSource \r\n\r\nJarWriter.java - output stream created in construnctor and used in another object methods\r\n\r\nSocketTargetServerConnection.java - network channel should be in open state after method return",
        "url": "https://github.com/spring-projects/spring-boot/pull/11779#issuecomment-360600978"
    },
    {
        "project": "pulsar",
        "id": 418471238,
        "body": "@rdhabalia I believe that having z-node created and deleted in very different parts of the code makes it very hard to figure out how things work. Having the entire logic for the lock and its cleanup within the same class encapsulate the complexity there and anyone reading that class can easily understand the behavior.\r\n\r\nRegarding `NoopLoadManager`: the standalone currently works with `ModularLoadManager`, but `ModularLoadManager` was really not designed for standalone. In fact, standalone mode doesn't need a load manager because there are no other brokers. All the features provided by a load manager are not used by standalone (and sometimes they complicate it): load-balancing, getting load reports, tracking rates of bundles, traffic shedding and so on.  All this is, in the best case, it's overhead to the functioning of Pulsar standalone. The `NoopLoadManager` short circuits all these features. The only piece of code it contains is the z-node registration, since it's needed in lookup logic. \r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/2487#issuecomment-418471238"
    },
    {
        "project": "skywalking",
        "id": 877541395,
        "body": "Happen to find these files are not used anymore, clean them up",
        "url": "https://github.com/apache/skywalking/pull/7277#issuecomment-877541395"
    },
    {
        "project": "spring-framework",
        "id": 77410087,
        "body": "I've revisited this PR with suggested changes.\n\n## Documentation Layout\n\nThe documentation is now organized in bookparts, so there are now less files to track. LevelOffsets for bookparts don't need to be incremented, as they're already starting at level 0. Still, I separated some parts that were long/important enough to be in their own file; those are imported with an incremented leveloffset.\n\nHere is the new documentation layout:\n\n```\nindex\n  |-- overview[]\n  |\n  |-- whats-new[]\n  |\n  |-- core[]\n  |   |-- core-beans[leveloffset=+1]\n  |   |-- core-aop[leveloffset=+1]\n  |-- testing[]\n  |\n  |-- data-access[]\n  |\n  |-- web[]\n  |   |-- web-mvc[leveloffset=+1]\n  |   |-- web-view[leveloffset=+1]\n  |-- integration[]\n  |\n  |-- appendix[]\n      |-- appx-spring-tld[leveloffset=+1]\n      |-- appx-spring-form-tld[leveloffset=+1]\n```\n\n## Remaining issues\n\nWhen viewing individually a bookpart (e.g. looking at `core` in its own HTML rendered page):\n1. `[partintro]` are not rendered in individual pages, since the bookpart is not a child of a book document when being rendered indivually\n\n```\nasciidoctor: ERROR: partintro block can only be used when doctype is book\nand it's a child of a book part. Excluding block content.\n```\n1. links to other bookparts don't work; they probably need to be updated:\n\n```\n<<section, My section>> must be changed into <<filename#section, My section>> \n```\n\nOf course, both issues are solved in single document mode.\n\n## Notes\n\nThis is a first stab at reorganizing our documentation, and I believe this must be done iteratively. Breaking down our huge `index.adoc` file will help us and avoid PR decay, merge conflicts, etc. And should give some fresh air to our file editors and browsers.\n\n@rwinch I know we're now relying on docbook, but I was wondering if something needed to be done re asciidoctor/asciidoctor-gradle-plugin#52 if we want to support multi-page documentation natively?\n\n@rstoyanchev using asciidoctor's `toclevels: N`, we can make that TOC significantly smaller - but I think that relying on docbook to render html prevents us from doing so at the moment.\n\nThoughts?\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/751#issuecomment-77410087"
    },
    {
        "project": "druid",
        "id": 1203328846,
        "body": "Thanks much for making this fix. An impressive amount of code is deleted in this PR!\r\n\r\nI wonder, will this fix the many issues with [`PrefetchableTextFilesFirehoseFactoryTest`](https://github.com/apache/druid/issues/12638)? It seems that one fails about 10-20% of the time for me in Travis builds.",
        "url": "https://github.com/apache/druid/pull/12852#issuecomment-1203328846"
    },
    {
        "project": "spring-security",
        "id": 825309949,
        "body": "@jzheaux This might not be the best way to test, cos that rework could break the logic of the  method `Authentication authentication(Map<String, Object> metadata)`, it basically takes a byte array from metadata, returns an Authentication, that is good enough. Break this into two method would make the code looks not that good.\r\n\r\nI can reproduce the LEAK through this:\r\n\r\n```java\r\nfor (int i = 0; i < 100000; i++) { \r\n    authenticationPayloadExchangeConverter.authentication(metadata);\r\n}\r\nSystem.gc();\r\n```\r\n\r\nbut I don't known how to make the assertion, because the leak report is just an error log not an exception.\r\n\r\nAny suggestion for this @jzheaux? and is it OK to make unit test like this? ",
        "url": "https://github.com/spring-projects/spring-security/pull/9671#issuecomment-825309949"
    },
    {
        "project": "druid",
        "id": 780906331,
        "body": "@sthetland I think I got most of your suggestions, but had reorganized the doc, PTAL.\r\n\r\n@suneet-s ,based upon our conversation I improved (or tried to) the context and organization of the caching topic. Still waiting for the list of scenarios where segment caching doesn't work.\r\n\r\nFYI @2bethere ",
        "url": "https://github.com/apache/druid/pull/10848#issuecomment-780906331"
    },
    {
        "project": "pulsar",
        "id": 1157377164,
        "body": "> LGTM, but I do have a question, it seems this method can be a replacement of newOutputMessage, so is it possible to deprecate newOutputMessage? if not, could you please add some more context comparing newOutputRecordBuilder and newOutputMessage? thanks.\r\n\r\nThat's 2 different approaches. `newOutputRecordBuilder` is useful if you design a Function that returns Record. `newOutputMessage` is more generic and could be used to send multiple messages for instance. It also gives more control on the message as you can control things such as deliverAt/deliverAfter.",
        "url": "https://github.com/apache/pulsar/pull/16041#issuecomment-1157377164"
    },
    {
        "project": "spring-security",
        "id": 758045871,
        "body": "I did consider a filter to wrap request, however `AuthenticationTrustResolver` is used and doesn't appear to be a bean to inject. I'd have to create a new instance (may not be an issue). As far as the principal object, I am using out of the box OIDC and haven't tracked down a good place to create new principal to extend/override the toString(). I can take another look at that.\r\n\r\nThanks for the quick response!",
        "url": "https://github.com/spring-projects/spring-security/pull/9211#issuecomment-758045871"
    },
    {
        "project": "druid",
        "id": 1201903841,
        "body": "Thanks for the fix! The eternity intervals have other issues. They are very cumbersome when comparing native queries, such as in tests. They require users to know how to define such intervals when all that is wanted is \"all data.\"\r\n\r\nI wonder, should we define an additional \"intervals\" subclass? The existing one for defined intervals, and a new one for the \"all data\" (i.e. \"eternity\") case? Then, a native query might look like:\r\n\r\n```json\r\n  \"intervals\" : {\r\n    \"type\" : \"eternity\"\r\n  },\r\n```\r\n\r\nInstead of:\r\n\r\n```json\r\n  \"intervals\" : {\r\n    \"type\" : \"intervals\",\r\n    \"intervals\" : [ \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\" ]\r\n  },\r\n```\r\n\r\nWith this, we express the intent, not the implementation.\r\n\r\nComparisons can then treat the \"eternity\" form special: eternity overlaps with all other intervals by definition. There can never be multiple intervals if there is eternity since, by definition, there is no additional period of time beyond eternity.\r\n\r\nThis might be more of a change than we want to take on in this PR, but it is a solution that would make life simpler in several ways.",
        "url": "https://github.com/apache/druid/pull/12844#issuecomment-1201903841"
    },
    {
        "project": "spring-boot",
        "id": 262294620,
        "body": "@philwebb I've updated the PR. As suggested, both MVC and JMX endpoints now simply wrap `AuditEventRepository` and expose its operations.\r\n\r\nSince there was no existing support for JMX endpoint implementations without a backing `Endpoint` (like with `AbstractMvcEndpoint`), I've decomposed the existing `EndpointMBean` to support such use case. That effort is extracted into a separate commit.",
        "url": "https://github.com/spring-projects/spring-boot/pull/6579#issuecomment-262294620"
    },
    {
        "project": "hadoop",
        "id": 1207252355,
        "body": ":confetti_ball: **+1 overall**\r\n\r\n\r\n\r\n\r\n\r\n\r\n| Vote | Subsystem | Runtime |  Logfile | Comment |\r\n|:----:|----------:|--------:|:--------:|:-------:|\r\n| +0 :ok: |  reexec  |   0m 39s |  |  Docker mode activated.  |\r\n|||| _ Prechecks _ |\r\n| +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n| +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n| +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n| +0 :ok: |  xmllint  |   0m  0s |  |  xmllint was not available.  |\r\n| +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n| +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 2 new or modified test files.  |\r\n|||| _ trunk Compile Tests _ |\r\n| +0 :ok: |  mvndep  |  15m  6s |  |  Maven dependency ordering for branch  |\r\n| +1 :green_heart: |  mvninstall  |  25m 29s |  |  trunk passed  |\r\n| +1 :green_heart: |  compile  |   9m 59s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  compile  |   8m 55s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  checkstyle  |   2m 19s |  |  trunk passed  |\r\n| +1 :green_heart: |  mvnsite  |   7m  7s |  |  trunk passed  |\r\n| +1 :green_heart: |  javadoc  |   6m 58s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   6m 36s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |  10m 44s |  |  trunk passed  |\r\n| +1 :green_heart: |  shadedclient  |  22m  6s |  |  branch has no errors when building and testing our client artifacts.  |\r\n|||| _ Patch Compile Tests _ |\r\n| +0 :ok: |  mvndep  |   0m 33s |  |  Maven dependency ordering for patch  |\r\n| +1 :green_heart: |  mvninstall  |   3m 30s |  |  the patch passed  |\r\n| +1 :green_heart: |  compile  |   9m 15s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javac  |   9m 15s |  |  the patch passed  |\r\n| +1 :green_heart: |  compile  |   8m 50s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  javac  |   8m 50s |  |  the patch passed  |\r\n| +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n| -0 :warning: |  checkstyle  |   2m  4s | [/results-checkstyle-hadoop-yarn-project_hadoop-yarn.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4712/3/artifact/out/results-checkstyle-hadoop-yarn-project_hadoop-yarn.txt) |  hadoop-yarn-project/hadoop-yarn: The patch generated 3 new + 178 unchanged - 0 fixed = 181 total (was 178)  |\r\n| +1 :green_heart: |  mvnsite  |   6m 23s |  |  the patch passed  |\r\n| +1 :green_heart: |  javadoc  |   6m  4s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   5m 44s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |  10m 38s |  |  the patch passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 50s |  |  patch has no errors when building and testing our client artifacts.  |\r\n|||| _ Other Tests _ |\r\n| +1 :green_heart: |  unit  |   1m 42s |  |  hadoop-yarn-api in the patch passed.  |\r\n| +1 :green_heart: |  unit  |   5m 29s |  |  hadoop-yarn-common in the patch passed.  |\r\n| +1 :green_heart: |  unit  |   3m 30s |  |  hadoop-yarn-server-common in the patch passed.  |\r\n| +1 :green_heart: |  unit  |  24m 34s |  |  hadoop-yarn-server-nodemanager in the patch passed.  |\r\n| +1 :green_heart: |  unit  |   4m 10s |  |  hadoop-yarn-server-router in the patch passed.  |\r\n| +1 :green_heart: |  asflicense  |   1m 25s |  |  The patch does not generate ASF License warnings.  |\r\n|  |   | 236m 33s |  |  |\r\n\r\n\r\n| Subsystem | Report/Notes |\r\n|----------:|:-------------|\r\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4712/3/artifact/out/Dockerfile |\r\n| GITHUB PR | https://github.com/apache/hadoop/pull/4712 |\r\n| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets xmllint |\r\n| uname | Linux 1c1dc9d9a128 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | dev-support/bin/hadoop.sh |\r\n| git revision | trunk / ee7f8217d7c22140672d42247dec4c2f01df0672 |\r\n| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n|  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4712/3/testReport/ |\r\n| Max. process+thread count | 1714 (vs. ulimit of 5500) |\r\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router U: hadoop-yarn-project/hadoop-yarn |\r\n| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4712/3/console |\r\n| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n| Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/4712#issuecomment-1207252355"
    },
    {
        "project": "spring-boot",
        "id": 526593835,
        "body": "Thanks for the PR, @IEnoobong. Before too much more time is invested in this, I think we should pause and consider https://github.com/spring-projects/spring-boot/issues/6313. That issue is already quite a big job and inlining Kotlin snippets in the documentation will make it bigger.",
        "url": "https://github.com/spring-projects/spring-boot/pull/18017#issuecomment-526593835"
    },
    {
        "project": "spring-security",
        "id": 487240941,
        "body": "Alternate approach to **avoid double parsing** is [NimbusJwtMultiTenantDecoder.java](https://github.com/gburboz/spring-security/blob/feature/6779-rs-multitenant-v2/oauth2/oauth2-jose/src/main/java/org/springframework/security/oauth2/jwt/NimbusJwtMultiTenantDecoder.java) with refactored [NimbusJwtDecoder.java](https://github.com/gburboz/spring-security/blob/feature/6779-rs-multitenant-v2/oauth2/oauth2-jose/src/main/java/org/springframework/security/oauth2/jwt/NimbusJwtDecoder.java)\r\n\r\nFurthermore we should be able to specify validator and converter beans for respective decoder in config (either by name or type). \r\n\r\n```yaml\r\nspring.security.oauth2.resourceserver:\r\n  multi-tenant-jwt:\r\n    -\r\n      issuer-uri: \"https://mockwebserver.com/\"\r\n      jwk-set-uri: ${mockwebserver.url}/.well-known/jwks.json\r\n      converter-bean-name: \"mockwebserverJwtConverter\"\r\n      validator-bean-name: \"mockwebserverJwtValidator\"\r\n    -\r\n      issuer-uri: \"https://some-domain.com/\"\r\n      public-key: \"${some-domain.public-key}\"\r\n      converter-bean-name: \"someDomainJwtConverter\"\r\n      validator-bean-name: \"someDomainJwtValidator\"\r\n\r\n```",
        "url": "https://github.com/spring-projects/spring-security/pull/6817#issuecomment-487240941"
    },
    {
        "project": "spring-security",
        "id": 499749153,
        "body": "Thank you for response! I'll rewrite the sample application without JS framework. \r\n\r\nTo implement the authenticator management feature, I thought writing with Angular would make code simple, but it is not good to require users to have knowledge of Angular.\r\nIf you have other concern, please let me know. I'll fix it.\r\n",
        "url": "https://github.com/spring-projects/spring-security/pull/6842#issuecomment-499749153"
    },
    {
        "project": "hadoop",
        "id": 1207220538,
        "body": ":broken_heart: **-1 overall**\r\n\r\n\r\n\r\n\r\n\r\n\r\n| Vote | Subsystem | Runtime |  Logfile | Comment |\r\n|:----:|----------:|--------:|:--------:|:-------:|\r\n| +0 :ok: |  reexec  |   0m 43s |  |  Docker mode activated.  |\r\n|||| _ Prechecks _ |\r\n| +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n| +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n| +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n| +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n|||| _ trunk Compile Tests _ |\r\n| +0 :ok: |  mvndep  |  15m  6s |  |  Maven dependency ordering for branch  |\r\n| +1 :green_heart: |  mvninstall  |  25m 36s |  |  trunk passed  |\r\n| +1 :green_heart: |  compile  |   4m  8s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  compile  |   3m 33s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  checkstyle  |   1m 31s |  |  trunk passed  |\r\n| +1 :green_heart: |  mvnsite  |   2m 21s |  |  trunk passed  |\r\n| +1 :green_heart: |  javadoc  |   2m  8s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 44s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 56s |  |  trunk passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 40s |  |  branch has no errors when building and testing our client artifacts.  |\r\n|||| _ Patch Compile Tests _ |\r\n| +0 :ok: |  mvndep  |   0m 29s |  |  Maven dependency ordering for patch  |\r\n| +1 :green_heart: |  mvninstall  |   1m 41s |  |  the patch passed  |\r\n| +1 :green_heart: |  compile  |   3m 52s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  cc  |   3m 52s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 52s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/3/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 generated 3 new + 446 unchanged - 0 fixed = 449 total (was 446)  |\r\n| +1 :green_heart: |  compile  |   3m 17s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  cc  |   3m 17s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 17s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/3/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 generated 3 new + 370 unchanged - 0 fixed = 373 total (was 370)  |\r\n| +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n| +1 :green_heart: |  checkstyle  |   1m 13s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server: The patch generated 0 new + 11 unchanged - 1 fixed = 11 total (was 12)  |\r\n| +1 :green_heart: |  mvnsite  |   1m 48s |  |  the patch passed  |\r\n| +1 :green_heart: |  javadoc  |   1m 33s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 27s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 39s |  |  the patch passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 35s |  |  patch has no errors when building and testing our client artifacts.  |\r\n|||| _ Other Tests _ |\r\n| +1 :green_heart: |  unit  |   3m  3s |  |  hadoop-yarn-server-common in the patch passed.  |\r\n| +1 :green_heart: |  unit  |  98m 27s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n| +1 :green_heart: |  asflicense  |   0m 51s |  |  The patch does not generate ASF License warnings.  |\r\n|  |   | 225m 53s |  |  |\r\n\r\n\r\n| Subsystem | Report/Notes |\r\n|----------:|:-------------|\r\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/3/artifact/out/Dockerfile |\r\n| GITHUB PR | https://github.com/apache/hadoop/pull/4711 |\r\n| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat |\r\n| uname | Linux 13f802f0fef4 4.15.0-58-generic #64-Ubuntu SMP Tue Aug 6 11:12:41 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | dev-support/bin/hadoop.sh |\r\n| git revision | trunk / 41da933f3e38150a114dbbfea1ec826fde60aeba |\r\n| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n|  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/3/testReport/ |\r\n| Max. process+thread count | 955 (vs. ulimit of 5500) |\r\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server |\r\n| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/3/console |\r\n| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n| Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/4711#issuecomment-1207220538"
    },
    {
        "project": "flink",
        "id": 1160201207,
        "body": "Hi @zentol , we considered this option before. The problem is that the classloader is held by many components in table, e.g. `CatalogManger`, `FunctionCatalog`, `Planner`, and so on. It's hard to replace all the classloader references when adding a JAR URL. It's very easy to miss updating classloader in a certain component and cause problems. \r\n\r\nThat's why we prefer to introduce a `MutableURLClassLoader` which allows us to update the classloader in place. ",
        "url": "https://github.com/apache/flink/pull/20003#issuecomment-1160201207"
    },
    {
        "project": "skywalking",
        "id": 573524127,
        "body": "I agree with you, the error of unreachable or network perf unstable issues are also important for service.\r\n\r\nI can simplify client-side tracing for this, like no `onMessage` event, only `Complete/Close` event for internal client-side tracing.",
        "url": "https://github.com/apache/skywalking/pull/4177#issuecomment-573524127"
    },
    {
        "project": "spring-boot",
        "id": 507437105,
        "body": "I've added a commit to this PR that handles deprecation of `RedisFlushMode` and `HazelcastFlushMode` in favor of new, common, `FlushMode` enum (see spring-projects/spring-session#1465).\r\n\r\nIt would be nice to get the first commit to `master` (upgrade to snapshots) as I'm planning to do more changes on Spring Session's config infrastructure and follow them up with PRs against Spring Boot.\r\n\r\nPerhaps it would be good to hold off any major restructuring of properties until that effort is completed.",
        "url": "https://github.com/spring-projects/spring-boot/pull/17278#issuecomment-507437105"
    },
    {
        "project": "apm-agent-java",
        "id": 620035271,
        "body": "@msievers we discussed that - while your solution works great now, we are soon to introduce a very useful enhancement to the agent logging that will rely on the directory being writeable.\r\n\r\nHowever, what you really need is a silent agent, so let's do that \ud83d\ude42. We thought of adding an `OFF` option to the [`log_level`](https://www.elastic.co/guide/en/apm/agent/java/current/config-logging.html#config-log-level) config to facilitate that. Would you like to do that?",
        "url": "https://github.com/elastic/apm-agent-java/pull/1154#issuecomment-620035271"
    },
    {
        "project": "flink",
        "id": 1193811700,
        "body": "Looks good to me. But please fix the compile problem. \r\n\r\nPS: maybe you didn't configure the automatic code format in IDE. See https://nightlies.apache.org/flink/flink-docs-master/docs/flinkdev/ide_setup/#code-formatting",
        "url": "https://github.com/apache/flink/pull/20247#issuecomment-1193811700"
    },
    {
        "project": "skywalking",
        "id": 1200628485,
        "body": "> Please update changes.md in the PR template. You missed one step. And remove <> around the issue number.\r\n\r\nplease check again.",
        "url": "https://github.com/apache/skywalking/pull/9414#issuecomment-1200628485"
    },
    {
        "project": "apm-agent-java",
        "id": 739901148,
        "body": "Unfortunately `excludeGroupIds` does not get honoured for a multimodule maven project. Might be related to [MDEP-516](https://issues.apache.org/jira/browse/MDEP-516)\r\n\r\nI've tried locally with:\r\n- `./mvnw -U org.apache.maven.plugins:maven-dependency-plugin:3.1.2:go-offline -DexcludeGroupIds=co.elastic.apm`\r\n- Even I added some changes in the pom.xml but same behaviour \r\n\r\nSo, the alternative is to run `go-offline` with `never fail` once and might reduce the number of times we hit the issue :/",
        "url": "https://github.com/elastic/apm-agent-java/pull/1556#issuecomment-739901148"
    },
    {
        "project": "hadoop",
        "id": 1164455182,
        "body": "> Thanks @cxzl25 for your patch. I have a question about it and looking forward your feedback. Can this change improve the namenode performance? I see that some places specially remove this judgment, and use log.info(\"{}\", XXX) replace it.\r\n> \r\n> Or can you do some performance test for it? use this judgment or not.\r\n\r\nWe have some JIRAs that use the `isDebugEnabled` judgment.\r\nAt the beginning, I found that the log of `Removing stale replica` has meaningless string splicing, and using `NameNode.blockStateChangeLog` can be replaced by `blockLog`, because this debug-level log is called a lot every day.\r\nSo I fixed the other problems by the way.",
        "url": "https://github.com/apache/hadoop/pull/4480#issuecomment-1164455182"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 503571503,
        "body": "Hi,\r\n\r\nnobody available to review this ?\r\n\r\nShould I backport the modification in 7.x branch or in 6.8 ?\r\n\r\nThank you",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1299#issuecomment-503571503"
    },
    {
        "project": "apm-agent-java",
        "id": 1117069432,
        "body": ":+1: for the proposal, it definitely helps keeping it simpler !\r\n\r\nFor the next steps, I'll suggest the following (feel free to comment if I've missed something)\r\n- wait for this PR build to complete without errors\r\n- merge it (actually we can auto-merge it)\r\n- once merged check the opbeans steps and tag builds are removed on the main pipeline\r\n- when doing the next release, double-check that opbeans is properly updated",
        "url": "https://github.com/elastic/apm-agent-java/pull/2612#issuecomment-1117069432"
    },
    {
        "project": "spring-boot",
        "id": 1063858373,
        "body": "I don't think `@JsonMixin` should be annotated with `@Component` at all. We don't want the mixins to be beans, prototypes or otherwise. Instead, I would use a custom sub-class of `ClassPathScanningCandidateComponentProvider` that overrides `isCandidateComponent` to find the `@JsonMixin`-annotated classes. They can then be registered with the `JsonMixinModule` by extracting information from the bean definitions that `ClassPathScanningCandidateComponentProvider` returns. The latter part of this is similar to what `EntityScanner` already does.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30152#issuecomment-1063858373"
    },
    {
        "project": "flink",
        "id": 1205786492,
        "body": ">It's a separate change--but what do you think about adding warning logs to UnregisteredMetricGroup to emit a log that says a metric may not be reported\r\n\r\nI'm not a fan of warnings if there's nothing the user can do about it, especially when the chance of us fixing these instances are slim.",
        "url": "https://github.com/apache/flink/pull/20449#issuecomment-1205786492"
    },
    {
        "project": "apm-agent-java",
        "id": 1201330774,
        "body": "I have fixed the instrumentation and it now works as expected for Vert.x 4.3.2.\r\nThe downside is that the 4.x plugin is now compiled and tested against only this version, and not 4.3.1 anymore.\r\n\r\nThus maybe the next step (maybe as a follow-up PR) here is :\r\n- add an integration test that cover the 3.x and 4.x versions, which would allow keep testing 4.3.1, but that might be an extensive task as tests are currently relying on Junit5.\r\n- (optional) remove some duplicated tests for 3.x",
        "url": "https://github.com/elastic/apm-agent-java/pull/2700#issuecomment-1201330774"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 423344570,
        "body": "@jbaiera sorry for the delay on my end as well, I'm having trouble finding a good way to test this class as we only have public access to the `build` method that takes input of the large string created by the method I\"ve updated in this PR.  Short of restructuring things quite a bit for this small change I\"m not sure the best way to test it.  If you have an idea that I\"m missing let me know and I\"ll throw together a test class.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1161#issuecomment-423344570"
    },
    {
        "project": "apm-agent-java",
        "id": 1208429186,
        "body": "\n## :green_heart: Build Succeeded\n\n\n<!-- BUILD BADGES-->\n> _the below badges are clickable and redirect to their specific view in the CI or DOCS_\n[![Pipeline View](https://img.shields.io/badge/pipeline-pipeline%20-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//pipeline) [![Test View](https://img.shields.io/badge/test-test-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//tests) [![Changes](https://img.shields.io/badge/changes-changes-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//changes) [![Artifacts](https://img.shields.io/badge/artifacts-artifacts-yellow)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//artifacts) [![preview](https://img.shields.io/badge/docs-preview-yellowgreen)](http://apm-agent-java_2737.docs-preview.app.elstc.co/diff)  [![preview](https://img.shields.io/badge/elastic-observability-blue)](https://ci-stats.elastic.co/app/apm/services/apm-ci/transactions/view?rangeFrom=2022-08-08T16:53:08.684Z&rangeTo=2022-08-08T17:13:08.684Z&transactionName=BUILD+apm-agent-java%2Fapm-agent-java-mbp%2FPR-%7Bnumber%7D&transactionType=job&latencyAggregationType=avg&traceId=354e21e5c7a68593612a4be27ce06854&transactionId=5b51422422076baf)\n\n<!-- BUILD SUMMARY-->\n<details><summary>Expand to view the summary</summary>\n<p>\n\n#### Build stats\n\n\n\n\n* Start Time: 2022-08-08T17:03:08.684+0000\n\n\n* Duration: 50 min 5 sec\n\n\n\n\n#### Test stats :test_tube:\n\n| Test         | Results                         |\n| ------------ | :-----------------------------: |\n| Failed       | 0  |\n| Passed       | 3081  |\n| Skipped      | 36 |\n| Total        | 3117   |\n\n\n</p>\n</details>\n\n<!-- TEST RESULTS IF ANY-->\n\n\n<!-- STEPS ERRORS IF ANY -->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## :green_heart: Flaky test report\nTests succeeded.\n\n\n\n## :robot: GitHub comments\n\nTo re-run your PR in the CI, just comment with:\n\n\n\n- `/test` : Re-trigger the build.\n\n\n- `run benchmark tests` : Run the benchmark tests.\n\n\n- `run jdk compatibility tests` : Run the JDK Compatibility tests.\n\n\n- `run integration tests` : Run the Agent Integration tests.\n\n\n- `run end-to-end tests` : Run the APM-ITs.\n\n\n- `run windows tests` : Build & tests on windows.\n\n\n- `run` `elasticsearch-ci/docs` : Re-trigger the docs validation. (use unformatted text in the comment!)\n\n\n\n\n<!--COMMENT_GENERATED_WITH_ID_comment.id-->",
        "url": "https://github.com/elastic/apm-agent-java/pull/2737#issuecomment-1208429186"
    },
    {
        "project": "skywalking",
        "id": 980809798,
        "body": "Just \r\n\r\n> Let's back to the original question, where shows OAP will increase load? Why do you say there is a performance impact to OAP? @mrproliu\r\n> From my understanding of this solution, there is only another new grpc service introduced, no performanceimpact for OAP at all.\r\n\r\nThe problem is that the new protocol is to replace the old one essentially, think in this way, an ALS message will be processed by and only by one of the protocols, if it\u2019s processed by the old one (before this new Satellite protocol) it is (highly) possible that we don\u2019t need to parse the node identifier but if it\u2019s processed by the new one it always needs to parse the node identifier, do you agree the parsing of node identifier is a waste of resources?",
        "url": "https://github.com/apache/skywalking/pull/8193#issuecomment-980809798"
    },
    {
        "project": "druid",
        "id": 1042657139,
        "body": "@JulianJaffePinterest   I tested it this way\r\n`Dataset<Row> dataset = sparkSession.createDataFrame(list, schema);`\r\n` Dataset<Row> bucketedDataset =  dataset.withColumn(\"quick_col\",SparkUdfs.bucketRow().apply(functions.col(\"_timeBucketCol\"),functions.lit(\"millis\"),functions.lit(\"DAY\")));`\r\n`    Dataset<Row> partitionedDataSet1 = bucketedDataset.repartition(functions.col(\"quick_col\")).drop(functions.col(\"quick_col\"));`\r\n`    Dataset<Row> partitionedDataSet2 = partitionedDataSet1.repartition(10);`  // Increase parallelism\r\n`    partitionedDataSet2.write().format(\"druid\").mode(SaveMode.Overwrite).options(map).save();`\r\nbut in log I find it \uff1a\r\n![image](https://user-images.githubusercontent.com/24448732/154592741-24bb2c24-6da5-45ba-8c26-2b73a6ae83b1.png)\r\nI think it should be five segments, but only two segment\r\nCorresponding code block\uff1a\r\nCode url: \r\nhttps://github.com/JulianJaffePinterest/druid/blob/2dd1cda37289b780328e26d08bedfb1f57bac0e3/spark/src/main/scala/org/apache/druid/spark/utils/SegmentRationalizer.scala\r\nWhat configuration items do I need to modify \uff1f\r\n",
        "url": "https://github.com/apache/druid/pull/12159#issuecomment-1042657139"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1191509791,
        "body": "It looks like it's failing with errors like this:\r\n```\r\njava.lang.ExceptionInInitializerError\r\n\tat org.mockito.cglib.core.KeyFactory$Generator.generateClass(KeyFactory.java:167)\r\n\tat org.mockito.cglib.core.DefaultGeneratorStrategy.generate(DefaultGeneratorStrategy.java:25)\r\n\tat org.mockito.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:217)\r\n\tat org.mockito.cglib.core.KeyFactory$Generator.create(KeyFactory.java:145)\r\n\tat org.mockito.cglib.core.KeyFactory.create(KeyFactory.java:117)\r\n\tat org.mockito.cglib.core.KeyFactory.create(KeyFactory.java:109)\r\n\tat org.mockito.cglib.core.KeyFactory.create(KeyFactory.java:105)\r\n\tat org.mockito.cglib.proxy.Enhancer.<clinit>(Enhancer.java:70)\r\n\tat org.mockito.internal.creation.jmock.ClassImposterizer.createProxyClass(ClassImposterizer.java:109)\r\n\tat org.mockito.internal.creation.jmock.ClassImposterizer.imposterise(ClassImposterizer.java:80)\r\n\tat org.mockito.internal.creation.jmock.ClassImposterizer.imposterise(ClassImposterizer.java:74)\r\n\tat org.mockito.internal.creation.CglibMockMaker.createMock(CglibMockMaker.java:23)\r\n\tat org.mockito.internal.util.MockUtil.createMock(MockUtil.java:26)\r\n\tat org.mockito.internal.MockitoCore.mock(MockitoCore.java:51)\r\n\tat org.elasticsearch.mock.orig.Mockito.mock(Mockito.java:1243)\r\n\tat org.elasticsearch.mock.orig.Mockito.mock(Mockito.java:1120)\r\n\tat org.mockito.Mockito$1.run(Mockito.java:69)\r\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\r\n\tat org.mockito.Mockito.mock(Mockito.java:66)\r\n\tat org.elasticsearch.hadoop.rest.FindPartitionsTest.testSlicePartitions(FindPartitionsTest.java:98)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\r\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)\r\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)\r\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)\r\n\tat org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)\r\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\r\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\r\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\r\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\r\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.processTestClass(Unknown Source)\r\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)\r\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\r\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\r\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\r\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\r\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)\r\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)\r\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\r\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\r\nCaused by: java.lang.reflect.InaccessibleObjectException: Unable to make protected final java.lang.Class java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain) throws java.lang.ClassFormatError accessible: module java.base does not \"opens java.lang\" to unnamed module @145f66e3\r\n\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\r\n\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\r\n\tat java.base/java.lang.reflect.Method.checkCanSetAccessible(Method.java:199)\r\n\tat java.base/java.lang.reflect.Method.setAccessible(Method.java:193)\r\n\tat org.mockito.cglib.core.ReflectUtils$2.run(ReflectUtils.java:69)\r\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\r\n\tat org.mockito.cglib.core.ReflectUtils.<clinit>(ReflectUtils.java:59)\r\n\t... 60 more\r\n```\r\nI'm not sure why a gradle version change would do that.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1980#issuecomment-1191509791"
    },
    {
        "project": "druid",
        "id": 1096475335,
        "body": "> @pjfanning can we indicate in the PR description what version we are upgrading to, and point out that this is a major version bump (including a link to the release notes would be nice). Did we look to make sure there aren't anything in the release notes we need to worry about?\r\n\r\nI modified the description",
        "url": "https://github.com/apache/druid/pull/12427#issuecomment-1096475335"
    },
    {
        "project": "pulsar",
        "id": 1191438111,
        "body": "> I think this will remove the possibility of having a stable assignment logic. Right now, if you specify the same consumer names, you will be guaranteed to receive the same keys after reconnections.\r\n\r\nOh, good point. Maybe we should try to add more information to the topic stats instead of changing the behavior.",
        "url": "https://github.com/apache/pulsar/pull/16672#issuecomment-1191438111"
    },
    {
        "project": "spring-framework",
        "id": 728851262,
        "body": "Additional note, while inspecting the `AspectJAdviceParameterNameDiscoverer` I noticed that it might be beneficial to extract the `isVariableName` method from the `AbstractAspectJAdvice` to a helper/utility class and use that from both classes. They share the same logic to determine if something is a variable name. ",
        "url": "https://github.com/spring-projects/spring-framework/pull/26101#issuecomment-728851262"
    },
    {
        "project": "pulsar",
        "id": 1147178378,
        "body": "> It might not be a good idea to add the implementation to the main Pulsar Admin API at all.\r\n> \r\n> HTTP consuming would be better to handle in a separate component. PIP-64 doesn't determine that this should be part of Pulsar Admin API and we should revisit this decision. I think it's a bad idea to add HTTP consuming to Pulsar Admin API and brokers.\r\n> \r\n> We need another discussion on the dev mailing list for deciding.\r\n\r\nYes, I also consider moving this part of the HTTP producer-consumer to pulsar-proxy, which will make the broker clear and reduce communication between brokers.\r\nHowever, moving this part to the pulsar proxy will require the user to deploy another component, especially if the user is not running the pulsar in k8s.\r\n\r\nI will draft a discussion in the dev mail list.\r\n\r\n\r\nBest,\r\nMattison\r\n",
        "url": "https://github.com/apache/pulsar/pull/15942#issuecomment-1147178378"
    },
    {
        "project": "druid",
        "id": 462086552,
        "body": "@gianm \r\n\r\nLooking for guidance here...\r\n\r\nI have spent the past few days studying the Druid-HLL code and have uncovered at least a half-dozen serious bugs and haven't even started on the merge logic, which from a brief look also has very serious problems.  A number of these problems are interconnected, so you can't just fix one at a time.   \r\n\r\nThis code needs to be redesigned from scratch.  I'm not sure I want to undertake this, but if I were, I would insist on some major changes. The API will have some required changes:  The biggest one is removing the ability for users to specify the hash function.  Any users that are currently doing that, using a different hash function and have historical stored images may not be able to use their history.  \r\n\r\nI am considering 2 strategies:\r\n1. **Rewrite existing code**: Many current internal methods that are now public will become private or package-private: e.g., no public access to internals such as the overflow registers, getNumNonZeroRegisters, getHeaderBytes, getPayloadBytePosition, setVersion, etc.  I may also insist on a merge class rather than a merge method ( fold() ).  \r\n\r\nThe storage would be a little larger (from 1031 bytes to perhaps 1080 bytes).  And merge performance may be a bit slower.  It could still be backward compatible, but old images will still propagate errors into the new design and there is nothing that can be done about that.  Users that record their history with the new design will see much better error performance.  \r\n\r\n2. **API Wrapper to the DS-HLL sketch**.  This will require some changes to the DS-HLL code, but may be easier to do and would eliminate a lot of code duplication.  Basically I have to allow for the non-seeded hash function, adapt to the big-endian fields used by the Druid-HLL, create a merge function that can read the old Druid-HLL images, etc.  \r\n\r\nThe advantage of this is (hopefully) a single code-base for the sketch internals with two different wrappers, one for the old Druid-HLL users, and a new full-function API wrapper for the DS-HLL customers.\r\n\r\nWhatever, the new design would have to be extensively characterized and tested.\r\n\r\nThoughts?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "url": "https://github.com/apache/druid/pull/6865#issuecomment-462086552"
    },
    {
        "project": "flink",
        "id": 1204197520,
        "body": "Looks like the issue is with unsused imports (should be removed after running `mvn spotless:apply`)\r\nAfter that it compiles ok",
        "url": "https://github.com/apache/flink/pull/20409#issuecomment-1204197520"
    },
    {
        "project": "spring-framework",
        "id": 77410087,
        "body": "I've revisited this PR with suggested changes.\n\n## Documentation Layout\n\nThe documentation is now organized in bookparts, so there are now less files to track. LevelOffsets for bookparts don't need to be incremented, as they're already starting at level 0. Still, I separated some parts that were long/important enough to be in their own file; those are imported with an incremented leveloffset.\n\nHere is the new documentation layout:\n\n```\nindex\n  |-- overview[]\n  |\n  |-- whats-new[]\n  |\n  |-- core[]\n  |   |-- core-beans[leveloffset=+1]\n  |   |-- core-aop[leveloffset=+1]\n  |-- testing[]\n  |\n  |-- data-access[]\n  |\n  |-- web[]\n  |   |-- web-mvc[leveloffset=+1]\n  |   |-- web-view[leveloffset=+1]\n  |-- integration[]\n  |\n  |-- appendix[]\n      |-- appx-spring-tld[leveloffset=+1]\n      |-- appx-spring-form-tld[leveloffset=+1]\n```\n\n## Remaining issues\n\nWhen viewing individually a bookpart (e.g. looking at `core` in its own HTML rendered page):\n1. `[partintro]` are not rendered in individual pages, since the bookpart is not a child of a book document when being rendered indivually\n\n```\nasciidoctor: ERROR: partintro block can only be used when doctype is book\nand it's a child of a book part. Excluding block content.\n```\n1. links to other bookparts don't work; they probably need to be updated:\n\n```\n<<section, My section>> must be changed into <<filename#section, My section>> \n```\n\nOf course, both issues are solved in single document mode.\n\n## Notes\n\nThis is a first stab at reorganizing our documentation, and I believe this must be done iteratively. Breaking down our huge `index.adoc` file will help us and avoid PR decay, merge conflicts, etc. And should give some fresh air to our file editors and browsers.\n\n@rwinch I know we're now relying on docbook, but I was wondering if something needed to be done re asciidoctor/asciidoctor-gradle-plugin#52 if we want to support multi-page documentation natively?\n\n@rstoyanchev using asciidoctor's `toclevels: N`, we can make that TOC significantly smaller - but I think that relying on docbook to render html prevents us from doing so at the moment.\n\nThoughts?\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/751#issuecomment-77410087"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 444629013,
        "body": "thanks!\r\n\r\ni am trying to publish a version with some minor modifications to our inhouse artifact server (nexus, artifactory, etc.) \r\n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1120#issuecomment-444629013"
    },
    {
        "project": "apm-agent-java",
        "id": 1201500736,
        "body": "\n## :green_heart: Build Succeeded\n\n\n<!-- BUILD BADGES-->\n> _the below badges are clickable and redirect to their specific view in the CI or DOCS_\n[![Pipeline View](https://img.shields.io/badge/pipeline-pipeline%20-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2733/detail/PR-2733/3//pipeline) [![Test View](https://img.shields.io/badge/test-test-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2733/detail/PR-2733/3//tests) [![Changes](https://img.shields.io/badge/changes-changes-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2733/detail/PR-2733/3//changes) [![Artifacts](https://img.shields.io/badge/artifacts-artifacts-yellow)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2733/detail/PR-2733/3//artifacts) [![preview](https://img.shields.io/badge/docs-preview-yellowgreen)](http://apm-agent-java_2733.docs-preview.app.elstc.co/diff)  [![preview](https://img.shields.io/badge/elastic-observability-blue)](https://ci-stats.elastic.co/app/apm/services/apm-ci/transactions/view?rangeFrom=2022-08-04T08:32:42.403Z&rangeTo=2022-08-04T08:52:42.403Z&transactionName=BUILD+apm-agent-java%2Fapm-agent-java-mbp%2FPR-%7Bnumber%7D&transactionType=job&latencyAggregationType=avg&traceId=2c21b777eaeccad46e260b4be6ec14ef&transactionId=ddf0e5d463b792a9)\n\n<!-- BUILD SUMMARY-->\n<details><summary>Expand to view the summary</summary>\n<p>\n\n#### Build stats\n\n\n\n\n* Start Time: 2022-08-04T08:42:42.403+0000\n\n\n* Duration: 54 min 27 sec\n\n\n\n\n#### Test stats :test_tube:\n\n| Test         | Results                         |\n| ------------ | :-----------------------------: |\n| Failed       | 0  |\n| Passed       | 3081  |\n| Skipped      | 36 |\n| Total        | 3117   |\n\n\n</p>\n</details>\n\n<!-- TEST RESULTS IF ANY-->\n\n\n<!-- STEPS ERRORS IF ANY -->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## :green_heart: Flaky test report\nTests succeeded.\n\n\n\n## :robot: GitHub comments\n\nTo re-run your PR in the CI, just comment with:\n\n\n\n- `/test` : Re-trigger the build.\n\n\n- `run benchmark tests` : Run the benchmark tests.\n\n\n- `run jdk compatibility tests` : Run the JDK Compatibility tests.\n\n\n- `run integration tests` : Run the Agent Integration tests.\n\n\n- `run end-to-end tests` : Run the APM-ITs.\n\n\n- `run windows tests` : Build & tests on windows.\n\n\n- `run` `elasticsearch-ci/docs` : Re-trigger the docs validation. (use unformatted text in the comment!)\n\n\n\n\n<!--COMMENT_GENERATED_WITH_ID_comment.id-->",
        "url": "https://github.com/elastic/apm-agent-java/pull/2733#issuecomment-1201500736"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 234259872,
        "body": "@costin @jbaiera, this PR introduces the SlicePartitionDefinition. I had to refactor (cleanup) some parts to make it work. This is still a work in progress and I need to add more tests. In the meanwhile it would be very helpful if you can check the progress. Considering the size of the PR I am available at your convenience for a deep dive. \n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/812#issuecomment-234259872"
    },
    {
        "project": "spring-boot",
        "id": 957226878,
        "body": "> Thanks for the PR. I've flagged this one to discuss if we want to bring support for cache2k in Spring Boot. Feel free to ignore the comments below for now.\r\n\r\nThanks for the swift review and giving it a look!\r\n\r\nSpring support is available in cache2k for a while and it is continuously integration tested. cache2k has a consistent and complete user guide that helps users when starting with caching. Looking at EHCache3 or Caffeine users need to puzzle together several blog posts and Stackoverflow answers when starting. I think in that perspective the project philosophy of Spring and cache2k is similar.\r\n\r\nNB: The cache2k Spring integration is exploiting the null value support, while most other caches are wrapping the cached value (EHCache3, Caffeine, JCache bases caches). This reduces the memory footprint.\r\n\r\nSince it is not mentioned in the Spring documentation, unfortunately it is not very visible for Spring users. I assume mentioning it in the Spring documentation without Spring boot support does not make sense, since the user experience would not be consistent to the other caches.",
        "url": "https://github.com/spring-projects/spring-boot/pull/28498#issuecomment-957226878"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 920354599,
        "body": "Do we have a good idea of what is present in the task attempt id? I imagine that it's mostly book keeping information about the currently executing task, but not much about the job itself? I wonder if we should also see about pulling in the application name from the job settings for the opaque id as well?",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1755#issuecomment-920354599"
    },
    {
        "project": "spring-boot",
        "id": 1143780917,
        "body": "It appears to be unhappy with my formatting. I'll uglify and push a follow-up commit here in a few minutes.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31230#issuecomment-1143780917"
    },
    {
        "project": "hadoop",
        "id": 1171921057,
        "body": "@slfan1989 \r\nBasically this thread is used for jobs to renew token from third-party service. \r\nRM_DELEGATIONTOKEN is generated from RM and it should be renewed from the client, not the server.\r\n\r\nAs for the verification, you can refer it to RMDelegationTokenSecretManager which extends from AbstractDelegationTokenSecretManager checkToken. \r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/4198#issuecomment-1171921057"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1006158100,
        "body": "I think there might be some additional problems to work out with scripted upserts and es-spark. When I try this (using the scripted upsert example from https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#scripted_upsert)\r\n```\r\n    val update_params = \"count: 4\"\r\n    val update_script = \"if ( ctx.op == 'create' ) {ctx._source.counter = params.new_count} else {ctx._source.counter += params.new_count}\"\r\n    val es_conf = Map(\"es.mapping.id\" -> \"id\", \"es.mapping.exclude\" -> \"id\", \"es.write.operation\" -> \"upsert\", \"es.update.script.params\" -> update_params, \"es.update.script.upsert\" -> \"true\", \"es.update.script.inline\" -> update_script)\r\n    val sqlContext = new SQLContext(sc)\r\n    val data = Seq(Row(\"1\", 3))\r\n    val rdd: RDD[Row] = sc.parallelize(data)\r\n    val schema = new StructType().add(\"id\", StringType, nullable = false).add(\"count\", IntegerType, nullable = false)\r\n    val df = sqlContext.createDataFrame(rdd, schema)\r\n    df.write.format(\"es\").options(es_conf).save(\"test\")\r\n```\r\nI get\r\n```\r\norg.elasticsearch.hadoop.serialization.EsHadoopSerializationException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: [DataFrameFieldExtractor for field [[4]]] cannot extract value from entity [class java.lang.String] | instance [([1,3],StructType(StructField(id,StringType,false), StructField(count,IntegerType,false)))]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.BulkEntryWriter.writeBulkEntry(BulkEntryWriter.java:136) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:170) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:83) ~[elasticsearch-spark_2.12-8.1.0-SNAPSHOT-spark30scala212.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1(EsSparkSQL.scala:101) ~[main/:?]\r\n\tat org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1$adapted(EsSparkSQL.scala:101) ~[main/:?]\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509) [spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]\r\nCaused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: [DataFrameFieldExtractor for field [[4]]] cannot extract value from entity [class java.lang.String] | instance [([1,3],StructType(StructField(id,StringType,false), StructField(count,IntegerType,false)))]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.AbstractBulkFactory$FieldWriter.write(AbstractBulkFactory.java:111) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.writeTemplate(TemplatedBulk.java:80) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:56) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.BulkEntryWriter.writeBulkEntry(BulkEntryWriter.java:68) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\t... 12 more\r\n```\r\nIt looks like `es.update.script.params` does not like having `4` there and instead is expecting a field name.  Just to get past that I tried changing the value of `es.update.script.params`:\r\n```\r\n    val update_params = \"new_count: count\"\r\n    val update_script = \"if ( ctx.op == 'create' ) {ctx._source.counter = params.new_count} else {ctx._source.counter += params.new_count}\"\r\n    val es_conf = Map(\"es.mapping.id\" -> \"id\", \"es.mapping.exclude\" -> \"id\", \"es.write.operation\" -> \"upsert\", \"es.update.script.params\" -> update_params, \"es.update.script.upsert\" -> \"true\", \"es.update.script.inline\" -> update_script)\r\n    val sqlContext = new SQLContext(sc)\r\n    val data = Seq(Row(\"1\", 3))\r\n    val rdd: RDD[Row] = sc.parallelize(data)\r\n    val schema = new StructType().add(\"id\", StringType, nullable = false).add(\"count\", IntegerType, nullable = false)\r\n    val df = sqlContext.createDataFrame(rdd, schema)\r\n    df.write.format(\"es\").options(es_conf).save(\"test\")\r\n```\r\nBut with that I get:\r\n```org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: java.lang.ClassCastException: java.util.Collections$EmptyMap cannot be cast to scala.Tuple2\r\n\tat org.elasticsearch.hadoop.serialization.bulk.BulkEntryWriter.writeBulkEntry(BulkEntryWriter.java:136) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:170) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:83) ~[elasticsearch-spark_2.12-8.1.0-SNAPSHOT-spark30scala212.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1(EsSparkSQL.scala:101) ~[main/:?]\r\n\tat org.elasticsearch.spark.sql.EsSparkSQL$.$anonfun$saveToEs$1$adapted(EsSparkSQL.scala:101) ~[main/:?]\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462) ~[spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509) [spark-core_2.12-3.2.0.jar:3.2.0]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]\r\n\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]\r\nCaused by: java.lang.ClassCastException: java.util.Collections$EmptyMap cannot be cast to scala.Tuple2\r\n\tat org.elasticsearch.spark.sql.DataFrameValueWriter.write(DataFrameValueWriter.scala:53) ~[main/:?]\r\n\tat org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.ScriptTemplateBulk.doWriteObject(ScriptTemplateBulk.java:43) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\tat org.elasticsearch.hadoop.serialization.bulk.BulkEntryWriter.writeBulkEntry(BulkEntryWriter.java:68) ~[elasticsearch-hadoop-mr-8.1.0-SNAPSHOT.jar:8.1.0-SNAPSHOT]\r\n\t... 12 more\r\n```\r\nIt doesn't seem to like the empty map being passed in.\r\nIt's possible I'm just doing something wrong in the code above. Do you have any examples that are working? It would be good to get one or more into https://github.com/elastic/elasticsearch-hadoop/blob/master/spark/sql-30/src/itest/scala/org/elasticsearch/spark/integration/AbstractScalaEsSparkSQL.scala (that's where I've put these to see them fail).",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1454#issuecomment-1006158100"
    },
    {
        "project": "spring-boot",
        "id": 1087522352,
        "body": "Thanks for your reply @mhalbritter. This PR was submitted based on Andy's [comment](https://github.com/spring-projects/spring-boot/issues/24968#issuecomment-1015365254). It's essentially a variant of `AnnotatedTypeMetadata.getAllAnnotationAttributes(String)` , simplifying some logic and accounting for attribute overriding.\r\n\r\nI didn't modify the code in `metaAndDirectAnnotationConditionDoesNotMatchWhenOnlyDirectPropertyIsSet` and this unit test passed.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30505#issuecomment-1087522352"
    },
    {
        "project": "spring-boot",
        "id": 1134460343,
        "body": "TIL about Checkstyle's `JavadocPackageCheck`. Thanks, @eddumelendez. I wonder if we should add this to Spring Java Format, alongside the [existing Javadoc-related checks](https://github.com/spring-io/spring-javaformat/blob/e8bc0a196ba77448305292929cc7504781d595bc/spring-javaformat/spring-javaformat-checkstyle/src/main/resources/io/spring/javaformat/checkstyle/spring-checkstyle.xml#L83-L105).",
        "url": "https://github.com/spring-projects/spring-boot/pull/31140#issuecomment-1134460343"
    },
    {
        "project": "flink",
        "id": 1160374181,
        "body": "> A classloader should not be passed to a component if at that time it does not have access to all required classes.\r\n\r\nThe classloader DOES have access to all required classes at the time when it is passed to a component. The required classes are dynamically changing when users interactively ADD JAR, so we have to replace the classloader or update classloader in place. \r\n\r\n> If you're lacking clear ownership of the classloader then mutating said classloader is a dangerous operation anyway.\r\n\r\nSorry, I don't understand what's the \"dangerous operation\" you mean. There is no classloader leaking and the classloader is clearly managed by the components. \r\n\r\nBtw, Spark SQL also introduced [`MutableURLClassLoader`](https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/MutableURLClassLoader.java) to support `ADD JAR`. I think this is a reasonable way to support this feature, not a hacking way.  ",
        "url": "https://github.com/apache/flink/pull/20003#issuecomment-1160374181"
    },
    {
        "project": "spring-framework",
        "id": 1183103660,
        "body": "Hi @jackycjw,\r\n\r\nThanks for submitting your first proposal to the Spring Framework.\r\n\r\nUnfortunately, we do not currently have any concrete needs for infix expression calculation in the core framework and especially not in a core utility such as `NumberUtils`.\r\n\r\nIn light of that, I am closing this PR.\r\n\r\nPlease note, however, that you may find the [Spring Expression Language (SpEL)](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#expressions) useful as an alternative.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28804#issuecomment-1183103660"
    },
    {
        "project": "hadoop",
        "id": 1158842875,
        "body": "@ayushtkn Please help me to review the code, I hope to replace the deprecated import with the recommended .",
        "url": "https://github.com/apache/hadoop/pull/4406#issuecomment-1158842875"
    },
    {
        "project": "pulsar",
        "id": 1105110631,
        "body": "@michaeljmarshall My bad\r\nThe main reason I want to replace is that  `maxDirectMemory()` can be changed by system property `io.netty.maxDirectMemory`, which will may lead to wrong result.\r\nTo avoid performance regression, I think we can put it in a common class, and others use the result.\r\nWhat do you think?",
        "url": "https://github.com/apache/pulsar/pull/15238#issuecomment-1105110631"
    },
    {
        "project": "flink",
        "id": 1198900364,
        "body": "It has a Compilation failure here,I will modified it later",
        "url": "https://github.com/apache/flink/pull/20387#issuecomment-1198900364"
    },
    {
        "project": "druid",
        "id": 788983700,
        "body": "Hello all,\r\nAny chance on adding the usage of a Password Provider to encapsulate the passwords, just like it is done on the Kafka Consumer properties (https://druid.apache.org/docs/latest/development/extensions-core/kafka-ingestion.html#kafkasupervisorioconfig) ?\r\nThank you all!",
        "url": "https://github.com/apache/druid/pull/10314#issuecomment-788983700"
    },
    {
        "project": "spring-boot",
        "id": 605632428,
        "body": "I recall it as this change may break compatibility with already existing tools expecting class name in the sourceType property. Will supply back after redesign with an extra property for the canonical class name.",
        "url": "https://github.com/spring-projects/spring-boot/pull/20710#issuecomment-605632428"
    },
    {
        "project": "spring-framework",
        "id": 1192282635,
        "body": "@jensdietrich thanks for the PR but  ASM is a 3rd party lib we shade and we don't want to modify them.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28852#issuecomment-1192282635"
    },
    {
        "project": "pulsar",
        "id": 1206018808,
        "body": "@BewareMyPower FYI - with this change implemented, you don't need to generate docs for the upcoming 2.8.4 release.",
        "url": "https://github.com/apache/pulsar/pull/16925#issuecomment-1206018808"
    },
    {
        "project": "hadoop",
        "id": 1035009857,
        "body": "@saintstack \r\nThe path is actually relative:\r\nhttps://github.com/apache/hadoop/blob/efdec92cab8a88deb5ec9e81f5c8feb7a0fa873b/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/protocol/SnapshotDiffReport.java#L85-L90\r\n\r\nFor rename entries it is made absolute here:\r\nhttps://github.com/apache/hadoop/blob/efdec92cab8a88deb5ec9e81f5c8feb7a0fa873b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java#L471-L474\r\n\r\nFor normal delete there won't be any target, it would be always ``null``, so it is added just like that in normal cases.\r\nhttps://github.com/apache/hadoop/blob/efdec92cab8a88deb5ec9e81f5c8feb7a0fa873b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java#L465-L468\r\n\r\nIn this particular case when using filters.\r\n\r\nThe actual entry is a ``RENAME`` entry which has target. Rename has to have a target. So, it takes this else block\r\nhttps://github.com/apache/hadoop/blob/efdec92cab8a88deb5ec9e81f5c8feb7a0fa873b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java#L254\r\n\r\nAnd when converting it to a ``DELETE`` entry, it even adds the target.\r\nhttps://github.com/apache/hadoop/blob/efdec92cab8a88deb5ec9e81f5c8feb7a0fa873b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java#L262-L265\r\n\r\nBut since it is a delete entry the path isn't made absolute wrt target. So it stays like a relative path. like `filterDir1` and since it doesn't start with / and the normal logic by default it gets resolved to home directory.\r\n\r\nThen the code that you shared does the magic, it moves it...\r\n\r\nOne example of target being set to ``null`` :\r\nhttps://github.com/apache/hadoop/blob/efdec92cab8a88deb5ec9e81f5c8feb7a0fa873b/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/DistCpSync.java#L446-L447\r\n\r\nMay be there could be sanity check in delete diff for target, but not very confident about that part, will explore sometime if there is any use case possible where it can be not-null & compat stuff.\r\n\r\nFurther general optimisations as well are possible, like don't rename to tmp and then delete, directly delete(There is a reason why it is like that), that is something in my TODO list, will chase in future\r\n\r\nGeneral Info: Filters are like quite used in DR setups, some time we don't want to copy some data to replica clusters. One example could be Trash data, many other use cases as well.\r\n\r\nLemme know if it isn't still convincing..",
        "url": "https://github.com/apache/hadoop/pull/3940#issuecomment-1035009857"
    },
    {
        "project": "spring-boot",
        "id": 1204153948,
        "body": "@weiwei121723 We discussed this today as a team and decided that we need a stronger use-case before we can consider adding a new event. There are several existing mechanisms that we feel should work and we'd like to prove that they won't before we offer another.\r\n\r\nIf you feel you have a real use-case that you can't get working with existing mechanisms then please open a new issue and attach a full application that we can run and debug. We're a little confused about how the `ApplicationPreparedEvent` is working on the snippet of code you shared so a full application is really needed.\r\n\r\nRegardless, thanks for working on the pull-request.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31958#issuecomment-1204153948"
    },
    {
        "project": "spring-framework",
        "id": 1120790333,
        "body": "Good point! This remained unnoticed in the course of a refactoring from one method name to a set of method names... Thanks for spotting it and raising this.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28427#issuecomment-1120790333"
    },
    {
        "project": "spring-security",
        "id": 1184787360,
        "body": "Thanks, @evgeniycheban. This is now merged into `5.8.x`. I also added a polish commit 5dff157755ed0fae8c68e91dc8e56cefa1b8fe65 to update the formatting of the `HttpSecurity` blocks in the tests you added.",
        "url": "https://github.com/spring-projects/spring-security/pull/11495#issuecomment-1184787360"
    },
    {
        "project": "spring-security",
        "id": 1026329576,
        "body": "@larsgrefer, I appreciate the research you've done here. I'd like to adhere to the [OWASP password recommendations](https://github.com/OWASP/CheatSheetSeries/blob/master/cheatsheets/Password_Storage_Cheat_Sheet.md#introduction) since they are a single reliable source we can use for decision making. If 10 is too low, let's start with OWASP to get their recommendation increased to 12.",
        "url": "https://github.com/spring-projects/spring-security/pull/10447#issuecomment-1026329576"
    },
    {
        "project": "pulsar",
        "id": 782099580,
        "body": "\r\n> It depends on what the goal of the SDK is. If you look at the [python pfunc SDK example](https://pulsar.apache.org/docs/en/functions-overview/#content-based-routing-example), it is just `context.publish(topic, item)` If that is the experience you are after, then, it seems best to keep it simple and consistent. Given the way the Go pfunc SDK is structured, this seems like the option more in keeping with the intent.\r\n\r\nWhat I am really concerned is, `context.outputMessage()` might return user a dead producer, and user cannot recreate it because it is cached. \r\n\r\nsuch as \r\n\r\n```\r\nproducer := context.outputMessage(\"topic\")\r\nproducer.Close() // <- user close the producer\r\n```\r\n\r\nlater when user try to get a producer for `topic`, a closed producer will return since it is cached. \r\n\r\n> > A relatively simple but not very mature idea is: we have a timing task in the background to process these resources regularly, for example: 30 minutes\r\n> Are there any objections to this idea? It makes sense to me.\r\n\r\nI agree with this, in addition, this setting should be configureable.",
        "url": "https://github.com/apache/pulsar/pull/9513#issuecomment-782099580"
    },
    {
        "project": "apm-agent-java",
        "id": 1023222737,
        "body": "Next steps before \"ready to merge\"\r\n- [x] update with `main`\r\n- [x] test optional windows steps activation through parameters (one-shot, not persistent)\r\n- [x] test optional windows steps activation through PR tags (persistent)\r\n- [x] some refactoring: format & cleanup, execute Windows build in parallel",
        "url": "https://github.com/elastic/apm-agent-java/pull/1703#issuecomment-1023222737"
    },
    {
        "project": "hadoop",
        "id": 1053924133,
        "body": "First of all there are test failure which must be fixed. \r\nAlso upgrading this has a history of breaking downstream builds, what testing is done for that ?\r\n\r\nSeems like 1.70 is a major upgrade with new features and enhancements. https://www.bouncycastle.org/latest_releases.html can we use 1.68/1.69 to just fix the vulnerabilities ?",
        "url": "https://github.com/apache/hadoop/pull/3980#issuecomment-1053924133"
    },
    {
        "project": "spring-boot",
        "id": 1056466005,
        "body": "Thanks for the follow-up. You are right that the mapping is wrong, I overlooked that, sorry. \r\n\r\nI don't think that we should rename the variables as I find them a bit confusing. Rather I'd rename the path variable pattern to use `{userId}` rather than `{user}. Would you be willing to update your PR in that direction?",
        "url": "https://github.com/spring-projects/spring-boot/pull/30027#issuecomment-1056466005"
    },
    {
        "project": "spring-framework",
        "id": 949779104,
        "body": "Sorry but I still don't understand the current situation.\r\n\r\n1) You're mentioning a \"non-String collection use case\" but `StringUtils#cleanPath` is not it, right? Is there a code path in Spring showing this behavior?\r\n\r\n2) If we can improve the `StringUtils#cleanPath` performance, I'd like to get more information about the performance issue you've hit - what's calling this method and what's the usage profile (repeated calls, long strings, etc).\r\n\r\nWith those two points I can work on JMH benchmarks and get a clearer picture. Thanks!",
        "url": "https://github.com/spring-projects/spring-framework/pull/27557#issuecomment-949779104"
    },
    {
        "project": "spring-security",
        "id": 453151090,
        "body": "I've found the free [Pro Git Book](https://git-scm.com/book/en/v2) to be a very valuable resource. Section [7.6 Git Tools - Rewriting History](https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History) is probably the most relevant in this situation. I have provided the steps below for you to make it easier, but I highly recommend reading Pro Git.\r\n\r\n```\r\ngit rebase -i HEAD~2\r\n```\r\n\r\nThis says you want to do an interactive rebase of the last two commits. This means you want to change those commits. This rewrites the history. Keep in mind on branches that others use this is not proper etiquette because it makes merging difficult. For feature branches like what you have this is expected and quite normal.\r\n\r\nYou editor will popup and look like this:\r\n\r\n```\r\npick 8de408229 Mark ApacheDSContainer as deprecated #6002\r\npick 8e7830791 Add an explanation as to why ApacheDSContainer is deprecated.\r\n...\r\n```\r\n\r\nChange the start of each like to be\r\n\r\n```\r\nr 8de408229 Mark ApacheDSContainer as deprecated #6002\r\nf 8e7830791 Add an explanation as to why ApacheDSContainer is deprecated.\r\n```\r\n\r\nr will indicate you want to reword that commit. f will indicate that the commit should be squashed into the previous commit.\r\n\r\nChange the wording of the commit and save. Then you can force push to your remote. Something like:\r\n\r\n```\r\ngit push -f origin master\r\n```",
        "url": "https://github.com/spring-projects/spring-security/pull/6376#issuecomment-453151090"
    },
    {
        "project": "flink",
        "id": 1194266048,
        "body": "We could add a dedicated test for the serializability of the format, but if this were violated it would already fail on the current master, and even more so after FLINK-28621 is merged (always registering date/time & optional extensions).",
        "url": "https://github.com/apache/flink/pull/20332#issuecomment-1194266048"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 50893365,
        "body": "Thanks - it's the side affect of a search/replace gone astray. Note that I haven't merged in the pull request since the [CLA](http://www.elasticsearch.org/contributor-agreement/) wasn't signed - please do so next time (it's a one time thing). \n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/224#issuecomment-50893365"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1033210551,
        "body": "Thanks so much for the thorough review, @masseyke. I pushed https://github.com/elastic/elasticsearch-hadoop/pull/1902/commits/8237c6aa68af6eba861502d4b4cca5b532300e30 to address the last round of feedback.\r\n\r\nLet me know if this looks good or if there are any other needed changes. Thanks!",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1902#issuecomment-1033210551"
    },
    {
        "project": "spring-framework",
        "id": 1024862638,
        "body": "@sbrannen Yes, I agree with the suggestion. I also take that approach for my usual tests - slightly modify the main code for tests.\r\nThanks for the advice. I have updated my code on the native side accordingly. \r\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/27979#issuecomment-1024862638"
    },
    {
        "project": "druid",
        "id": 1182535667,
        "body": ">Is it intentionally undocumented in this PR? Do you plan to add documentation?\r\n\r\nI was planning to add documentation in a follow-up PR since I thought this one was already big enough \ud83d\ude05 \r\n\r\n>Are there any impediments to maintaining forwards compatibility of the storage format, such that new versions of Druid will always be able to read JSON columns written by older versions? Do you foresee any reason we might want to break compatibility?\r\n\r\nI modeled the column after existing Druid columns so most things are decorated with a version byte which should allow us to make changes in the future while still being able to continue reading the existing data. For the specific list of what is versioned: \r\n* `NestedDataColumnSerializer` for the complex column itself (currently on v3 actually, i removed the reader code for older versions from prototyping to get rid of dead code)\r\n* `GlobalDictionaryEncodedFieldColumnWriter` which writes the nested columns and is currently re-using `DictionaryEncodedColumnPartSerde.VERSION` (i should probably decouple this at some point in the future...)\r\n* `FixedIndexed` (building block used to store local to global dictionary mapping and long and double value dictionaries)\r\n* `CompressedVariableSizedBlobColumnSerializer` (used to compress raw data)\r\n* `CompressedBlockSerializer` (used internally by `CompressedVariableSizedBlobColumnSerializer`)\r\n\r\nIn the \"Future work\" section of #12695 I mention storage format as an area that we can iterate on in the future, the biggest things I have in mind right now are storing arrays of literal values as array typed columns instead of broken out as they currently are, as well as customization such as allowing skipping building indexes on certain columns or storing them all-together also probably falls into this. Nothing about the current code should block this afaik, nor should those future enhancements interfere with our ability to read data that is stored with the current versions of stuff, so long as we practice good version hygiene whenever we make changes.\r\n\r\n> Would you recommend we present this feature in its current state as experimental or production-ready, & why?\r\nThis is a hard one to answer, though I am hesitant to call it production ready right from the start, I think the answer might vary a bit per use case.\r\n\r\nThe surface area here is huge since it essentially provides all of the normal Druid column functionality within these `COMPLEX<json>` columns, and I definitely won't claim this to be bug free. That said, quite a lot of internal testing has been done at this point, even at scale and with complicated nested schemas, which has allowed this codebase to be iterated on to get it to the place it currently is. There are some rough spots which I'm looking to improve in the near future, such as ingest time memory footprint, better array handling, etc, but I think if we get the documentation in a good enough state and can list out the limitations it could be used today.\r\n\r\nThe use cases I would feel most comfortable with are replacements for what can currently be done via flattening, meaning not heavily centered on nested arrays. I do have ideas of how to better support nested arrays and my goal is to allow arrays extracted from nested columns to be exposed as druid `ARRAY` types, but I am not there yet, so I'm not sure I would recommend most array use cases unless they are more like vectors which have expected lengths and array positions are known/meaningful (and such that most queries would be extracting specific array positions, not entire arrays).\r\n\r\nThere is also the matter of different performance characteristics at both ingest and query time for these columns. Ingestion time segment merge is pretty heavy right now because the global value dictionary is stored in heap. Query performance can vary a fair bit with nested columns compared to flat columns, especially with numbers due to the existence of indexes on these numeric columns, which currently at least sometimes results in dramatically faster but also sometimes slower query performance. I'm still exploring this quite a bit, besides documentation follow-up I also have been working on doing some benchmarking to see where things currently stand and plan on sharing those results relatively soon.\r\n\r\nSo, long story short, due to the unknowns I think the answer for right now is that operators should experiment with `COMPLEX<json>` columns to see if they work well for their use case, and use them in production if so, otherwise provide feedback so that we can continue to make improvements and expand the use cases this is good for?",
        "url": "https://github.com/apache/druid/pull/12753#issuecomment-1182535667"
    },
    {
        "project": "druid",
        "id": 780906331,
        "body": "@sthetland I think I got most of your suggestions, but had reorganized the doc, PTAL.\r\n\r\n@suneet-s ,based upon our conversation I improved (or tried to) the context and organization of the caching topic. Still waiting for the list of scenarios where segment caching doesn't work.\r\n\r\nFYI @2bethere ",
        "url": "https://github.com/apache/druid/pull/10848#issuecomment-780906331"
    },
    {
        "project": "apm-agent-java",
        "id": 651637178,
        "body": "@eyalkoren that works just as expected! Right out of the box, no issue.\r\n\r\nWhat I would suggest is to add e.g. jira with a few different versions to your build / end2end testing pipeline.\r\n\r\n`docker run -v /Users/e_pkah/Desktop/setenv.sh:/opt/atlassian/jira/bin/setenv.sh -v /Users/e_pkah/Desktop/elastic-apm-agent-1.17.1-SNAPSHOT.jar:/opt/atlassian/jira/elastic-apm-agent.jar --name=\"jira\" -d -p 8080:8080 atlassian/jira-software`\r\n\r\nwith that you get a jira container that is ready after a few seconds and you can run a curl against `http://localhost:8080` and check if something is broken ;). I would say, as long as it returns something, that is a good sign.\r\n\r\nhttps://gist.github.com/philippkahr/6c12c99028e5dfc08847edb73dc9f4cf here you go for the successful Catalina log. I added the `setenv.sh` I used, so you can recreate it at any given time.\r\n\r\n![image](https://user-images.githubusercontent.com/12175559/86102201-7f8fc280-babb-11ea-9098-9a7e773d6029.png)\r\n\r\n",
        "url": "https://github.com/elastic/apm-agent-java/pull/1259#issuecomment-651637178"
    },
    {
        "project": "spring-security",
        "id": 934003140,
        "body": "Sure. \n\nSince this is my first time contributing here, please confirm if already written test case for post processor is as per requirement or needs to be modified too. \n\nThis will help me in understanding expectations for writing test cases for other two post processors. ",
        "url": "https://github.com/spring-projects/spring-security/pull/10339#issuecomment-934003140"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 445299930,
        "body": "Integration tests are failing at the moment with these changes on the local build-tools snapshot. It's tripping up on some low level Hive + JDBC + JDO mess related to the newer java runtime. I know that there are some Hadoop incompatibilities as well from code that parses java versions in static blocks. We will have to modify the build to use different java runtimes for the tests like ES does.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1221#issuecomment-445299930"
    }
]