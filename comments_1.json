[
    {
        "project": "spring-boot",
        "id": 1161823058,
        "body": "Hi @mhalbritter thanks for review, I replaced the` CompletableFuture` implementation with a normal one. I will wait for you to check about the WebClient initialization. My logic was that `@ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.REACTIVE)` in `ZipkinConfigurations` should mean that Netty should be on the classpath when true. I could be wrong though.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30792#issuecomment-1161823058"
    },
    {
        "project": "spring-boot",
        "id": 1134460343,
        "body": "TIL about Checkstyle's `JavadocPackageCheck`. Thanks, @eddumelendez. I wonder if we should add this to Spring Java Format, alongside the [existing Javadoc-related checks](https://github.com/spring-io/spring-javaformat/blob/e8bc0a196ba77448305292929cc7504781d595bc/spring-javaformat/spring-javaformat-checkstyle/src/main/resources/io/spring/javaformat/checkstyle/spring-checkstyle.xml#L83-L105).",
        "url": "https://github.com/spring-projects/spring-boot/pull/31140#issuecomment-1134460343"
    },
    {
        "project": "pulsar",
        "id": 1198850308,
        "body": "> @lordcheng10 Looks like we have the lock to ensure concurrency problem, could you describe the detail?\r\n\r\n@mattisonchao When there is a serious Full GC, there may be multiple ledgers to update the metadata information on zookeeper at the same time. Suppose we create ledger in the order of ledger1, ledger2, ledger3, ledger4, ledger5, and assume that a comparison occurs. In severe Full GC, ledger5 may grab the metadataMutex lock before ledger4, so ledger5 may trigger the zookeeper callback method operationComplete before ledger4, so that ledger4 may cover ledger5:\r\nhttps://github.com/apache/pulsar/blob/4d64e2e66689381ebbb94fbfc03eb4e1dfba0405/managed-ledger/src/main/java/org/apache/bookkeeper/mledger/impl/ManagedLedgerImpl.java#L1559-L1566\r\n",
        "url": "https://github.com/apache/pulsar/pull/16647#issuecomment-1198850308"
    },
    {
        "project": "spring-framework",
        "id": 1146499026,
        "body": "> Please make the requested change and introduce a test that fails before this PR and passes after this PR.\r\n\r\nI've already added it. Please review, thanks.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28549#issuecomment-1146499026"
    },
    {
        "project": "spring-boot",
        "id": 1150239380,
        "body": "Thanks for the update @philwebb. I'll be afk for a few weeks, so feel free to rework my contribution as you see fit in my absence.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31230#issuecomment-1150239380"
    },
    {
        "project": "hadoop",
        "id": 1201654095,
        "body": "Can the unchecked flag be removed?\r\n\r\n```\r\n@SuppressWarnings(\"unchecked\")\r\npublic <T extends FSEditLogOp> T get(FSEditLogOpCodes opCode) {\r\n  return useCache ? (T)CACHE.get().get(opCode) : (T)newInstance(opCode);\r\n} \r\n```",
        "url": "https://github.com/apache/hadoop/pull/4667#issuecomment-1201654095"
    },
    {
        "project": "flink",
        "id": 1097657238,
        "body": "Maybe the first commit message should be renamed:\r\n\r\n`Fix SavepointITCase#testStopWithSavepointFailingAfterSnapshotCreation`\r\n\r\nIn my understanding it doesn't really fix anything and just aligns the code path for both schedulers. Is that correct?",
        "url": "https://github.com/apache/flink/pull/19439#issuecomment-1097657238"
    },
    {
        "project": "druid",
        "id": 292977631,
        "body": "I'll take a closer look at that in a bit. I guess you would propose that query endpoints like QueryResource should accept a QueryWithContext? My first thought is that there's not a serious need to reorganize Query to split out the context. Some considerations:\r\n\r\n1. IMO, we don't want to change the query API `/druid/v2/` as part of this change. So queries submitted there should still include a context.\r\n2. Sometimes \"internal\" context parameters are useful for users to be able to provide, like `finalize` (if a user wants to get complex metrics in raw form), `queryId` (if a user wants to use a specific query id).\r\n3. We need a way for users to provide parameters that affect execution but not results. For example, `priority`, `timeout`, `useCache`, `populateCache`, `groupByStrategy`.\r\n4. We need a way for brokers to pass down information to compute nodes, which needs to be part of the serialized form of a query. For example, when queries are issued from the broker, the groupBy query populates `groupByStrategy` if it wasn't set by the user, to ensure the same strategy is used on all nodes.\r\n5. Maybe not the best, but, sometimes query context parameters that users provide _do_ affect the results and need to be included in the cache key. For example: the timeseries parameter `skipEmptyBuckets`. That probably should have originally been a top level thing rather than a context parameter, but we should retain it in context for query API compatibility.\r\n\r\nThese points, taken as a whole, to me suggest it makes sense to keep the current design of Query. It satisfies all these needs well and does that in a relatively simple way.\r\n\r\nIt still makes sense to me to add a QueryPlus object and change QueryRunner to take that, but the \"plus\" wouldn't be query context, it would be probably response context and queryMetrics.",
        "url": "https://github.com/apache/druid/pull/4131#issuecomment-292977631"
    },
    {
        "project": "spring-security",
        "id": 936453977,
        "body": "Hi, @ahus1, thanks for the improvements!\r\n\r\nIf I understand correctly, it would make sense to backport the section id and AsciiDoc style listing changes. \r\n\r\nSince Antora is not used on earlier versions, I don't think it makes sense to backport those changes. Would you agree? If so, will you please separate those into two commits so that we can more easily backport the other improvements?",
        "url": "https://github.com/spring-projects/spring-security/pull/10345#issuecomment-936453977"
    },
    {
        "project": "apm-agent-java",
        "id": 813022351,
        "body": "> Do you fear eager linking if it's a static inner class? I don't think it should be an issue as they're compiled into separate class files.\r\n\r\nNo, I agree, there shouldn't be any linkage error due to inner classes. This is just a matter of source code.\r\nIt is very misleading to place sources of two classes at the same file, where it is important to load them in different times and with different dependencies. I would even say the ideal encapsulation would be to separate them to two different modules, where the instrumentation class cannot have a dependency in the advice class and the library classes, but I proposed a middle-way that will not require a very tough refactoring. What we have with this PR is only the [javadoc warning](https://github.com/elastic/apm-agent-java/pull/1736/files#diff-4d130dca8badaf3c00f2282a5751067e55ee9c47f9154595f3733e316fb70175R148-R161), but I think it's much easier to miss improper dependencies this way. An example: https://github.com/elastic/apm-agent-java/pull/1657/commits/c86ecb758089722693e620f5cdb7a0c2c11b41ee",
        "url": "https://github.com/elastic/apm-agent-java/pull/1736#issuecomment-813022351"
    },
    {
        "project": "flink",
        "id": 1048955937,
        "body": "@pnowojski, I entirely rewrote the code it is why I recreated all commits. So let's wait for the tests to be sure that I broke nothing and then we can review it again.",
        "url": "https://github.com/apache/flink/pull/18845#issuecomment-1048955937"
    },
    {
        "project": "spring-framework",
        "id": 1008660583,
        "body": "@rstoyanchev totally makes sense, I've reworked it according to your recommendations (netty, apache http 5, jetty).\r\np.s. I've noticed you marked these changes to the 6.x milestone - is there any chance this enhancement to be backported on 5.x ?",
        "url": "https://github.com/spring-projects/spring-framework/pull/27768#issuecomment-1008660583"
    },
    {
        "project": "flink",
        "id": 1028174877,
        "body": "## Some implementation notes\r\n\r\nBelow some implementation details worth to be mentioned\r\n\r\n### Why `SqlTryCastFunction` is `SqlKind.OTHER_FUNCTION`\r\n\r\nInitially I tried to use `SqlKind.CAST` for `SqlTryCastFunction`, as it effectively unlocks a couple of nice features from calcite, notably several optimizations and cast removals. This unfortunately also comes with some implicit assumptions done by calcite, after checking that the call is of `SqlKind.CAST`.\r\n\r\nIn particular, I hit a wall with `RexSimplify`, which was trying to optimize `TRY_CAST('non-numeric' AS BIGINT)` in a test. Because the value to cast is a literal, calcite tried to cast it at planning time, failing rather than converting the whole expression to `NULL`. Fixing this issue requires me to hardcode some custom logic in `RexSimplify`, which is not extensible.\r\n\r\n### Implicit casting doesn't fail\r\n\r\nIMHO this is a bigger issue than this PR, as the planner shouldn't try at all to perform implicit casting among types which casting can fail. To avoid substantial differences, I kept the situation as it is, but I suggest we go back on this issue later in future as we should disallow such implicit castings. \r\n\r\n### How the old legacy behavior is still supported\r\n\r\nInitially my idea to support the legacy behavior was to implement a planner rule converting every CAST to TRY_CAST, in case legacy behavior is enabled.\r\n\r\nThe problem with this approach is that converting to `TRY_CAST` during planning entails modifying the result type of the expression, eventually generating a breaking change anyway. For example, given a column defined as `f0 STRING NOT NULL` and the query `SELECT CAST(f0 AS TIME) FROM T`, the plan generated from master would be:\r\n\r\n```\r\nLogicalProject(EXPR$0=[CAST($0):TIME(0) NOT NULL])\r\n``` \r\n\r\nBut now, because `STRING` to `TIME` can fail, the plan should rather be:\r\n\r\n```\r\nLogicalProject(EXPR$0=[TRY_CAST($0):TIME(0)])\r\n``` \r\n\r\nSuch change effectively break the plans, as now a different nullability constraints can change the outcome of the query planning. Hence the user should handle this null as it would be with `TRY_CAST`. e.g. think about a function that accepts NOT NULL parameters and its argument is the above cast, that would not compile anymore with this change without user intervention to check the nullability, for example wrapping the cast in a `COALESCE`.\r\n\r\nOn top of that, Calcite doesn't allow to modify the output type of a `RelNode` inside rules (check `HepRuleCall#transformTo`), so modifying the type requires to perform this call rewrite at the SqlNode/Expression -> RexNode level, rather than in a planner rule.\r\n\r\nThen there's also the issue with implicit castings, placed here and there not just by our rules, but also by calcite various components'.\r\n\r\nBecause of the aforementioned problems, It's quite complicated to write a clean rule that does the job, which made me fallback to the _quick and dirty_ fix directly in the codegen.\r\n\r\nIt must be noted that, while on one hand having a rule nicely converting `CAST` to `TRY_CAST` changes the plan and constitutes a breaking change, it effectively makes the upgrade path for future Flink versions easier, as such rule effectively adapt the `CAST` semantics as if the user just used `TRY_CAST`.",
        "url": "https://github.com/apache/flink/pull/18611#issuecomment-1028174877"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 397394107,
        "body": "This PR is only exposing 1 more Extractor then is already exposed. As Field, Index, and Byte extractors are already exposed. As well as given the code that is/was already in place around MetadataExtractor it looked like the intent was already there with the Interface's existence and only 1 inheritance tree created (ScalaMetadataExtractor->PerEntityPoolingMetadataExtractor->MetadataExtractor).\r\n\r\nI think an argument could be made for further exposing the use of PerEntityPoolingMetadataExtractor. But not to expose MetadataExtractor and its usage seems kinda limiting in the amount of gained support for a user to be able to optimize their pipelines flows and get the best performance out of it. With this PR I was able to better optimize and reduce each flow by 1-2sec per 10sec batch on SparkStreaming. As before I was having to do filter/split of data to 2 separate ES-Hadoop instance to achieve the same effect. This saved me from having to write my own Index Pattern logic. As well as still maintain isolation of all the logic around ES into one central ES layer instead of trying to weave it into my parse/transform layers. ",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1159#issuecomment-397394107"
    },
    {
        "project": "pulsar",
        "id": 1198020631,
        "body": "Hi @codelipenghui\r\n\r\n> We have synchronized (pendingMarkDeleteOps) for both create cursor failure and `internalAsyncMarkDelete`, if triggered the ledger creation means the left thread got the lock and the right thread can only get the lock after the `mdEntry` is added to the `pendingMarkDeleteOps `, no?\r\n\r\nAccording to your tips, I found a new problem: see Flow-2 above. And I have modified the `Motivation`. \r\n\r\n\r\n@codelipenghui @Technoboy- @mattisonchao  Could you review this PR again?\r\n\r\n\r\n\r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/16841#issuecomment-1198020631"
    },
    {
        "project": "apm-agent-java",
        "id": 814889575,
        "body": "> For example, the RabbitMQ plugin is a good example of this where common instrumentation is tested with one RabbitMQ version and is then tested with multiple versions in integration tests.\r\n\r\nI don't understand why. There is no rigid definition of what integration test is, but let's assume it means a test that spins and communicates with another process that runs some complex third party software (not mock APM server for that matter). Then if a test is doing that, it gets this suffix, otherwise it doesn't. In fact, the RabbitMQ test was lately renamed to reflect that, and its two subclasses are suffixed with IT as well.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1748#issuecomment-814889575"
    },
    {
        "project": "apm-agent-java",
        "id": 780555912,
        "body": "> --pid is deprecated in favor of --include-pid\r\n\r\nSeems to contradict\r\n\r\n> The -p/--pid options have been removed in favor of the --include-pid option.\r\n\r\nGiven the number of changes here, breaking compatibility with the `--pid` option could be a relevant option to ensure that other changes are not silently ignored on upgrade.\r\n\r\nAlso, most of the support cases we had in the past with `--include/--exclude` were due to the fact that it was relying on `jps` output parsing which isn't the most straightforward.\r\n\r\nWhat exactly would be the use-case of the attacher without bundled agent in practice ? Having them bundled together tends to simplify deployment and compatibility checks.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1667#issuecomment-780555912"
    },
    {
        "project": "pulsar",
        "id": 1197649855,
        "body": "@codelipenghui @Technoboy- @AlphaWang @congbobo184 @gaozhangmin I extract a new class `ClientCnxIdleState` to make `ClientCnx` more simple, could you review this PR again?",
        "url": "https://github.com/apache/pulsar/pull/16165#issuecomment-1197649855"
    },
    {
        "project": "skywalking",
        "id": 1144926453,
        "body": "Pending on @kezhenxu94, If you need this to update dashboards, merge if you want.",
        "url": "https://github.com/apache/skywalking/pull/9170#issuecomment-1144926453"
    },
    {
        "project": "skywalking",
        "id": 524563334,
        "body": "```\r\n        final ByteBuddy byteBuddy = new ByteBuddy()\r\n            .with(TypeValidation.of(Config.Agent.IS_OPEN_DEBUGGING_CLASS));\r\n\r\n        Junction<TypeDescription> ignores = nameStartsWith(\"net.bytebuddy.\");\r\n\r\n//change start \r\n        String ignoreList = System.getProperty(\"skywalking_ignore_class_prefixlist\");\r\n        if (ignoreList != null && (ignoreList = ignoreList.trim()).length() > 0) {\r\n            logger.info(\"skywalking_ignore_class_prefixlist:{}\",ignoreList);\r\n            for (String p : ignoreList.split(\"[ ,;:]+\")) {\r\n                ignores = ignores.or(nameStartsWith(p));\r\n            }\r\n        }\r\n//change end\r\n\r\n        ignores = ignores.or(nameStartsWith(\"org.slf4j.\"))\r\n        .or(nameStartsWith(\"org.apache.logging.\"))\r\n        .or(nameStartsWith(\"org.groovy.\"))\r\n        .or(nameContains(\"javassist\"))\r\n        .or(nameContains(\".asm.\"))\r\n        .or(nameStartsWith(\"sun.reflect\"))\r\n        .or(allSkyWalkingAgentExcludeToolkit())\r\n        .or(ElementMatchers.<TypeDescription>isSynthetic());\r\n        new AgentBuilder.Default(byteBuddy)\r\n            .ignore(\r\n                ignores)\r\n            .type(pluginFinder.buildMatch())\r\n            .transform(new Transformer(pluginFinder))\r\n            .with(new Listener())\r\n            .installOn(instrumentation);\r\n```\r\n\r\nthen -Dskywalking_ignore_class_prefixlist=com.jprofiler",
        "url": "https://github.com/apache/skywalking/pull/3314#issuecomment-524563334"
    },
    {
        "project": "hadoop",
        "id": 1204496373,
        "body": "I am ignoring the 2 checkstyle violations for the following reasons:\r\n\r\n------\r\n\r\n```\r\n./hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java:285:  /**: First sentence should end with a period. [JavadocStyle]\r\n```\r\n\r\nThis is an existing comment: https://github.com/apache/hadoop/blob/60433bffc3a7fbb2153a78782d257534f8c7e34f/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CorruptReplicasMap.java#L285\r\n\r\nSince I am not modifying this comment (i.e. lines of code) in any way, I think its better that I don't touch it\r\n\r\n-------\r\n\r\n```\r\n./hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java:1932:  public void testDeleteCorruptReplicaForUnderReplicatedBlockInternal() throws Exception {:3: Method length is 233 lines (max allowed is 150). [MethodLength]\r\n```\r\n\r\n\"testDeleteCorruptReplicaForUnderReplicatedBlockInternal\" is an existing method \"testDeleteCorruptReplicaForUnderReplicatedBlock\" which was renamed. Since the method was already merged I don't think its necessary that I reduce the number of lines in the method.\r\n",
        "url": "https://github.com/apache/hadoop/pull/4568#issuecomment-1204496373"
    },
    {
        "project": "spring-boot",
        "id": 903915974,
        "body": "@bono007 Are you interested in reworking this to support multiple paths?  In light of https://github.com/spring-projects/spring-boot/issues/27306#issuecomment-903907394, that can be done without considering the disk space health indicator.  I think we'd probably end up with a `MetricBinder` implementation that consumes the paths. When called, it would create one `DiskSpaceMetrics` instance per path and binds it to the registry.",
        "url": "https://github.com/spring-projects/spring-boot/pull/27660#issuecomment-903915974"
    },
    {
        "project": "apm-agent-java",
        "id": 577033130,
        "body": "> If we do this through the message body then we will need a mechanism to override the trace ID on the consumer end to stitch together. Is it possible to override the trace ID?\r\n\r\nNo need to override the trace ID. If your Kafka messages carry textual body, you can use our public API: use [`injectTraceHeaders`](https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-transaction-inject-trace-headers) on the sender side, where you would implement the `HeaderInjector` interface to add the traceparent header (as key and value) to the message body. An interceptor would be a good candidate to doing that. \r\nOn the consumer side, you need to identify the proper location in your code that would best represent the message-handling transaction (for example- the method that encapsulates the message-handling logic) and call [`startTransactionWithRemoteParent`](https://www.elastic.co/guide/en/apm/agent/java/current/public-api.html#api-start-transaction-with-remote-parent-header), where you would provide your `HeaderExtractor` implementation that knows how to read the value of a given field from the message body.\r\nWhen using these APIs, the agent will treat these transactions like it does in any other distributed  traces.",
        "url": "https://github.com/elastic/apm-agent-java/pull/981#issuecomment-577033130"
    },
    {
        "project": "flink",
        "id": 1195637892,
        "body": "> It's very common for deserializers to include an implementation of getProducedType. [...] why don't we need that here?\r\n\r\nWe extend the `AbstractDeserializationSchema` which takes care of that based on the class/typeinfo passed to the constructor.",
        "url": "https://github.com/apache/flink/pull/20355#issuecomment-1195637892"
    },
    {
        "project": "pulsar",
        "id": 1199155775,
        "body": "> Want to align requirements firstly\r\n\r\nThanks, @Anonymitaet aligned the requirements with me this afternoon, and I will improve it.",
        "url": "https://github.com/apache/pulsar/pull/16836#issuecomment-1199155775"
    },
    {
        "project": "flink",
        "id": 1203877925,
        "body": "lgtm from my side however i'm not a committer => i can't merge\r\n\r\nMeanwhile the thing that could be improved and probably could be asked by committers is explicitly mentioning where description is fixed + filling PR form according to template.\r\nAlso this link might be useful https://flink.apache.org/contributing/code-style-and-quality-pull-requests.html",
        "url": "https://github.com/apache/flink/pull/20424#issuecomment-1203877925"
    },
    {
        "project": "hadoop",
        "id": 1201689352,
        "body": "> Personally, I think this optimization should still be valuable, I hope that the submitted pr is not at the class level, at least at the moudle level.\r\n\r\nI wouldn't go for one per class but maybe subpackage.\r\nI also think this PR is doing a couple of things more other than adding lambdas.\r\nFor example, adjusting for iterations.\r\n",
        "url": "https://github.com/apache/hadoop/pull/4668#issuecomment-1201689352"
    },
    {
        "project": "druid",
        "id": 1196168007,
        "body": "@rohangarg, thanks for fixing this flaky test! Thanks also for your detailed analysis. From prior observations, and based on your analysis, here's my guess as to what was happening. We spin up the Docker cluster and then start tests. The current ITs don't check the status of the cluster before they run: they simply rely on the retry utility to handle any failures.\r\n\r\nI've noticed that, on a single machine, it can take significant time for the cluster to come up. The \"new ITs\" specifically check the health of each service before starting the tests, and I've seen that this can take a while.\r\n\r\nSo, in the scenario you describe, it may be that the good historical comes up and serves the 50 queries before the bad one has had a chance to get itself organized. So, an alternative fix would have been to check the health of all services before running the tests.\r\n\r\nStill, that leaves the statistical nature of the failure detection, which is unappealing. The deterministic mechanism you implemented seems much better.\r\n\r\nOne suggestion: please add your explanation of the new behavior to the test code itself as a comment. I had a hard time figuring out what the code was doing until I read your explanation, which cleared thing right up. Would be great to preserve that explanation for future readers of the code. ",
        "url": "https://github.com/apache/druid/pull/12818#issuecomment-1196168007"
    },
    {
        "project": "spring-security",
        "id": 1016968629,
        "body": "Thank you for the PR!\r\n\r\nI did a quick comparison on the build performance and it appears that it is slower when running with the PR's changes of using a forked compiler (56 seconds) than the original code of not using a forked compiler (34 seconds). My test was rather simple and naive, so please correct me if I am missing something. I first ran `./gradlew clean` to ensure nothing was built. Then I ran `./gradlew classes testClasses integrationTestClasses --no-build-cache` to compile everything and looked at the build time. Note that `--no-build-cache` is important as we make use of Gradle Enterprise Cache which will impact the build if it has already been built.\r\n\r\nThe [documentation](https://docs.gradle.org/current/userguide/performance.html#compiler_daemon) states:\r\n\r\n> It\u2019s unlikely to be useful for small projects, but you should definitely consider it if a single task is compiling close to a thousand or more source files together.\r\n\r\nWhile the entire project has more than a thousand source files, there is a separate compile task per source set per project, so the count is much smaller per \"single task\".\r\n\r\nIn short, I think this option actually slows the Spring Security build down. Please let me know if you see any problems with my logic here.",
        "url": "https://github.com/spring-projects/spring-security/pull/10679#issuecomment-1016968629"
    },
    {
        "project": "pulsar",
        "id": 767427730,
        "body": "> I am happy to help Pulsar to redesign their CI to be a bit more friendlier towards other projects? I have quite an experience with that - in Apache Airflow I introduced changes that brought the usage of Github Actions by 70% maintaining it's usefulness and I am pretty sure the way you are using GitHub Actions can be optimised. Super Happy to help with that if needed.\r\n\r\n@potiuk any help is very appreciated\r\nas @zymap says the problem is that we have lots of tests and they take much time.\r\n\r\nWe are working on fixing all of the flakes, but this will take time. This will help because we won't need to rerun PR validation more times than needed.\r\n\r\nProbably it would be interesting to try to merge the workflows: all java tests, all integration tests, all cpp....in some test GitHub repository and work on a new setup.\r\nWe also should remove the 'retry' at build script level.\r\n\r\nif we let the build continue in case of errors we should have some tool to gather all of the failures and we must be sure that we are not missing any of them\r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/9159#issuecomment-767427730"
    },
    {
        "project": "skywalking",
        "id": 706905179,
        "body": "The config didn't work from the 8.0 or 8.1. Actually, the backend initializer doesn't support list or map :) I just refactored the codes.",
        "url": "https://github.com/apache/skywalking/pull/5650#issuecomment-706905179"
    },
    {
        "project": "spring-security",
        "id": 469628713,
        "body": "Sure @d3jie \r\n\r\nIn `createDefault()`, you had...\r\n\r\n\r\n```java\r\nReactiveAuthenticationManager result = new OAuth2LoginReactiveAuthenticationManager(getAccessTokenResponseClient(), getOauth2UserService());\r\n\r\n...\r\n\r\nOidcAuthorizationCodeReactiveAuthenticationManager oidc = new OidcAuthorizationCodeReactiveAuthenticationManager(getAccessTokenResponseClient(), getOidcUserService());\r\n```\r\n\r\nI changed it to...\r\n\r\n```java\r\nReactiveOAuth2AccessTokenResponseClient<OAuth2AuthorizationCodeGrantRequest> client = getAccessTokenResponseClient();\r\n\r\nReactiveAuthenticationManager result = new OAuth2LoginReactiveAuthenticationManager(client, getOauth2UserService());\r\n\r\n...\r\n\r\nOidcAuthorizationCodeReactiveAuthenticationManager oidc = new OidcAuthorizationCodeReactiveAuthenticationManager(client, getOidcUserService());\r\n```\r\n\r\nSo we re-use the `client` instance in both managers if the `@Bean` **was not** provided.\r\n\r\n\r\nAnd in`OAuth2LoginTests`, I renamed the `@Bean` method from `oAuth2AccessTokenResponseClient()` to `accessTokenResponseClient()`. We typically don't prefix member variables or method declarations with `oauth2*`.\r\n\r\nThose were the only 2 changes.",
        "url": "https://github.com/spring-projects/spring-security/pull/6587#issuecomment-469628713"
    },
    {
        "project": "apm-agent-java",
        "id": 1208429186,
        "body": "\n## :green_heart: Build Succeeded\n\n\n<!-- BUILD BADGES-->\n> _the below badges are clickable and redirect to their specific view in the CI or DOCS_\n[![Pipeline View](https://img.shields.io/badge/pipeline-pipeline%20-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//pipeline) [![Test View](https://img.shields.io/badge/test-test-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//tests) [![Changes](https://img.shields.io/badge/changes-changes-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//changes) [![Artifacts](https://img.shields.io/badge/artifacts-artifacts-yellow)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2737/detail/PR-2737/1//artifacts) [![preview](https://img.shields.io/badge/docs-preview-yellowgreen)](http://apm-agent-java_2737.docs-preview.app.elstc.co/diff)  [![preview](https://img.shields.io/badge/elastic-observability-blue)](https://ci-stats.elastic.co/app/apm/services/apm-ci/transactions/view?rangeFrom=2022-08-08T16:53:08.684Z&rangeTo=2022-08-08T17:13:08.684Z&transactionName=BUILD+apm-agent-java%2Fapm-agent-java-mbp%2FPR-%7Bnumber%7D&transactionType=job&latencyAggregationType=avg&traceId=354e21e5c7a68593612a4be27ce06854&transactionId=5b51422422076baf)\n\n<!-- BUILD SUMMARY-->\n<details><summary>Expand to view the summary</summary>\n<p>\n\n#### Build stats\n\n\n\n\n* Start Time: 2022-08-08T17:03:08.684+0000\n\n\n* Duration: 50 min 5 sec\n\n\n\n\n#### Test stats :test_tube:\n\n| Test         | Results                         |\n| ------------ | :-----------------------------: |\n| Failed       | 0  |\n| Passed       | 3081  |\n| Skipped      | 36 |\n| Total        | 3117   |\n\n\n</p>\n</details>\n\n<!-- TEST RESULTS IF ANY-->\n\n\n<!-- STEPS ERRORS IF ANY -->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## :green_heart: Flaky test report\nTests succeeded.\n\n\n\n## :robot: GitHub comments\n\nTo re-run your PR in the CI, just comment with:\n\n\n\n- `/test` : Re-trigger the build.\n\n\n- `run benchmark tests` : Run the benchmark tests.\n\n\n- `run jdk compatibility tests` : Run the JDK Compatibility tests.\n\n\n- `run integration tests` : Run the Agent Integration tests.\n\n\n- `run end-to-end tests` : Run the APM-ITs.\n\n\n- `run windows tests` : Build & tests on windows.\n\n\n- `run` `elasticsearch-ci/docs` : Re-trigger the docs validation. (use unformatted text in the comment!)\n\n\n\n\n<!--COMMENT_GENERATED_WITH_ID_comment.id-->",
        "url": "https://github.com/elastic/apm-agent-java/pull/2737#issuecomment-1208429186"
    },
    {
        "project": "apm-agent-java",
        "id": 876513726,
        "body": "I don't think this litte enhancement is worth an entry in supported-technologies.asciidoc",
        "url": "https://github.com/elastic/apm-agent-java/pull/1905#issuecomment-876513726"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1012421189,
        "body": "Here's an example of the warning I had been getting before this change:\r\n```\r\nResolution of the configuration :elasticsearch-hadoop-mr:itestRuntimeClasspath was attempted from a context different than the project context. Have a look at the documentation to understand why this is a problem and how it can be resolved. This behaviour has been deprecated and is scheduled to be removed in Gradle 8.0. See https://docs.gradle.org/7.3/userguide/viewing_debugging_dependencies.html#sub:resolving-unsafe-configuration-resolution-errors for more details.\r\nResolution of the configuration :elasticsearch-storm:itestRuntimeClasspath was attempted from a context different than the project context. Have a look at the documentation to understand why this is a problem and how it can be resolved. This behaviour has been deprecated and is scheduled to be removed in Gradle 8.0. See https://docs.gradle.org/7.3/userguide/viewing_debugging_dependencies.html#sub:resolving-unsafe-configuration-resolution-errors for more details.\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 8.0```",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1868#issuecomment-1012421189"
    },
    {
        "project": "spring-boot",
        "id": 328465773,
        "body": "I wonder if we should add `mockito-inline` to `spring-boot-starter-test`?",
        "url": "https://github.com/spring-projects/spring-boot/pull/10239#issuecomment-328465773"
    },
    {
        "project": "apm-agent-java",
        "id": 1201330774,
        "body": "I have fixed the instrumentation and it now works as expected for Vert.x 4.3.2.\r\nThe downside is that the 4.x plugin is now compiled and tested against only this version, and not 4.3.1 anymore.\r\n\r\nThus maybe the next step (maybe as a follow-up PR) here is :\r\n- add an integration test that cover the 3.x and 4.x versions, which would allow keep testing 4.3.1, but that might be an extensive task as tests are currently relying on Junit5.\r\n- (optional) remove some duplicated tests for 3.x",
        "url": "https://github.com/elastic/apm-agent-java/pull/2700#issuecomment-1201330774"
    },
    {
        "project": "druid",
        "id": 308265592,
        "body": "@freakyzoidberg please don't close and recreate PR in this situation in the future, just add commits on top.",
        "url": "https://github.com/apache/druid/pull/4399#issuecomment-308265592"
    },
    {
        "project": "flink",
        "id": 1160403004,
        "body": "> The required classes are dynamically changing when users interactively ADD JAR, so we have to replace the classloader or update classloader in place.\r\n\r\nAKA the classloader provided earlier does not contain everything it needs _now_. If the requirements can change then the API of these components should be adjusted to support that.\r\n\r\n> the classloader is clearly managed by the components.\r\n\r\nThat's quite a statement given that something external can just mutate the classloader under the hood without any these components being aware of it.\r\n\r\n> Spark SQL also introduced MutableURLClassLoader to support ADD JAR\r\n\r\nSince when is Spark SQL our measure for whether something is a good idea?",
        "url": "https://github.com/apache/flink/pull/20003#issuecomment-1160403004"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 541753024,
        "body": "When I run `./gradlew clean build` locally I'm getting:\r\n\r\n```\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/Users/james.baiera/Documents/source/elasticsearch-hadoop/hive/build.gradle' line: 2\r\n\r\n* What went wrong:\r\nA problem occurred evaluating project ':elasticsearch-hadoop-hive'.\r\n> Failed to apply plugin [class 'org.elasticsearch.gradle.DistributionDownloadPlugin']\r\n   > Cannot get property 'bwcVersions' on extra properties extension as it does not exist\r\n```\r\n\r\nI don't have gradle scans set up yet but I can get one up if needed.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1365#issuecomment-541753024"
    },
    {
        "project": "spring-security",
        "id": 1184787360,
        "body": "Thanks, @evgeniycheban. This is now merged into `5.8.x`. I also added a polish commit 5dff157755ed0fae8c68e91dc8e56cefa1b8fe65 to update the formatting of the `HttpSecurity` blocks in the tests you added.",
        "url": "https://github.com/spring-projects/spring-security/pull/11495#issuecomment-1184787360"
    },
    {
        "project": "pulsar",
        "id": 1175880638,
        "body": "I have a question:\r\nwe use pulsar-2.8.0 in pro env, but this pr was merged into master, could you please merge this pr into pulsar-2.8.0's new tag, then I can use this new tag to replace our pro env pulsar-2.8.0 directly.",
        "url": "https://github.com/apache/pulsar/pull/16313#issuecomment-1175880638"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 705203573,
        "body": "> what is the scenario where having spark as a compile-time/runtime dependency is useful?\r\n\r\nGoing off what I remember from April, the driving force for this change was 2-fold: ES-Hadoop was being updated to better comply with Gradle's project modeling and the plugin that provided this functionality is a third party plugin that we have little to no control over. A lot of changes were happening in order to support cross compilation of Scala/Spark artifacts, and getting the community plugin that added `provided` and `optional` scopes to work with the changes we were making was becoming a maintenance nightmare.\r\n\r\nAt the time it seemed logical that a project depending on ES-Hadoop/Spark would already have defined Spark as a provided dependency, and thus having Spark as a compile time dependency of the library (which would end up transitive) would be relatively innocuous to the end-user project development environment.\r\n\r\nAs for the topic of deployments, my expectation at the time was that most deployments make use of the spark submit jar arguments to specify their dependencies. This was informed mostly from my experiences with the community over the years: Using the spark submit jar argument was the more common approach. Users who bundled their application and libraries together would just need to exclude Spark from that artifact. A small extra step.\r\n\r\nWith this line of thinking, we removed the plugin, set the scopes to what would be the most reasonable scopes Gradle could support out of the box, and moved on with our refactoring. Several months after this change though, I'm a bit more skeptical that the spark submit jar model of deployment is as ubiquitous as previously thought.\r\n\r\nI'll open an issue for returning the dependencies to `provided`. I'm not sure how reasonable it would be to return the previously `optional` dependencies, which at the time was the Spark SQL libraries, as well as every integration library in the uberjar. Those would most likely make a return as `provided` if we went forward with that change.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1452#issuecomment-705203573"
    },
    {
        "project": "spring-boot",
        "id": 1031868062,
        "body": "I like the `appending` prefix on the section titles, but we should update [`anchor-rewrite.properties`](https://github.com/spring-projects/spring-boot/blob/main/spring-boot-project/spring-boot-docs/src/docs/asciidoc/anchor-rewrite.properties) so that older links work.\r\n\r\nthe format of that file is pretty self explanatory, but it's also covered [here](https://github.com/spring-io/spring-asciidoctor-backends#anchor-rewriting)",
        "url": "https://github.com/spring-projects/spring-boot/pull/29667#issuecomment-1031868062"
    },
    {
        "project": "skywalking",
        "id": 915358643,
        "body": "let's try to chance jdk versions e2e testing? Remove some version or rename? It is strange they always fail on that mission only.",
        "url": "https://github.com/apache/skywalking/pull/7681#issuecomment-915358643"
    },
    {
        "project": "skywalking",
        "id": 900853862,
        "body": "# [Codecov](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report\n> Merging [#7475](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (59d4138) into [master](https://codecov.io/gh/apache/skywalking/commit/39d23c16785b73b7607faa2b09ff3db94cd5333a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (39d23c1) will **decrease** coverage by `0.24%`.\n> The diff coverage is `48.27%`.\n\n> :exclamation: Current head 59d4138 differs from pull request most recent head ddd842a. Consider uploading reports for the commit ddd842a to get more accurate results\n[![Impacted file tree graph](https://codecov.io/gh/apache/skywalking/pull/7475/graphs/tree.svg?width=650&height=150&src=pr&token=qrILxY5yA8&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)\n\n```diff\n@@             Coverage Diff              @@\n##             master    #7475      +/-   ##\n============================================\n- Coverage     58.81%   58.57%   -0.25%     \n+ Complexity     4390     4371      -19     \n============================================\n  Files          1035     1035              \n  Lines         26592    26614      +22     \n  Branches       2627     2629       +2     \n============================================\n- Hits          15640    15589      -51     \n- Misses         9571     9641      +70     \n- Partials       1381     1384       +3     \n```\n\n\n| [Impacted Files](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage \u0394 | |\n|---|---|---|\n| [.../provider/log/listener/RecordAnalysisListener.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9wcm92aWRlci9sb2cvbGlzdGVuZXIvUmVjb3JkQW5hbHlzaXNMaXN0ZW5lci5qYXZh) | `72.41% <\u00f8> (\u00f8)` | |\n| [...g/oap/log/analyzer/dsl/spec/filter/FilterSpec.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvc3BlYy9maWx0ZXIvRmlsdGVyU3BlYy5qYXZh) | `37.33% <30.76%> (-4.21%)` | :arrow_down: |\n| [...pache/skywalking/oap/log/analyzer/dsl/Binding.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvQmluZGluZy5qYXZh) | `59.18% <50.00%> (-1.80%)` | :arrow_down: |\n| [...log/analyzer/dsl/spec/extractor/ExtractorSpec.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvc3BlYy9leHRyYWN0b3IvRXh0cmFjdG9yU3BlYy5qYXZh) | `27.88% <75.00%> (+0.88%)` | :arrow_up: |\n| [...er/provider/handler/MeterServiceHandlerCompat.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcmVjZWl2ZXItcGx1Z2luL3NreXdhbGtpbmctbWV0ZXItcmVjZWl2ZXItcGx1Z2luL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9za3l3YWxraW5nL29hcC9zZXJ2ZXIvcmVjZWl2ZXIvbWV0ZXIvcHJvdmlkZXIvaGFuZGxlci9NZXRlclNlcnZpY2VIYW5kbGVyQ29tcGF0LmphdmE=) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |\n| [...r/handler/JVMMetricReportServiceHandlerCompat.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcmVjZWl2ZXItcGx1Z2luL3NreXdhbGtpbmctanZtLXJlY2VpdmVyLXBsdWdpbi9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvc2t5d2Fsa2luZy9vYXAvc2VydmVyL3JlY2VpdmVyL2p2bS9wcm92aWRlci9oYW5kbGVyL0pWTU1ldHJpY1JlcG9ydFNlcnZpY2VIYW5kbGVyQ29tcGF0LmphdmE=) | `0.00% <0.00%> (-100.00%)` | :arrow_down: |\n| [.../rest/BrowserErrorLogReportListServletHandler.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcmVjZWl2ZXItcGx1Z2luL3NreXdhbGtpbmctYnJvd3Nlci1yZWNlaXZlci1wbHVnaW4vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL3NlcnZlci9yZWNlaXZlci9icm93c2VyL3Byb3ZpZGVyL2hhbmRsZXIvcmVzdC9Ccm93c2VyRXJyb3JMb2dSZXBvcnRMaXN0U2VydmxldEhhbmRsZXIuamF2YQ==) | `22.22% <0.00%> (-66.67%)` | :arrow_down: |\n| [...earch7/query/ProfileThreadSnapshotQueryEs7DAO.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItc3RvcmFnZS1wbHVnaW4vc3RvcmFnZS1lbGFzdGljc2VhcmNoNy1wbHVnaW4vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL3NlcnZlci9zdG9yYWdlL3BsdWdpbi9lbGFzdGljc2VhcmNoNy9xdWVyeS9Qcm9maWxlVGhyZWFkU25hcHNob3RRdWVyeUVzN0RBTy5qYXZh) | `50.00% <0.00%> (-50.00%)` | :arrow_down: |\n| [...dler/rest/BrowserPerfDataReportServletHandler.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcmVjZWl2ZXItcGx1Z2luL3NreXdhbGtpbmctYnJvd3Nlci1yZWNlaXZlci1wbHVnaW4vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL3NlcnZlci9yZWNlaXZlci9icm93c2VyL3Byb3ZpZGVyL2hhbmRsZXIvcmVzdC9Ccm93c2VyUGVyZkRhdGFSZXBvcnRTZXJ2bGV0SGFuZGxlci5qYXZh) | `38.70% <0.00%> (-45.17%)` | :arrow_down: |\n| [...vider/handler/ProfileTaskServiceHandlerCompat.java](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcmVjZWl2ZXItcGx1Z2luL3NreXdhbGtpbmctcHJvZmlsZS1yZWNlaXZlci1wbHVnaW4vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL3NlcnZlci9yZWNlaXZlci9wcm9maWxlL3Byb3ZpZGVyL2hhbmRsZXIvUHJvZmlsZVRhc2tTZXJ2aWNlSGFuZGxlckNvbXBhdC5qYXZh) | `0.00% <0.00%> (-40.00%)` | :arrow_down: |\n| ... and [11 more](https://codecov.io/gh/apache/skywalking/pull/7475/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)\n> `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation). Last update [6bb4d35...ddd842a](https://codecov.io/gh/apache/skywalking/pull/7475?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation).\n",
        "url": "https://github.com/apache/skywalking/pull/7475#issuecomment-900853862"
    },
    {
        "project": "spring-boot",
        "id": 931479593,
        "body": "Thanks for the PR, @jonatan-ivanov.\r\n\r\nNote to the Boot team's future selves: I've labelled this for merging with amendments as we might rename some of the DTOs.",
        "url": "https://github.com/spring-projects/spring-boot/pull/28136#issuecomment-931479593"
    },
    {
        "project": "pulsar",
        "id": 1033140590,
        "body": "@lhotari thanks for the suggestion. Unfortunately it doesn't help. I inspected the `mockito-inline` codebase and I haven't found a way to by-pass this check. \r\n\r\nhttps://github.com/mockito/mockito/blob/c3d0847e6caee970e2db3ef77e1d31facf090550/src/main/java/org/mockito/internal/creation/bytebuddy/InlineDelegateByteBuddyMockMaker.java#L544-L558",
        "url": "https://github.com/apache/pulsar/pull/14098#issuecomment-1033140590"
    },
    {
        "project": "hadoop",
        "id": 889861061,
        "body": "Force pushed the branch.\r\n - Created a separate Jira for the unit test refactor (YARN-10874).\r\n - Extracting the variable names from the bash version is not feasible with regexp, so that change is abandoned.\r\n \r\nNOTE: the new regex batch variable name extraction works for the %:% variable, it will extract the : as the name. The original code did not find the : character to be a variable name. (https://stackoverflow.com/questions/37973141/colon-in-batch-variable-name/37992843). Since the test refactor is moved to separate pull request, I will add an extra test case with %:% to this patch if the other pull request is merged before this change.\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/3220#issuecomment-889861061"
    },
    {
        "project": "apm-agent-java",
        "id": 1030853364,
        "body": "I actually intend to get rid of `TraceContext#applicationClassLoader` as part of #2428 ...\r\nWe don't need this reference anymore and it's always a good idea to store one less weak reference (and any CL reference).\r\nSince you are actually only interested in the implicit side-effect of `TraceContext#setApplicationClassLoader`, which is to override the service info for the transaction, the API seems inappropriate to me. Calling it `overrideServiceInfoFor(ClassLoader)` would be better (the javadoc would how the provided `ClassLoader` is used).\r\n\r\nGenerally speaking, I would prefer a cleaner and more generic API such as `overrideServiceInfo(String serviceName, String serviceVersion)`. If extracting the service version and name are simple enough- just implement that instead of relying on the automatic discovery. If relying on the automatic discovery is important, I assume this is mostly relevant for external plugins (not for other usages of the API), so we can expose `ElasticApmTracer#getServiceInfo(ClassLoader)` through the sdk by making it part of the `co.elastic.apm.agent.impl.Tracer` interface. WDYT?",
        "url": "https://github.com/elastic/apm-agent-java/pull/2444#issuecomment-1030853364"
    },
    {
        "project": "apm-agent-java",
        "id": 1014364875,
        "body": "As per comments in the shared doc on test naming, I would suggest instead to use the \"test scope\" to remove ambiguities:\r\n- \"Agent Integration Tests\", for what we call \"integration tests\" in the agent, we could then later split that into \"Plugins Integration Tests\" and \"Core Integration Tests\" if we need to.\r\n- \"APM Integration Tests\" could also be used as an alternative to \"End-to-End Integration Tests\", but I'm fine to keep this one as you suggested.\r\n\r\nFor simplicity, I think it could be relevant to have a 1:1 mapping between maven profiles and the pipeline stages, so each stage maps to a maven profile, and \"test partitioning\" is written once for all at the maven profiles level instead of enabling/disabling profiles in a shell script.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2393#issuecomment-1014364875"
    },
    {
        "project": "spring-security",
        "id": 741884140,
        "body": "No. We follow semantic versioning. Generally, enhancements are only included in the latest branch. The other branches are for patches only to reduce risk.",
        "url": "https://github.com/spring-projects/spring-security/pull/9147#issuecomment-741884140"
    },
    {
        "project": "pulsar",
        "id": 1196733223,
        "body": "> I don't see any suggestion in the javadoc that indicates the usage of `clearInlineMocks()` could cause the error we're facing, would you mind to indicate more details? @nodece\r\n\r\nI just guess the log is related to #15513. \r\n\r\n\r\n>In certain specific, rare scenarios (issue https://github.com/mockito/mockito/pull/1619) inline mocking causes memory leaks. There is no clean way to mitigate this problem completely. Hence, we introduced a new API to explicitly clear mock state (only make sense in inline mocking!). See example usage in [MockitoFramework.clearInlineMocks()](https://javadoc.io/static/org.mockito/mockito-core/4.6.1/org/mockito/MockitoFramework.html#clearInlineMocks()). If you have feedback or a better idea how to solve the problem please reach out.\r\n\r\nLink to https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#47\r\n",
        "url": "https://github.com/apache/pulsar/pull/16820#issuecomment-1196733223"
    },
    {
        "project": "apm-agent-java",
        "id": 1192521307,
        "body": "```\r\njava.lang.IllegalArgumentException: null\r\n\tat org.objectweb.asm.ClassVisitor.<init>(Unknown Source) ~[?:5.1]\r\n\tat org.objectweb.asm.ClassVisitor.<init>(Unknown Source) ~[?:5.1]\r\n\tat net.bytebuddy.pool.TypePool$Default$TypeExtractor.<init>(TypePool.java:8111) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.pool.TypePool$Default.parse(TypePool.java:856) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.pool.TypePool$Default.doDescribe(TypePool.java:841) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.pool.TypePool$Default$WithLazyResolution.access$001(TypePool.java:921) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.pool.TypePool$Default$WithLazyResolution.doResolve(TypePool.java:1019) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.pool.TypePool$Default$WithLazyResolution$LazyTypeDescription.delegate(TypePool.java:1088) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.description.type.TypeDescription$AbstractBase$OfSimpleType$WithDelegation.getDeclaredAnnotations(TypeDescription.java:8435) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.implementation.attribute.TypeAttributeAppender$ForInstrumentedType$Differentiating.<init>(TypeAttributeAppender.java:125) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.dynamic.scaffold.inline.RedefinitionDynamicTypeBuilder.<init>(RedefinitionDynamicTypeBuilder.java:79) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.ByteBuddy.redefine(ByteBuddy.java:886) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$TypeStrategy$Default$3.builder(AgentBuilder.java:2630) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer.doTransform(AgentBuilder.java:11888) ~[elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer.transform(AgentBuilder.java:11834) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer.access$1700(AgentBuilder.java:11551) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer$LegacyVmDispatcher.run(AgentBuilder.java:12234) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer$LegacyVmDispatcher.run(AgentBuilder.java:12174) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_202]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer.doPrivileged(AgentBuilder.java) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$ExecutingTransformer.transform(AgentBuilder.java:11743) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default$WarmupStrategy$Enabled.apply(AgentBuilder.java:11180) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default.doInstall(AgentBuilder.java:10910) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat net.bytebuddy.agent.builder.AgentBuilder$Default.installOn(AgentBuilder.java:10848) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat co.elastic.apm.agent.bci.ElasticApmAgent.initInstrumentation(ElasticApmAgent.java:279) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat co.elastic.apm.agent.bci.ElasticApmAgent.initInstrumentation(ElasticApmAgent.java:164) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat co.elastic.apm.agent.bci.ElasticApmAgent.initialize(ElasticApmAgent.java:150) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_202]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_202]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_202]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_202]\r\n\tat co.elastic.apm.agent.premain.AgentMain.loadAndInitializeAgent(AgentMain.java:160) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat co.elastic.apm.agent.premain.AgentMain.init(AgentMain.java:101) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat co.elastic.apm.agent.premain.AgentMain.premain(AgentMain.java:50) [elastic-apm-agent-1.33.1-SNAPSHOT.jar:1.33.1-SNAPSHOT]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_202]\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_202]\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_202]\r\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_202]\r\n\tat sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:386) [?:1.8.0_202]\r\n\tat sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:401) [?:1.8.0_202]\r\n```\r\n",
        "url": "https://github.com/elastic/apm-agent-java/pull/2720#issuecomment-1192521307"
    },
    {
        "project": "apm-agent-java",
        "id": 780562378,
        "body": "> --pid is deprecated in favor of --include-pid\r\n\r\nThat's outdated. I removed -p/--pid.\r\n\r\n> What exactly would be the use-case of the attacher without bundled agent in practice ? Having them bundled together tends to simplify deployment and compatibility checks.\r\n\r\nI think that this is the mode we'll be using when integrating APM Server. While we want to be able to update the Java agent independently from APM Server, there is a strong dependency between the UI, the server and the attacher-cli. These three components have to have a known set of features. If we were to implement a new type of include/exclude matcher in the attacher-cli in the future, we couldn't easily expose it in the UI if different versions of the attacher-cli may be used.\r\n\r\nSo, we'd bundle the attacher-cli-slim into APM Server and download the agent jar on demand, based on what's configured in the Fleet UI.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1667#issuecomment-780562378"
    },
    {
        "project": "hadoop",
        "id": 1186492545,
        "body": "```\r\n[ERROR] Tests run: 6803, Failures: 0, Errors: 2, Skipped: 24, Flakes: 8\r\n[INFO] \r\n[ERROR] There are test failures\r\n```\r\n\r\n```\r\n[ERROR] Errors: \r\n[ERROR] org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckCorruptWhenOneReplicaIsCorrupt(org.apache.hadoop.hdfs.server.namenode.TestFsck)\r\n[ERROR]   Run 1: TestFsck.testFsckCorruptWhenOneReplicaIsCorrupt \u00c2\u00bb Remote java.lang.NullPointer...\r\n[ERROR]   Run 2: TestFsck.testFsckCorruptWhenOneReplicaIsCorrupt \u00c2\u00bb Remote java.lang.NullPointer...\r\n[ERROR]   Run 3: TestFsck.testFsckCorruptWhenOneReplicaIsCorrupt \u00c2\u00bb Remote java.lang.NullPointer...\r\n[INFO] \r\n[ERROR] org.apache.hadoop.hdfs.server.namenode.ha.TestObserverNode.testMkdirsRaceWithObserverRead(org.apache.hadoop.hdfs.server.namenode.ha.TestObserverNode)\r\n[ERROR]   Run 1: TestObserverNode.testMkdirsRaceWithObserverRead:557 Client #3 lastSeenStateId=-9223372036854775808 activStateId=37\r\nnull\r\n[ERROR]   Run 2: TestObserverNode.testMkdirsRaceWithObserverRead:511 \u00c2\u00bb Connect Call From 653bab...\r\n[ERROR]   Run 3: TestObserverNode.cleanUp:111 \u00c2\u00bb Connect Call From 653bab42d979/172.17.0.2 to lo...\r\n[ERROR]   Run 4: TestObserverNode.testMkdirsRaceWithObserverRead:511 \u00c2\u00bb Connect Call From 653bab...\r\n[ERROR]   Run 5: TestObserverNode.cleanUp:111 \u00c2\u00bb Connect Call From 653bab42d979/172.17.0.2 to lo...\r\n```\r\n\r\n- TestObserverNode#testMkdirsRaceWithObserverRead is a flaky test: https://issues.apache.org/jira/browse/HDFS-15646\r\n\r\nThe other test failure is relevant TestFsck#testFsckCorruptWhenOneReplicaIsCorrupt:\r\n\r\n```\r\n[ERROR] Tests run: 35, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 281.876 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.TestFsck\r\n[ERROR] testFsckCorruptWhenOneReplicaIsCorrupt(org.apache.hadoop.hdfs.server.namenode.TestFsck)  Time elapsed: 7.446 s  <<< ERROR!\r\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap.removeFromCorruptReplicasMap(CorruptReplicasMap.java:145)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap.removeFromCorruptReplicasMap(CorruptReplicasMap.java:134)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeStoredBlock(BlockManager.java:4255)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.removeStaleReplicas(BlockManager.java:4262)\r\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.updateLastBlock(BlockManager.java:4772)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipelineInternal(FSNamesystem.java:6004)\r\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipeline(FSNamesystem.java:5966)\r\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updatePipeline(NameNodeRpcServer.java:1009)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updatePipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:1195)\r\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:620)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:588)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)\r\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1192)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1193)\r\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1116)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1919)\r\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3131)\r\n\r\n```\r\n",
        "url": "https://github.com/apache/hadoop/pull/4568#issuecomment-1186492545"
    },
    {
        "project": "skywalking",
        "id": 1172464008,
        "body": "I am not sure why we need this kind of changes. Are these improving performance or?\nSkyWalking don't require code styles. ",
        "url": "https://github.com/apache/skywalking/pull/9298#issuecomment-1172464008"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 384569927,
        "body": "Can you explain why do you think this change is necessary ?\r\nWe compute the number of partitions per shard since each map is responsible for a slice inside a specific shard. So we need to ensure that the mappers responsible for a specific shard handle all partitions created for that shard (from 0 to max slice id). With your modification we'd miss documents since all slices wouldn't run on all shards. \r\n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1141#issuecomment-384569927"
    },
    {
        "project": "spring-security",
        "id": 941160751,
        "body": "@qavid I think let's stick to JavaDoc improvements for this ticket.\r\n\r\nThe initial reason is that it simplifies maintenance to have a PR commit limit itself to the related ticket. Also, I think there is some discussion to be had as to whether or not the methods should handle null values since having a claim set with a null value seems like a configuration error.\r\n\r\nInstead of changing to `Assert.notNull`, would you please add the following to the JavaDoc:\r\n\r\n```\r\n@throws NullPointerException if the claim value is {@code null}\r\n```\r\n\r\nIf you feel like `ClaimAccessor` should handle null values, let's please open another ticket with the production use cases you have where a claim value is null.",
        "url": "https://github.com/spring-projects/spring-security/pull/10357#issuecomment-941160751"
    },
    {
        "project": "spring-framework",
        "id": 33982138,
        "body": "@philwebb Update SPR please, I recreated the pull request to fix the target branch (Should be easier to merge now), it is the same branch, just fixed the pull request target branch to be 3.2.x\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/454#issuecomment-33982138"
    },
    {
        "project": "pulsar",
        "id": 418471238,
        "body": "@rdhabalia I believe that having z-node created and deleted in very different parts of the code makes it very hard to figure out how things work. Having the entire logic for the lock and its cleanup within the same class encapsulate the complexity there and anyone reading that class can easily understand the behavior.\r\n\r\nRegarding `NoopLoadManager`: the standalone currently works with `ModularLoadManager`, but `ModularLoadManager` was really not designed for standalone. In fact, standalone mode doesn't need a load manager because there are no other brokers. All the features provided by a load manager are not used by standalone (and sometimes they complicate it): load-balancing, getting load reports, tracking rates of bundles, traffic shedding and so on.  All this is, in the best case, it's overhead to the functioning of Pulsar standalone. The `NoopLoadManager` short circuits all these features. The only piece of code it contains is the z-node registration, since it's needed in lookup logic. \r\n\r\n",
        "url": "https://github.com/apache/pulsar/pull/2487#issuecomment-418471238"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1042993320,
        "body": "There is no change in functionality here, just a better error message. If it was working before, it will continue to work.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1900#issuecomment-1042993320"
    },
    {
        "project": "spring-security",
        "id": 1182553200,
        "body": "Hi, @priya1712d. We also need to edit the task itself in `buildSrc`. If done correctly, you should be able to run `./gradlew publishMavenJavaPublicationToBuildDirectoryRepository` on your local machine and see the Spring Security jars copied to `{PROJECT_DIR}/build/publications/repos`.\r\n\r\nIt appears that `PublishLocalPlugin` may be where the change needs to be made.",
        "url": "https://github.com/spring-projects/spring-security/pull/11494#issuecomment-1182553200"
    },
    {
        "project": "spring-framework",
        "id": 823944437,
        "body": "Hey @dreis2211 it's indeed worth handling the common `java.annotation.Valid` case explicitly there, and also worth extracting the evaluation logic. However, it'll have to be a new `org.springframework.validation.annotation.ValidationAnnotationUtils` class in order to avoid a package cycle, since it refers to the `Validated` annotation in the subpackage there. If you could refactor it that way with a `since` marker for 5.3.7, we'd be happy to roll it in...",
        "url": "https://github.com/spring-projects/spring-framework/pull/26787#issuecomment-823944437"
    },
    {
        "project": "flink",
        "id": 1097361070,
        "body": "Thanks a lot for your review @fapaul. I originally did not want to invest much time into addressing code duplication because I was under the impression that the ES6-related tests are going to be removed soon after the connector externalization. However I still see your point in doing so. At least when ES8 will be introduced, we'll be able to make use of the proper setup.\r\nI reworked the structure and extracted everything up until the level when a lot of concrete Elasticsearch classes are used. Further deduplication between the `Elasticsearch6Client` and `Elasticsearch7Client` is potentially possible but would require reintroducing a facade for a huge chunk of low-level ES abstractions and it does not seem to be worth it. ",
        "url": "https://github.com/apache/flink/pull/19405#issuecomment-1097361070"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 423344570,
        "body": "@jbaiera sorry for the delay on my end as well, I'm having trouble finding a good way to test this class as we only have public access to the `build` method that takes input of the large string created by the method I\"ve updated in this PR.  Short of restructuring things quite a bit for this small change I\"m not sure the best way to test it.  If you have an idea that I\"m missing let me know and I\"ll throw together a test class.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1161#issuecomment-423344570"
    },
    {
        "project": "hadoop",
        "id": 989365939,
        "body": "Opened #3774 to use `maven.test.failure.ignore` to simplify the configuration.",
        "url": "https://github.com/apache/hadoop/pull/3765#issuecomment-989365939"
    },
    {
        "project": "spring-framework",
        "id": 1183139717,
        "body": "This has been merged.\r\n\r\nThanks",
        "url": "https://github.com/spring-projects/spring-framework/pull/28800#issuecomment-1183139717"
    },
    {
        "project": "hadoop",
        "id": 538868639,
        "body": "Hi @bshashikant \r\n\r\nfirst of all, thank you for the review.\r\n\r\nLet me share my thought process when I selected to have a utility method that is dependent on the Configuration we have:\r\n- the logic of conversion is changing based on a configuration value\r\n- we don't use the utility method so far in any other context where the decision of which implementation is chosen is made based on anything else then configuration\r\n- this way I need to write the reading and handling of the related Configuration value once\r\n- this coupling of the implementation choosing logic and configuration I think is best to be encapsulated into the utility method that creates our conversion strategy, to express it clearly that this is dependent on a configuration value in the current environment.\r\n\r\n\r\nSo all in all I would refuse to have the method with a boolean parameter for now as I don't see its benefits at the moment, and also because if there will ever be an other arbitrary decision making process coexisting besides the using of the Configuration, then I think we can factor a method out from the current one, which will accept a boolean as a parameter, or based on other parameters we can implement the decision logic for other type of clients when there is a need.\r\nDoes this sound reasonable and acceptable?",
        "url": "https://github.com/apache/hadoop/pull/1596#issuecomment-538868639"
    },
    {
        "project": "spring-security",
        "id": 626142310,
        "body": "Thanks @rwinch for your response, I sent another commit with the requested changes:\r\n\r\n- modified rnc file, then generated the xsd-s, so my earlier manual modifications could be overwritten.\r\n- removed FIXME comment\r\n- extracted parser method which is called only if there is related tag in the xml configuration, and which has deprecation notice.",
        "url": "https://github.com/spring-projects/spring-security/pull/8450#issuecomment-626142310"
    },
    {
        "project": "druid",
        "id": 1201903841,
        "body": "Thanks for the fix! The eternity intervals have other issues. They are very cumbersome when comparing native queries, such as in tests. They require users to know how to define such intervals when all that is wanted is \"all data.\"\r\n\r\nI wonder, should we define an additional \"intervals\" subclass? The existing one for defined intervals, and a new one for the \"all data\" (i.e. \"eternity\") case? Then, a native query might look like:\r\n\r\n```json\r\n  \"intervals\" : {\r\n    \"type\" : \"eternity\"\r\n  },\r\n```\r\n\r\nInstead of:\r\n\r\n```json\r\n  \"intervals\" : {\r\n    \"type\" : \"intervals\",\r\n    \"intervals\" : [ \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\" ]\r\n  },\r\n```\r\n\r\nWith this, we express the intent, not the implementation.\r\n\r\nComparisons can then treat the \"eternity\" form special: eternity overlaps with all other intervals by definition. There can never be multiple intervals if there is eternity since, by definition, there is no additional period of time beyond eternity.\r\n\r\nThis might be more of a change than we want to take on in this PR, but it is a solution that would make life simpler in several ways.",
        "url": "https://github.com/apache/druid/pull/12844#issuecomment-1201903841"
    },
    {
        "project": "spring-framework",
        "id": 937926159,
        "body": "I've incorporated the commit with the tests, then simplified a little, removed the unrelated test, and applied a fix. That should take care of the main issue. ",
        "url": "https://github.com/spring-projects/spring-framework/pull/27262#issuecomment-937926159"
    },
    {
        "project": "druid",
        "id": 1085015717,
        "body": ">Additionally, reading through all of those methods reminded me of the methods that I wish didn't exist on ComplexMetricSerde. I'm sad that some of them are continuing their life into ComplexTypeSerde, but it's not worth it to introduce backwards incompatibility just for a rename.\r\n\r\nThinking more about this, I've decided to just close this PR in favor of doing something cooler in the future so we can just rework `ComplexMetricSerde` completely as something new.",
        "url": "https://github.com/apache/druid/pull/12382#issuecomment-1085015717"
    },
    {
        "project": "druid",
        "id": 209294544,
        "body": "@fjy doc reorganized. Is it ok?\n",
        "url": "https://github.com/apache/druid/pull/2817#issuecomment-209294544"
    },
    {
        "project": "spring-boot",
        "id": 428062844,
        "body": "Thanks for the proposal. I\u2019m not keen on opening up the `HttpTrace` API like this. We don\u2019t intend `HttpTrace` instances to be created from JSON only serialised to JSON. Making things like the timestamp mutable is also problematic.\r\n\r\nCan we please take a step back? Can you open an issue that describes what you\u2019d like to do? I think you want to be able to persist and recreate `HttpTrace` instances on the server-side, although you haven\u2019t said that in so many words. Once we fully understand the problem we can hopefully agree on some changes that solve that problem.",
        "url": "https://github.com/spring-projects/spring-boot/pull/14725#issuecomment-428062844"
    },
    {
        "project": "hadoop",
        "id": 1204343374,
        "body": "> Hmm, there are two PR now linked to the Jira and both to trunk?\r\n\r\nAh, didn't realize that. @snmvaughan you can get your PR merged if you would like, I just felt taking this up only because the Jira was not assigned to you by the time I raised PR, but anything is fine. All upto you :)",
        "url": "https://github.com/apache/hadoop/pull/4671#issuecomment-1204343374"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 407532209,
        "body": "> Yes, I have used Spark to extract data from Elasticsearch cluster, however the schema of one field is not correct.\r\n\r\n@viirya Is this due to the field not being stored the way you expect in Elasticsearch? In the test I saw that you provide a schema stating that the field should be an int, but by default Elasticsearch treats regular numbers as longs. Would you be able to make changes or conversions to the data in Spark to meet your expected data types? If not, what kinds of fields are you seeing this happen to?\r\n\r\n> Even I don't need the data of this field and only select fields I need, elasticsearch-hadoop still can't allow me to pull the data and throw exception when parsing the schema.\r\n\r\nDo you have a reproduction of those issues? I'm mostly confused since when you perform a select statement and discard a column in Spark SQL, it should exclude that column from the list of fields to read from Elasticsearch. ",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1169#issuecomment-407532209"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1029332965,
        "body": "I cloned the project at https://github.com/maxmin93/agens-spark to test this. I realized that the problem is more than just that it's a jar with in a jar -- it is also complicated by the fact that Spring Boot provides its own custom URLStreamHandler. That URLStreamHandler results in the URLConnection returning a different jar file URL than the default one. I added a unit test to simulate what the spring boot one does.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1899#issuecomment-1029332965"
    },
    {
        "project": "hadoop",
        "id": 934619131,
        "body": "> On the other hand,Pushing those changes after a major milestone would mean only one Jira and one PR (i.e., HADOOP-XYZ: migrating banned imports to use restrict-imports-enforcer-rule).\r\n\r\nIMHO the problem here is that if someone has already pushed a couple of commits with illegal imports by the time we are done with all modules, then we will have to take care of such individual modules violating the rule (by replacing the respective imports), it's kind of rework right?",
        "url": "https://github.com/apache/hadoop/pull/3503#issuecomment-934619131"
    },
    {
        "project": "skywalking",
        "id": 1173126954,
        "body": "I have some concerns about the new `forEach` function. See my comments inline. \n\n> 1. Add `forEach`, `processRelation` function to MAL Expression.\n\n`xxx.forEach(yyy)` usually iterates over `xxx` but in this PR it instead iterates `yyy`, this is confusing for most language programmers in my opinion. \n\n> \n> \n> Here is an example, I will add the configuration after the network analysis function is complete.\n> \n> ```yaml\n> \n> expSuffix: |-\n> \n>   processRelation('side', ['service'], ['instance'], 'client_process_id', 'server_process_id')\n> \n> expPrefix: |-\n> \n>   forEach(['client', 'server'], { prefix, tags ->\n> \n>     if (tags[prefix + '_process_id'] != null) {\n> \n>       return\n> \n>     }\n> \n>     if (tags[prefix + '_local'] == 'true') {\n> \n>       tags[prefix + '_process_id'] = ProcessRegistry.generateVirtualLocalProcess(tags.service, tags.instance)\n> \n>       return\n> \n>     }\n> \n>     tags[prefix + '_process_id'] = ProcessRegistry.generateVirtualRemoteProcess(tags.service, tags.instance, tags[prefix + '_address'])\n> \n>   })\n> \n> metricPrefix: network_profiling\n> \n> metricsRules:\n> \n>   - name: write_bytes\n> \n>     exp:  rover_network_monitor_write_bytes_counter.sum(['service', 'instance', 'side', 'client_process_id', 'server_process_id'])\n> \n> ```\n\nI think we expose too many details/logics in the MAL. I'm wondering why we can't use the similar style as `retagByK8sMeta`. \n",
        "url": "https://github.com/apache/skywalking/pull/9299#issuecomment-1173126954"
    },
    {
        "project": "hadoop",
        "id": 964407226,
        "body": "That's great progress, @ArkenKiran ! You got there before I could. On the read path, any block cache miss will go to the hdfs client for a read (can still be local via short-circuit read). On the write path, we should see hdfs interaction via WAL writes and by MemStore flushes. WAL writes are inline with the client activity, MemStore flushes happen in the background.",
        "url": "https://github.com/apache/hadoop/pull/3445#issuecomment-964407226"
    },
    {
        "project": "druid",
        "id": 972395652,
        "body": ">Faced with this fact I would say let's stick with getColumnType and getFinalizedColumnType, rather than planning to change them back to getType and getFinalizedType. It simplifies our lives (less work, less churn) and either name seems equally ok.\r\n\r\nI've changed these method names to `getIntermediateType` and `getResultType` to avoid room for confusion with input column types.",
        "url": "https://github.com/apache/druid/pull/11917#issuecomment-972395652"
    },
    {
        "project": "spring-boot",
        "id": 1184500705,
        "body": "OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file.\n\nIf you change your mind, just re-open this PR and I'll resolve any conflicts on it.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31712#issuecomment-1184500705"
    },
    {
        "project": "flink",
        "id": 1201908318,
        "body": "@godfreyhe thanks for reviewing this!  I'm rewriting the new scala class to java version(the progress is not as fast as expected), and will split the physical operator implementation into [FLINK-28568](https://issues.apache.org/jira/browse/FLINK-28568)",
        "url": "https://github.com/apache/flink/pull/20393#issuecomment-1201908318"
    },
    {
        "project": "flink",
        "id": 1031497798,
        "body": "@twalthr I tried to reorganize a bit the code, in particular in relation to the fact that I was wrongly setting the uids in batch nodes. It should work, although it may look better. Let me know if this is good enough or if i should spend more efforts on it",
        "url": "https://github.com/apache/flink/pull/18621#issuecomment-1031497798"
    },
    {
        "project": "skywalking",
        "id": 906837099,
        "body": "@daimingzhi We are using this week to split java agent codebase into https://github.com/apache/skywalking-java. Please move your pull request there.",
        "url": "https://github.com/apache/skywalking/pull/7477#issuecomment-906837099"
    },
    {
        "project": "spring-framework",
        "id": 1117674660,
        "body": "@rstoyanchev just reworked a bit the commit, can you please have a look?",
        "url": "https://github.com/spring-projects/spring-framework/pull/28084#issuecomment-1117674660"
    },
    {
        "project": "flink",
        "id": 1159753671,
        "body": "@deadwind4 Thanks for the update! LGTM overall. I'm wondering if we should rename SimpleElasticsearchEmitter to MapElasticsearchEmitter to indicate that it actually accepts Map as input. What's your thought?",
        "url": "https://github.com/apache/flink/pull/20002#issuecomment-1159753671"
    },
    {
        "project": "spring-security",
        "id": 1194502957,
        "body": "Thanks, @mmoussa-mapfre. Will you please add a unit test that confirms the success handler will now use the custom logout request repository?",
        "url": "https://github.com/spring-projects/spring-security/pull/11618#issuecomment-1194502957"
    },
    {
        "project": "pulsar",
        "id": 1126255563,
        "body": "@nicoloboschi @mattisonchao \r\nI have to rework this patch, actually there is no way to pass values containing the \"comma\" (,) or the equals (=) characters, so you are very limited.\r\n\r\nI will update it early next week",
        "url": "https://github.com/apache/pulsar/pull/15503#issuecomment-1126255563"
    },
    {
        "project": "flink",
        "id": 1047648112,
        "body": "> What I am trying to do here is to make our modules and services more configurable. My first thought was to do an abstraction layer for each APIService and allows others to reuse those in their apps. However, after reconsidering it, it makes more sense to directly extend the services and override api methods for customization. So I guess the abstraction is meaningless in this sense.\r\n\r\nThanks for the explanation. For what it's worth, with TypeScript's structural typing approach you don't _need_ dedicated base classes anyway since every class defines both a class and a type (of course depending on the situation, a dedicated type would still be cleaner).",
        "url": "https://github.com/apache/flink/pull/18868#issuecomment-1047648112"
    },
    {
        "project": "flink",
        "id": 1047629822,
        "body": "Oh, thanks for the feedbacks @Airblader. I did some changes to the origin PR, mainly removing the abstractions  that I was actually overthinking. \r\n\r\nWhat I am trying to do here is to make our modules and services more configurable. My first thought was to do an abstraction layer for each APIService and allows others to reuse those in their apps. However, after reconsidering it, it makes more sense to directly extend the services and override api methods for customization. So I guess the abstraction is meaningless in this sense.",
        "url": "https://github.com/apache/flink/pull/18868#issuecomment-1047629822"
    },
    {
        "project": "druid",
        "id": 462086552,
        "body": "@gianm \r\n\r\nLooking for guidance here...\r\n\r\nI have spent the past few days studying the Druid-HLL code and have uncovered at least a half-dozen serious bugs and haven't even started on the merge logic, which from a brief look also has very serious problems.  A number of these problems are interconnected, so you can't just fix one at a time.   \r\n\r\nThis code needs to be redesigned from scratch.  I'm not sure I want to undertake this, but if I were, I would insist on some major changes. The API will have some required changes:  The biggest one is removing the ability for users to specify the hash function.  Any users that are currently doing that, using a different hash function and have historical stored images may not be able to use their history.  \r\n\r\nI am considering 2 strategies:\r\n1. **Rewrite existing code**: Many current internal methods that are now public will become private or package-private: e.g., no public access to internals such as the overflow registers, getNumNonZeroRegisters, getHeaderBytes, getPayloadBytePosition, setVersion, etc.  I may also insist on a merge class rather than a merge method ( fold() ).  \r\n\r\nThe storage would be a little larger (from 1031 bytes to perhaps 1080 bytes).  And merge performance may be a bit slower.  It could still be backward compatible, but old images will still propagate errors into the new design and there is nothing that can be done about that.  Users that record their history with the new design will see much better error performance.  \r\n\r\n2. **API Wrapper to the DS-HLL sketch**.  This will require some changes to the DS-HLL code, but may be easier to do and would eliminate a lot of code duplication.  Basically I have to allow for the non-seeded hash function, adapt to the big-endian fields used by the Druid-HLL, create a merge function that can read the old Druid-HLL images, etc.  \r\n\r\nThe advantage of this is (hopefully) a single code-base for the sketch internals with two different wrappers, one for the old Druid-HLL users, and a new full-function API wrapper for the DS-HLL customers.\r\n\r\nWhatever, the new design would have to be extensively characterized and tested.\r\n\r\nThoughts?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
        "url": "https://github.com/apache/druid/pull/6865#issuecomment-462086552"
    },
    {
        "project": "spring-framework",
        "id": 686895618,
        "body": "> @quaff that is a hard-core optimization. In the classical double-check locking there are 3 volatile reads, with a local variable - there are 2. Especially since with a local variable there is a _single racy read_, meaning a single read of a volatile, that is not protected by the synchronized block (as opposed by two in the classic example).\r\n> \r\n> For `x86` this does not matter - as no re-orderings are done for volatile reads; on other platforms (like ARM), this matters.\r\n\r\nIt's a performance improvement, not safety enhance?",
        "url": "https://github.com/spring-projects/spring-framework/pull/25688#issuecomment-686895618"
    },
    {
        "project": "spring-security",
        "id": 1166007190,
        "body": "@cbot59, thanks for the PR! It looks like some things might have gotten mixed up since there are several commits, but just one small change. Will you please try and rebase with `main` and force push your PR to see if it fixes the issue?",
        "url": "https://github.com/spring-projects/spring-security/pull/11394#issuecomment-1166007190"
    },
    {
        "project": "druid",
        "id": 1201903841,
        "body": "Thanks for the fix! The eternity intervals have other issues. They are very cumbersome when comparing native queries, such as in tests. They require users to know how to define such intervals when all that is wanted is \"all data.\"\r\n\r\nI wonder, should we define an additional \"intervals\" subclass? The existing one for defined intervals, and a new one for the \"all data\" (i.e. \"eternity\") case? Then, a native query might look like:\r\n\r\n```json\r\n  \"intervals\" : {\r\n    \"type\" : \"eternity\"\r\n  },\r\n```\r\n\r\nInstead of:\r\n\r\n```json\r\n  \"intervals\" : {\r\n    \"type\" : \"intervals\",\r\n    \"intervals\" : [ \"-146136543-09-08T08:23:32.096Z/146140482-04-24T15:36:27.903Z\" ]\r\n  },\r\n```\r\n\r\nWith this, we express the intent, not the implementation.\r\n\r\nComparisons can then treat the \"eternity\" form special: eternity overlaps with all other intervals by definition. There can never be multiple intervals if there is eternity since, by definition, there is no additional period of time beyond eternity.\r\n\r\nThis might be more of a change than we want to take on in this PR, but it is a solution that would make life simpler in several ways.",
        "url": "https://github.com/apache/druid/pull/12844#issuecomment-1201903841"
    },
    {
        "project": "druid",
        "id": 1171941124,
        "body": "Created issue to improve these docs: #12726 ",
        "url": "https://github.com/apache/druid/pull/12717#issuecomment-1171941124"
    },
    {
        "project": "apm-agent-java",
        "id": 595098537,
        "body": " > About distributed tracing don't work, i think i forget to change the way to start a transaction in the MessageListener Wrapper , i will fix it and add a test for it.\r\n\r\nExcellent, thanks! Try to use the Kafka module tests as reference as to what we want to test. It will help you cover all sorts of scenarios and make sure your distributed tracing works as expected.\r\n\r\nRegarding the Lambdas - take a look at [the JMS `MessageListener` handling](https://github.com/elastic/apm-agent-java/blob/96b7d60e4edcb39242ab5d7922111d52fd9c1023/apm-agent-plugins/apm-jms-plugin/src/main/java/co/elastic/apm/agent/jms/JmsInstrumentationHelperImpl.java#L134-L143) - it identifies Lambdas and wraps only them when they are registered, thus enabling straightforward instrumentation of the `onMessage` interface method. See if you can do the same. \r\nLater, we may consider reworking things and enable Lambdas instrumentation, in which case we will be able to just cancel this wrapping.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1057#issuecomment-595098537"
    },
    {
        "project": "apm-agent-java",
        "id": 480591829,
        "body": "**Summary of research and implementation rationale:**\r\n\r\n**Current solution:**\r\nInstrumenting the main `HttpAsyncClient#execute` API, which is invoked by the implementation of all other APIs. This is where a span is created and started. In addition, the `HttpAsyncRequestProducer` and `FutureCallback` arguments are wrapped. The way to connect the response to the callback is through the `HttpContext` which is where the client sets the response object as an attribute.\r\n\r\nLimitations/risks:\r\n- The legacy `HttpAsyncRequester` API is not supported. This API was very complicated to use and was replaced by the HttpAsyncClient to greatly simplify usage already at October 2013, so we assume it is not commonly used.\r\n- Wrapped `HttpAsyncRequestProducer` means we need to watch out to future changes of the API, but it seems very consistent so far\r\n- We rely on `HttpAsyncRequestProducer#close` to end the span and recycle the wrapper. If not invoked, we may get span/wrapper leak or even wrong delegations if reused after closed. Tests didn't find any problem\r\n- Getting the response code from the `HttpContext` is not part of the API, so no guarantee about it, especially when invoked in the callback where it may be stripped from attributes already. Tests didn't find any problem.\r\n- The pipelining API is not supported at this stage. We can add support for it if demand rises.\r\n\r\n**Other solutions considered:**\r\n1. Instrumenting implementations of `HttpAsyncClientExchangeHandler` so that we don't need to wrap and we can support the legacy async API.\r\n  Cons:\r\n    - we must rely on the context in order to relate a response to a request, but this is not part of the API and requires specific instrumentation of all implementations to locate the context.\r\n    - no indication whether a response is the final one or can be redirected.\r\n2. Registering `HttpRequestInterceptor` and `HttpResponseInterceptor`. \r\n  Cons:\r\n    - registering during build time mean we only support startup and not agent attach. Otherwise, we need to add implementation-specific instrumentation to add interceptors to the list- probably not very resistant to client changes.\r\n    - we don't know when the final response is called in case of redirects, so we don't know when to finish the span.\r\n\r\n",
        "url": "https://github.com/elastic/apm-agent-java/pull/558#issuecomment-480591829"
    },
    {
        "project": "flink",
        "id": 1039297278,
        "body": "Thanks for the review @zentol! I've tried to go bit more fancy with the PR, let me know what you think (it's basically a complete rework \ud83d\ude02 )",
        "url": "https://github.com/apache/flink/pull/18761#issuecomment-1039297278"
    },
    {
        "project": "hadoop",
        "id": 1201714219,
        "body": "> > Personally, I think this optimization should still be valuable, I hope that the submitted pr is not at the class level, at least at the moudle level.\r\n> \r\n> I wouldn't go for one per class but maybe subpackage. I also think this PR is doing a couple of things more other than adding lambdas. For example, adjusting for iterations.\r\n\r\nI understand your suggestion that the lambda should be tuned with other code modifications, not just the lambda code.\r\n",
        "url": "https://github.com/apache/hadoop/pull/4668#issuecomment-1201714219"
    },
    {
        "project": "spring-framework",
        "id": 528336680,
        "body": "Indeed it wouldn't make sense to add anything to do with \"remote\" host or port. `UriComponentsBuilder` is just another way of building URIs as you said, and in this case we would be building a URI that represents the remote address. Just like we currently create `ServerHttpRequest` from the URL and then call `adaptFromForwardedHeaders`, I was thinking we could also separately create a `ServerHttpRequest` from the remote address URL and then call `adaptFromForwardedForHeaders` to update it. That way the logic for extracting such header values is encapsulated which also provide opportunity for re-use in the implementation.",
        "url": "https://github.com/spring-projects/spring-framework/pull/23582#issuecomment-528336680"
    },
    {
        "project": "spring-boot",
        "id": 1161627638,
        "body": "Thanks @StefanBratanov for the changes, the PR looks quite nice now!\r\n\r\n> Hi Moritz, sorry for the delay. I did make a refactor based on the comments, which i agree with. I wasn't sure about handling the `WebClient` async call, but now i think it makes sense and is cleaner. I also changed the `ZipkinRestTemplateSender` async implementation to use a `CompletableFuture` since it was blocking before. I also added tests for both sync and async in both implementations.\r\n\r\nNo need to be sorry :)\r\n\r\nI don't think we need that `CompletableFuture` handling. The `UrlConnectionSender` from zipkin implements the enqueue call without additional threading, so we should be fine here without introducing multi-threading.\r\n\r\n> I am not sure about initializing the `WebClient` in `ZipkinConfigurations`. I was using the `ReactorClientHttpConnector` which is based on` Netty`, so that i can honor the read and connect timeouts set in `ZipkinProperties`. What do you think is a better approach for initializing the `WebClient`? \r\n\r\nI have to double check if there's really no way to set timeouts without tying us directly on the implementation.\r\n\r\n> Also i couldn't find a mock server similar to `MockRestServiceServer` for `WebClient`, so kept my old implementation. If there is one, I am happy to change it.\r\n\r\nYes, I was wrong, there's no such support in the Spring Framework. The recommendation is to use the [OkHttp MockServer](https://docs.spring.io/spring-framework/docs/6.0.0-SNAPSHOT/reference/html/web-reactive.html#webflux-client-testing), like you did.\r\n",
        "url": "https://github.com/spring-projects/spring-boot/pull/30792#issuecomment-1161627638"
    },
    {
        "project": "hadoop",
        "id": 1053924133,
        "body": "First of all there are test failure which must be fixed. \r\nAlso upgrading this has a history of breaking downstream builds, what testing is done for that ?\r\n\r\nSeems like 1.70 is a major upgrade with new features and enhancements. https://www.bouncycastle.org/latest_releases.html can we use 1.68/1.69 to just fix the vulnerabilities ?",
        "url": "https://github.com/apache/hadoop/pull/3980#issuecomment-1053924133"
    },
    {
        "project": "apm-agent-java",
        "id": 1202280115,
        "body": "> Since the log correlation instrumentation is independent of the ECS-reformatting one, we need to distinguish between two use cases:\r\n> \r\n> 1. ECS-logging-JUL is used as an application dependency, in which case we need to update the `JulMdc` that is loaded by the agent CL.\r\n> 2. ECS-reformatting - where we need to add to the `JulMdc` that is loaded by the agent plugin CL.\r\n> \r\n> I think the proper way to achieve that is by having two instrumentations (within two plugins):\r\n> \r\n> 1. one that is only applied if the application CL can load `JulMdc`. Its advice adds to the `JulMdc` that is loaded by the application class loader. In order to achieve that, you may need to create a separate plugin (so that the plugin class loader does not contain the ECS logging classes bundled with the agent) AND you may need to update [the list](https://github.com/elastic/apm-agent-java/blob/cb870b072523f83c56669ba8f4cabc3b48f79c44/apm-agent-core/src/main/java/co/elastic/apm/agent/bci/classloading/IndyPluginClassLoader.java#L71) that says what's loaded from the agent class loader and what's not.\r\n> 2. the second is only applied if the application CL cannot load `JulMdc`, in which case the advice would add to the agent-bundled `JulMdc`. This one needs to have the `JulMdc` loaded by the plugin CL, which is the case with the current logging plugin, so your `JulEcsLogCorrelationInstrumentation` should be fit for this purpose.\r\n> \r\n> Sorry if I confused or mislead you, it is all based on static code analysis and better be verified with tests of both scenarios.\r\n\r\nThe way I understand it, there are three cases:\r\n- `jul-ecs-formatter` is used by the application, without agent, the `JulMdc` is not read and is always empty\r\n- `jul-ecs-formatter` is used by the application with the agent with log reformatting, in this case we should update the `JulMdc` that is used by the application and ignore the one internal to the agent.\r\n- `jul-ecs-formatter` is not used by the application with the agent + log reformatting, in this case we just rely on agent JulMdc and agent EcsFormatter.\r\n\r\nSo, in a sense, we just need to alter which instance of the `JulMdc` we update depending on the scenario.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2724#issuecomment-1202280115"
    },
    {
        "project": "druid",
        "id": 1182535667,
        "body": ">Is it intentionally undocumented in this PR? Do you plan to add documentation?\r\n\r\nI was planning to add documentation in a follow-up PR since I thought this one was already big enough \ud83d\ude05 \r\n\r\n>Are there any impediments to maintaining forwards compatibility of the storage format, such that new versions of Druid will always be able to read JSON columns written by older versions? Do you foresee any reason we might want to break compatibility?\r\n\r\nI modeled the column after existing Druid columns so most things are decorated with a version byte which should allow us to make changes in the future while still being able to continue reading the existing data. For the specific list of what is versioned: \r\n* `NestedDataColumnSerializer` for the complex column itself (currently on v3 actually, i removed the reader code for older versions from prototyping to get rid of dead code)\r\n* `GlobalDictionaryEncodedFieldColumnWriter` which writes the nested columns and is currently re-using `DictionaryEncodedColumnPartSerde.VERSION` (i should probably decouple this at some point in the future...)\r\n* `FixedIndexed` (building block used to store local to global dictionary mapping and long and double value dictionaries)\r\n* `CompressedVariableSizedBlobColumnSerializer` (used to compress raw data)\r\n* `CompressedBlockSerializer` (used internally by `CompressedVariableSizedBlobColumnSerializer`)\r\n\r\nIn the \"Future work\" section of #12695 I mention storage format as an area that we can iterate on in the future, the biggest things I have in mind right now are storing arrays of literal values as array typed columns instead of broken out as they currently are, as well as customization such as allowing skipping building indexes on certain columns or storing them all-together also probably falls into this. Nothing about the current code should block this afaik, nor should those future enhancements interfere with our ability to read data that is stored with the current versions of stuff, so long as we practice good version hygiene whenever we make changes.\r\n\r\n> Would you recommend we present this feature in its current state as experimental or production-ready, & why?\r\nThis is a hard one to answer, though I am hesitant to call it production ready right from the start, I think the answer might vary a bit per use case.\r\n\r\nThe surface area here is huge since it essentially provides all of the normal Druid column functionality within these `COMPLEX<json>` columns, and I definitely won't claim this to be bug free. That said, quite a lot of internal testing has been done at this point, even at scale and with complicated nested schemas, which has allowed this codebase to be iterated on to get it to the place it currently is. There are some rough spots which I'm looking to improve in the near future, such as ingest time memory footprint, better array handling, etc, but I think if we get the documentation in a good enough state and can list out the limitations it could be used today.\r\n\r\nThe use cases I would feel most comfortable with are replacements for what can currently be done via flattening, meaning not heavily centered on nested arrays. I do have ideas of how to better support nested arrays and my goal is to allow arrays extracted from nested columns to be exposed as druid `ARRAY` types, but I am not there yet, so I'm not sure I would recommend most array use cases unless they are more like vectors which have expected lengths and array positions are known/meaningful (and such that most queries would be extracting specific array positions, not entire arrays).\r\n\r\nThere is also the matter of different performance characteristics at both ingest and query time for these columns. Ingestion time segment merge is pretty heavy right now because the global value dictionary is stored in heap. Query performance can vary a fair bit with nested columns compared to flat columns, especially with numbers due to the existence of indexes on these numeric columns, which currently at least sometimes results in dramatically faster but also sometimes slower query performance. I'm still exploring this quite a bit, besides documentation follow-up I also have been working on doing some benchmarking to see where things currently stand and plan on sharing those results relatively soon.\r\n\r\nSo, long story short, due to the unknowns I think the answer for right now is that operators should experiment with `COMPLEX<json>` columns to see if they work well for their use case, and use them in production if so, otherwise provide feedback so that we can continue to make improvements and expand the use cases this is good for?",
        "url": "https://github.com/apache/druid/pull/12753#issuecomment-1182535667"
    },
    {
        "project": "spring-security",
        "id": 758045871,
        "body": "I did consider a filter to wrap request, however `AuthenticationTrustResolver` is used and doesn't appear to be a bean to inject. I'd have to create a new instance (may not be an issue). As far as the principal object, I am using out of the box OIDC and haven't tracked down a good place to create new principal to extend/override the toString(). I can take another look at that.\r\n\r\nThanks for the quick response!",
        "url": "https://github.com/spring-projects/spring-security/pull/9211#issuecomment-758045871"
    },
    {
        "project": "flink",
        "id": 1202302082,
        "body": "Hi @Myasuka, this test has always been unstable due to the way we obtained checkpoints, I refactored it. Could you please take a look?",
        "url": "https://github.com/apache/flink/pull/20420#issuecomment-1202302082"
    },
    {
        "project": "spring-framework",
        "id": 724095180,
        "body": "The current proposal introduces support for the following abstraction.\r\n\r\n```java\r\n/**\r\n * {@code ApplicationEvents} encapsulates all {@linkplain ApplicationEvent\r\n * application events} that were fired during the execution of a single test\r\n * method.\r\n *\r\n * @author Sam Brannen\r\n * @author Oliver Drotbohm\r\n * @since 5.3.1\r\n * @see ApplicationEventsExtension\r\n * @see org.springframework.context.ApplicationEvent\r\n * @see org.springframework.context.ApplicationListener\r\n */\r\npublic interface ApplicationEvents {\r\n\r\n\t/**\r\n\t * Stream all application events that were fired during test execution.\r\n\t * @return a stream of all application events\r\n\t */\r\n\tStream<ApplicationEvent> stream();\r\n\r\n\t/**\r\n\t * Stream all application events or event payloads of the given type that\r\n\t * were fired during test execution.\r\n\t * @param <T> the event type\r\n\t * @param type the type of events or payloads to stream; never {@code null}\r\n\t * @return a stream of all application events or event payloads of the\r\n\t * specified type\r\n\t */\r\n\t<T> Stream<T> stream(Class<T> type);\r\n\r\n}\r\n```\r\n\r\nCurrent work on this issue can be viewed in the following branch that builds on top of this PR: https://github.com/spring-projects/spring-framework/compare/master...sbrannen:issues/gh-25616-application-events-extension\r\n\r\nTentatively slated for inclusion in `5.3.2`.",
        "url": "https://github.com/spring-projects/spring-framework/pull/25616#issuecomment-724095180"
    },
    {
        "project": "spring-boot",
        "id": 903985263,
        "body": "> @bono007 Are you interested in reworking this to support multiple paths? In light of [#27306 (comment)](https://github.com/spring-projects/spring-boot/issues/27306#issuecomment-903907394), that can be done without considering the disk space health indicator. I think we'd probably end up with a `MetricBinder` implementation that consumes the paths. When called, it would create one `DiskSpaceMetrics` instance per path and binds it to the registry.\r\n\r\nYou know I am \ud83d\ude3a \r\n\r\nI like the MeterBinder -> N DiskSpaceMetrics approach as well. This also allows us to close https://github.com/micrometer-metrics/micrometer/pull/2747#issuecomment-903915522 . \r\n\r\nI will get to it in the next 1-2 days. That timeline work?",
        "url": "https://github.com/spring-projects/spring-boot/pull/27660#issuecomment-903985263"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 242165223,
        "body": "Thanks for opening a PR for this! I left a comment inline with the commit.\n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/831#issuecomment-242165223"
    },
    {
        "project": "skywalking",
        "id": 1203426902,
        "body": "@mrproliu I added a `ebpf_profiling_schedule_id` column  in `EBPFProfilingScheduleRecord`. Because when merging all metrics in one index, the `id` will rewrite to `modelName_id` to avoid conflict.\r\n\r\nplease check.",
        "url": "https://github.com/apache/skywalking/pull/9416#issuecomment-1203426902"
    },
    {
        "project": "spring-framework",
        "id": 698274273,
        "body": "Yes too many flags indeed. Maybe we can add a protected method that extracts the desired request properties to a `Map`, which is then easy to format to a String, and a sub-class can override this method to customize what goes in the Map:\r\n\r\n```java\r\nprotected Map<String, Object> extractRequestProperties(HttpServletRequest request) {\r\n\tMap<String, Object> result = new LinkedHashMap<>();\r\n\tif (isIncludeClientInfo()) {\r\n\t\tresult.put(\"client\", request.getRemoteAddr());\r\n\t\tHttpSession session = request.getSession(false);\r\n\t\tif (session != null) {\r\n\t\t\tresult.put(\"session\", session.getId());\r\n\t\t}\r\n\t\tresult.put(\"user\", request.getRemoteUser());\r\n\t}\r\n\tif (isIncludeHeaders()) {\r\n\t\tHttpHeaders headers = new ServletServerHttpRequest(request).getHeaders();\r\n\t\tif (getHeaderPredicate() != null) {\r\n\t\t\tEnumeration<String> names = request.getHeaderNames();\r\n\t\t\twhile (names.hasMoreElements()) {\r\n\t\t\t\tString header = names.nextElement();\r\n\t\t\t\tif (!getHeaderPredicate().test(header)) {\r\n\t\t\t\t\theaders.set(header, \"masked\");\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\tresult.put(\"headers\", headers);\r\n\t}\r\n\r\n\tif (isIncludePayload()) {\r\n\t\tString payload = getMessagePayload(request);\r\n\t\tif (payload != null) {\r\n\t\t\tresult.put(\"payload\", payload);\r\n\t\t}\r\n\t}\r\n\treturn result;\r\n}\r\n```\r\n\r\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/25757#issuecomment-698274273"
    },
    {
        "project": "pulsar",
        "id": 913594641,
        "body": "> Could we reuse the existing perf components? Such as `PerformanceClient`, `PerformanceProducer` and `PerformanceConsumer`.\r\n\r\nTheir logic is different, and they did not extract the generic component. I don't think it's suitable",
        "url": "https://github.com/apache/pulsar/pull/11933#issuecomment-913594641"
    },
    {
        "project": "spring-boot",
        "id": 1195353873,
        "body": "I'm not sure why I added `merge-with-amendements`, perhaps it was because I thought some changes would be required to apply the fixes to 2.6.x. Turns out it applied pretty cleanly.\r\n\r\nThanks for the PR @marcwrobel! Much appreciated.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31734#issuecomment-1195353873"
    },
    {
        "project": "flink",
        "id": 1179279118,
        "body": "hi @afedulov , Parameter name has been modified, thank you for your suggestion",
        "url": "https://github.com/apache/flink/pull/20127#issuecomment-1179279118"
    },
    {
        "project": "pulsar",
        "id": 782183735,
        "body": "> What I am really concerned is, context.outputMessage() might return user a dead producer, and user cannot recreate it because it is cached.\r\n\r\n_EDIT: got my issues confused on earlier response. Clarification:_\r\n\r\nMy proposal on #9515 is to deprecate `context.NewOutputMesage()`, disallowing direct access to producers entirely, but since these are separate issues, `context.NewOutputMessage()` could be replaced with `context.NewProducer()`, which does not use cache at all and then the caching issues could be addressed in #9515, thoughts?",
        "url": "https://github.com/apache/pulsar/pull/9513#issuecomment-782183735"
    },
    {
        "project": "druid",
        "id": 985050495,
        "body": "Ah, looks like a later PR (https://github.com/apache/druid/pull/11912) entirely reworked the Kafka ingestion docs.",
        "url": "https://github.com/apache/druid/pull/11865#issuecomment-985050495"
    },
    {
        "project": "spring-security",
        "id": 1144083940,
        "body": "Dependencies removed.",
        "url": "https://github.com/spring-projects/spring-security/pull/11319#issuecomment-1144083940"
    },
    {
        "project": "druid",
        "id": 982054041,
        "body": "@yuanlihan, cool feature! Here is one question you might want to explain: what is that use case that will be improved by this change?\r\n\r\nThe feature would seem to optimize queries that hit exactly the same data every time. How common is this? It might solve a use case that you have, say, 100 dashboards, each of which will issue the same query. One of them runs the real query, 99 fetch from the cache.\r\n\r\nHere's a similar use case I've seen many times, but would *not* benefit from a post-merge cache: a dashboard wants to show the \"last 10 minutes\" of data, and we have minute-grain cubes (segments). Every query shifts the time range by one minute which will add a bit more new data, and exclude a bit of old data. With this design, we cache the post-merge data, so each shift in the time window will result in a new result set to cache.\r\n\r\nCan this feature instead cache pre-merge results? That way, we cache each minute slice once. The merge is still needed to gather up the slices required by the query. But, that merge is in-memory and should be pretty fast. The result would be, in this example, caching 1/10 the amount of data compared to caching post-merge data. And, less load on the historicals since we don't hit them each time the query time range shifts forward one minute.\r\n\r\nIn short, please explain your use case a bit more so we an understand the goal of this enhancement.\r\n\r\nEdit: I'm told we may have some of the above. Historicals can already cache per-segment results. In a system with large numbers of segments, and where the query hits a good percentage of those, I'm told the merge cost can be high. Is it the merge cost that this PR seeks to avoid?",
        "url": "https://github.com/apache/druid/pull/11989#issuecomment-982054041"
    },
    {
        "project": "druid",
        "id": 1202687938,
        "body": "> One thing I'm not sure is if `pull-deps` works under JRE 17.\r\n\r\nI did notice that it is run as part of the openjdk17 CI, because there was a problem with it in an earlier version of this patch. So it works at least well enough for that usage (if the strong-encapsulation-related command line parameters are included). I didn't try further usages.",
        "url": "https://github.com/apache/druid/pull/12839#issuecomment-1202687938"
    },
    {
        "project": "spring-boot",
        "id": 1100863867,
        "body": "Removed and added commits",
        "url": "https://github.com/spring-projects/spring-boot/pull/30708#issuecomment-1100863867"
    },
    {
        "project": "apm-agent-java",
        "id": 555023371,
        "body": "I totally agree with everything \ud83d\ude42 - it should have been few small PRs. Because they are dependent on each other, I didn't want to block on waiting for review/approval, but probably the wrong call.\r\nI will add more documentation in code. I hoped the PR main comment would provide the context. I assume everything in this comment will go into changelog (which I always do last, to reduce merges \ud83d\ude0a )",
        "url": "https://github.com/elastic/apm-agent-java/pull/911#issuecomment-555023371"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 985668199,
        "body": "As an example, here are the X-Opaque-ID headers that you would see from a simple mapreduce job named Sample Job that uses EsInputFormat to read all of the books index, and then EsOutputFormat in a reducer to write it all back out to the books index. Each line below shows the X-Opaque-ID header and the URL being hit:\r\n```\r\n[[mapreduce] [] [Sample Job] []] /\r\n[[mapreduce] [hduser] [Sample Job] []] /\r\n[[mapreduce] [hduser] [Sample Job] []] /books\r\n[[mapreduce] [hduser] [Sample Job] []] /_cluster/health/books\r\n[[mapreduce] [hduser] [Sample Job] []] /_nodes/http\r\n[[mapreduce] [hduser] [Sample Job] []] /_nodes/http\r\n[[mapreduce] [hduser] [Sample Job] []] /books\r\n[[mapreduce] [hduser] [Sample Job] []] /books/_search_shards\r\n[[mapreduce] [hduser] [Sample Job] []] /books/_mapping\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_m_000000_0]] /books/_search\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_m_000000_0]] /_search/scroll\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_m_000000_0]] /_search/scroll\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /_nodes/http\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /_nodes/http\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /_all/_alias/books\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /books\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /books/_search_shards\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /_nodes/http\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /books/_bulk\r\n[[mapreduce] [hduser] [Sample Job] [attempt_local61453229_0001_r_000000_0]] /books/_refresh\r\n```\r\nHere is the spark equivalent, run from spark shell:\r\n```\r\nsc.esJsonRDD(\"books\").saveAsSequenceFile(\"/keith/books-seq-9\")\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /books\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /_cluster/health/books\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /_nodes/http\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /_nodes/http\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /books\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /books/_search_shards\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /books/_mapping\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 0] [task attempt 0]] /books/_search\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 0] [task attempt 0]] /_search/scroll\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 0] [task attempt 0]] /_search/scroll\r\n\r\nsc.sequenceFile(\"/keith/books-seq\", classOf[Text], classOf[Text]).saveToEs(\"books-restore12\")\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036]] /\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /_nodes/http\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /_nodes/http\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /_all/_alias/books-restore12\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /books-restore12\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /books-restore12/_search_shards\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /_nodes/http\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /books-restore12/_bulk\r\n[[spark] [hduser] [Spark shell] [application_1631651348162_0036] [stage 1] [task attempt 1]] /books-restore12/_refresh\r\n```",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1770#issuecomment-985668199"
    },
    {
        "project": "druid",
        "id": 1131205948,
        "body": "My current guess about `GroupByQueryRunnerTest.testGroupByWithExpressionAggregatorWithComplex` is that the problem is ExpressionLambdaAggregatorFactory's getComparator, combine, and finalizeComputation methods are not thread-safe. I replaced the Suppliers with ThreadLocals in the latest commit.",
        "url": "https://github.com/apache/druid/pull/12521#issuecomment-1131205948"
    },
    {
        "project": "spring-framework",
        "id": 1183308182,
        "body": "This has been merged into `5.3.x` and `main`.\r\n\r\nThanks for all of your effort recently in cleaning up typos! \ud83d\udc4d ",
        "url": "https://github.com/spring-projects/spring-framework/pull/28807#issuecomment-1183308182"
    },
    {
        "project": "spring-framework",
        "id": 77410087,
        "body": "I've revisited this PR with suggested changes.\n\n## Documentation Layout\n\nThe documentation is now organized in bookparts, so there are now less files to track. LevelOffsets for bookparts don't need to be incremented, as they're already starting at level 0. Still, I separated some parts that were long/important enough to be in their own file; those are imported with an incremented leveloffset.\n\nHere is the new documentation layout:\n\n```\nindex\n  |-- overview[]\n  |\n  |-- whats-new[]\n  |\n  |-- core[]\n  |   |-- core-beans[leveloffset=+1]\n  |   |-- core-aop[leveloffset=+1]\n  |-- testing[]\n  |\n  |-- data-access[]\n  |\n  |-- web[]\n  |   |-- web-mvc[leveloffset=+1]\n  |   |-- web-view[leveloffset=+1]\n  |-- integration[]\n  |\n  |-- appendix[]\n      |-- appx-spring-tld[leveloffset=+1]\n      |-- appx-spring-form-tld[leveloffset=+1]\n```\n\n## Remaining issues\n\nWhen viewing individually a bookpart (e.g. looking at `core` in its own HTML rendered page):\n1. `[partintro]` are not rendered in individual pages, since the bookpart is not a child of a book document when being rendered indivually\n\n```\nasciidoctor: ERROR: partintro block can only be used when doctype is book\nand it's a child of a book part. Excluding block content.\n```\n1. links to other bookparts don't work; they probably need to be updated:\n\n```\n<<section, My section>> must be changed into <<filename#section, My section>> \n```\n\nOf course, both issues are solved in single document mode.\n\n## Notes\n\nThis is a first stab at reorganizing our documentation, and I believe this must be done iteratively. Breaking down our huge `index.adoc` file will help us and avoid PR decay, merge conflicts, etc. And should give some fresh air to our file editors and browsers.\n\n@rwinch I know we're now relying on docbook, but I was wondering if something needed to be done re asciidoctor/asciidoctor-gradle-plugin#52 if we want to support multi-page documentation natively?\n\n@rstoyanchev using asciidoctor's `toclevels: N`, we can make that TOC significantly smaller - but I think that relying on docbook to render html prevents us from doing so at the moment.\n\nThoughts?\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/751#issuecomment-77410087"
    },
    {
        "project": "pulsar",
        "id": 983228833,
        "body": "\r\n> @Anonymitaet\r\n> \r\n> Please don't block the PR due to the documentation format. yes, I agree, the better format helps but the reorganization of the content can be handled in separate PR as well because every individual has a different idea about the formation.\r\n\r\nHi @rdhabalia, \r\n- I did not mean to block anything, I just want to improve the user experience since no one would like to read \"a mountain of\" information at a glance. Technical information should be easy to find/understand/use.\r\n- Agree that documentation can be submitted in a separate PR. \r\n- Everyone has a different opinion on everything but there are some general rules/standards accepted by the majority. In technical writing, often, text that is difficult to understand in paragraph form becomes clear in a table format. Tables help highlight relationships among similar pieces of information and are easily scanned for quick reference.\r\n",
        "url": "https://github.com/apache/pulsar/pull/12902#issuecomment-983228833"
    },
    {
        "project": "skywalking",
        "id": 901243931,
        "body": "# [Codecov](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) Report\n> Merging [#7486](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (a5d4124) into [master](https://codecov.io/gh/apache/skywalking/commit/39d23c16785b73b7607faa2b09ff3db94cd5333a?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) (39d23c1) will **decrease** coverage by `9.83%`.\n> The diff coverage is `76.56%`.\n\n[![Impacted file tree graph](https://codecov.io/gh/apache/skywalking/pull/7486/graphs/tree.svg?width=650&height=150&src=pr&token=qrILxY5yA8&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)\n\n```diff\n@@             Coverage Diff              @@\n##             master    #7486      +/-   ##\n============================================\n- Coverage     58.81%   48.98%   -9.84%     \n+ Complexity     4390     3874     -516     \n============================================\n  Files          1035     1901     +866     \n  Lines         26592    41258   +14666     \n  Branches       2627     4619    +1992     \n============================================\n+ Hits          15640    20210    +4570     \n- Misses         9571    19930   +10359     \n+ Partials       1381     1118     -263     \n```\n\n\n| [Impacted Files](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | Coverage \u0394 | |\n|---|---|---|\n| [.../provider/log/listener/RecordAnalysisListener.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9wcm92aWRlci9sb2cvbGlzdGVuZXIvUmVjb3JkQW5hbHlzaXNMaXN0ZW5lci5qYXZh) | `72.41% <\u00f8> (\u00f8)` | |\n| [...ng/oap/server/core/query/type/ServiceInstance.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItY29yZS9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvc2t5d2Fsa2luZy9vYXAvc2VydmVyL2NvcmUvcXVlcnkvdHlwZS9TZXJ2aWNlSW5zdGFuY2UuamF2YQ==) | `0.00% <\u00f8> (\u00f8)` | |\n| [...alking/oap/query/graphql/GraphQLQueryProvider.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcXVlcnktcGx1Z2luL3F1ZXJ5LWdyYXBocWwtcGx1Z2luL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9za3l3YWxraW5nL29hcC9xdWVyeS9ncmFwaHFsL0dyYXBoUUxRdWVyeVByb3ZpZGVyLmphdmE=) | `0.00% <0.00%> (\u00f8)` | |\n| [.../server/receiver/mesh/TelemetryDataDispatcher.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcmVjZWl2ZXItcGx1Z2luL3NreXdhbGtpbmctbWVzaC1yZWNlaXZlci1wbHVnaW4vc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvYW9wL3NlcnZlci9yZWNlaXZlci9tZXNoL1RlbGVtZXRyeURhdGFEaXNwYXRjaGVyLmphdmE=) | `7.00% <0.00%> (-77.77%)` | :arrow_down: |\n| [...g/oap/log/analyzer/dsl/spec/filter/FilterSpec.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvc3BlYy9maWx0ZXIvRmlsdGVyU3BlYy5qYXZh) | `65.33% <30.76%> (+23.79%)` | :arrow_up: |\n| [...pache/skywalking/oap/log/analyzer/dsl/Binding.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvQmluZGluZy5qYXZh) | `77.55% <50.00%> (+16.57%)` | :arrow_up: |\n| [...log/analyzer/dsl/spec/extractor/ExtractorSpec.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvc3BlYy9leHRyYWN0b3IvRXh0cmFjdG9yU3BlYy5qYXZh) | `50.00% <75.00%> (+23.00%)` | :arrow_up: |\n| [...lking/oap/query/graphql/resolver/LogTestQuery.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9zZXJ2ZXItcXVlcnktcGx1Z2luL3F1ZXJ5LWdyYXBocWwtcGx1Z2luL3NyYy9tYWluL2phdmEvb3JnL2FwYWNoZS9za3l3YWxraW5nL29hcC9xdWVyeS9ncmFwaHFsL3Jlc29sdmVyL0xvZ1Rlc3RRdWVyeS5qYXZh) | `88.13% <88.13%> (\u00f8)` | |\n| [...rg/apache/skywalking/oap/log/analyzer/dsl/DSL.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9sb2ctYW5hbHl6ZXIvc3JjL21haW4vamF2YS9vcmcvYXBhY2hlL3NreXdhbGtpbmcvb2FwL2xvZy9hbmFseXplci9kc2wvRFNMLmphdmE=) | `100.00% <100.00%> (\u00f8)` | |\n| [.../apache/skywalking/oap/meter/analyzer/dsl/DSL.java](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation#diff-b2FwLXNlcnZlci9hbmFseXplci9tZXRlci1hbmFseXplci9zcmMvbWFpbi9qYXZhL29yZy9hcGFjaGUvc2t5d2Fsa2luZy9vYXAvbWV0ZXIvYW5hbHl6ZXIvZHNsL0RTTC5qYXZh) | `96.29% <100.00%> (+6.29%)` | :arrow_up: |\n| ... and [1385 more](https://codecov.io/gh/apache/skywalking/pull/7486/diff?src=pr&el=tree-more&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation) | |\n\n------\n\n[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation).\n> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation)\n> `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n> Powered by [Codecov](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=footer&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation). Last update [c04b4e0...a5d4124](https://codecov.io/gh/apache/skywalking/pull/7486?src=pr&el=lastupdated&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=The+Apache+Software+Foundation).\n",
        "url": "https://github.com/apache/skywalking/pull/7486#issuecomment-901243931"
    },
    {
        "project": "spring-framework",
        "id": 33982138,
        "body": "@philwebb Update SPR please, I recreated the pull request to fix the target branch (Should be easier to merge now), it is the same branch, just fixed the pull request target branch to be 3.2.x\n",
        "url": "https://github.com/spring-projects/spring-framework/pull/454#issuecomment-33982138"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 705203573,
        "body": "> what is the scenario where having spark as a compile-time/runtime dependency is useful?\r\n\r\nGoing off what I remember from April, the driving force for this change was 2-fold: ES-Hadoop was being updated to better comply with Gradle's project modeling and the plugin that provided this functionality is a third party plugin that we have little to no control over. A lot of changes were happening in order to support cross compilation of Scala/Spark artifacts, and getting the community plugin that added `provided` and `optional` scopes to work with the changes we were making was becoming a maintenance nightmare.\r\n\r\nAt the time it seemed logical that a project depending on ES-Hadoop/Spark would already have defined Spark as a provided dependency, and thus having Spark as a compile time dependency of the library (which would end up transitive) would be relatively innocuous to the end-user project development environment.\r\n\r\nAs for the topic of deployments, my expectation at the time was that most deployments make use of the spark submit jar arguments to specify their dependencies. This was informed mostly from my experiences with the community over the years: Using the spark submit jar argument was the more common approach. Users who bundled their application and libraries together would just need to exclude Spark from that artifact. A small extra step.\r\n\r\nWith this line of thinking, we removed the plugin, set the scopes to what would be the most reasonable scopes Gradle could support out of the box, and moved on with our refactoring. Several months after this change though, I'm a bit more skeptical that the spark submit jar model of deployment is as ubiquitous as previously thought.\r\n\r\nI'll open an issue for returning the dependencies to `provided`. I'm not sure how reasonable it would be to return the previously `optional` dependencies, which at the time was the Spark SQL libraries, as well as every integration library in the uberjar. Those would most likely make a return as `provided` if we went forward with that change.",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1452#issuecomment-705203573"
    },
    {
        "project": "pulsar",
        "id": 1200360742,
        "body": "Release notes\r\nhttps://logging.apache.org/log4j/2.x/changes-report.html\r\n\r\n@liudezhi2098 if you think that there is a high security risk then please do not send a PR but reach out to private@pulsar.apache.org to discuss the problem.\r\nDisclosing a security issue on GH means to disclose it to the public and put pressure on the whole community ",
        "url": "https://github.com/apache/pulsar/pull/16884#issuecomment-1200360742"
    },
    {
        "project": "pulsar",
        "id": 1198952363,
        "body": "> @nodece Please add unit test to avoid any changes to break here.\r\n\r\nAdded after your comment, so merged. ",
        "url": "https://github.com/apache/pulsar/pull/16834#issuecomment-1198952363"
    },
    {
        "project": "spring-framework",
        "id": 1004232965,
        "body": "I haven't followed closely the latest discussions about the observability theme in Framework, so I'm probably missing a lot of context here. I'll try to list a few questions here to make progress on this PR.\r\n\r\n`ApplicationStartup` is a very basic infrastructure to collect data during context startup - it does not replace a Java profiler nor proper metrics instrumentation. Different types of events are mixed and collected under String keys, with no support for tags nor proper measurements (taking into account GC behavior, for example). The main goal here is to provide a lightweight infrastructure, with zero dependency, that you can bootstrap with the application context. Because startup is very time-sensitive, our number one priority here is to avoid expensive operations or costly dependencies.\r\n\r\nThis data is still useful and could be collected for metrics purposes, this is why we've discussed how the `BufferingApplicationStartup` implementation could re-expose collected information for the metrics infrastructure (see spring-projects/spring-boot#27475).\r\n\r\nTo me, the first step towards observability integration in Spring Framework would be to target an existing metric where instrumentation can be considered quite convoluted (because done outside of Framework) and bake it in. If anything, I thought that the observability theme could deprecate `ApplicationStartup` in favor of targeted metrics with proper types (counter, gauges, distributions, etc).\r\n\r\nI'm not seeing any new API for collecting metrics in Framework. Are we expecting to depend directly on Micrometer in the core Framework?\r\n\r\nWhile \"runtime metrics\" can rely on a configured registry in the application context, I can see how collecting metrics before the application context is fully refreshed is a different problem and should be considered separately. Was this discussed already?",
        "url": "https://github.com/spring-projects/spring-framework/pull/27779#issuecomment-1004232965"
    },
    {
        "project": "druid",
        "id": 634211872,
        "body": "I think now I see more clear what you want to do. This change doesn't seem doing anything with CalciteQueryTests, but is more for local testing and potentially integration testing in the future. If it's the case, the title seems confusing to me. \r\n\r\n> > 2. Also, directly setting rows and using ingestion specs can result in different data stored in segments, for example when rollup is set. How do we manage this?\r\n> \r\n> I believe all the ingestion specs specify rollup is disabled so that they mimic what the tables look like in CalciteTests\r\n\r\nSorry, I meant the ingestion specs can be evolved independently from the data in CalciteTests. I think it would be fine if they don't have to be synced.\r\n\r\n> > 3. Finally, since ingestion specs don't look related to any other codes directly, it's likely possible that someone will delete those ingestion specs in the future. How can we avoid this?\r\n> \r\n> Good question. I was deciding between leaving these in the druid-sql package vs the druid-integration-tests package. Eventually I want to move these to the integration testing framework. This way devs can write easy sql validation integration tests without having to worry about how Calcite translates the query to a native query. I was planning on moving it into the integration testing framework as part of another patch. Hopefully that patch will make it a lot easier for any dev to write a SQL integration test against these datasources.\r\n\r\nWhy do we want to not worry about Calcite planning? What the SQL layer does in Druid is mostly query planning. It doesn't seem right to me to not test query planning part for SQL layer. Or am I missing something?\r\n\r\nThis makes me curious what kind of integration testing we want for SQL. Per my understanding, the integration testing should verify the integrity of the entire Druid system. AFAIT, the missing parts in SQL layer are 1) querying SQL via HTTP or JDBC, and 2) querying against large data (this is more like for query processing engine rather than SQL but would be nice to run using SQL since it's easier). The details of query planning, query processing, etc should be covered by unit testing because integration testing takes too long. For 1), we don't need to test various queries since it focuses on `via HTTP` and `via JDBC` parts. For 2), we need to test various queries but also need large data. For integration test data, we are currently using precreated segments as well as synthetic data generator. The new ingestion specs doesn't seem to have benefits over existing data in neither cases. It might be more useful to share the queries in CalciteQueryTest and 2).",
        "url": "https://github.com/apache/druid/pull/9922#issuecomment-634211872"
    },
    {
        "project": "hadoop",
        "id": 907053394,
        "body": "> Hi,\r\n> \r\n> only just seen this...feel free to let me know when you are doing big bits work, especially with unstable bits of the API that I've worked on.\r\n> \r\n> FWIW, I'll be able to use this in the manifest committer #2971; as it well let me go straight from the listing of the directory of manifests to opening each manifest without the HEAD requests. I also have an internal PoC parquet lib which does the same -and showed up the bugs in s3a FS there.\r\n> \r\n> I have some concerns with the patch as is, which I'd like addressed in a \"stabilize abfs openFile.withStatus\" JIRA, which, once is in, can go in to -3.3 back to back with this\r\n> \r\n> In particular, the openFile code on L705 warns if the FileStatus is of the wrong type.\r\n> This turns out to cause problems in Hive or any other FS which wraps the inner FS, and\r\n> creates its own FileStatus objects. Why does hive do this? Because FileStatus.getPath()\r\n> returns the full path of the reference; hive has to replace this with a new FileStatus\r\n> with its mapped path.\r\n> \r\n> _printing WARN if the type is wrong is going to fill up Hive logs and remove all speedup options_\r\n> \r\n> In #3289 I soften the requirement for the path to == the object; I'm only checking the final elements are equal. This allows parquet through Hive to use the API.\r\n> \r\n> https://github.com/apache/hadoop/pull/3289/files#diff-4050f95b7e3912145415b6e2f9cd3b0760fcf2ce96bf0980c6c30a6edad2d0fbR5402\r\n> \r\n> In #2584, which is my real \"stabilize openfile patch\" I do it properly\r\n> \r\n> https://github.com/apache/hadoop/blob/700f99835f21571d9f29f2194327894776abe13f/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/PrepareToOpenFile.java\r\n> \r\n> * only check name\r\n> * reject directories\r\n> * handle LocatedFileStatus\r\n> * and any other FS subclass, only picking up the length value\r\n> * and as this has got complex, factored out into its own class\r\n> \r\n> Really, it's only the length which is important, other than that it is just a declaration of \"this is a file, skip the HEAD\"\r\n> \r\n> So for ABFS to work, I'm going to require the equivalent of that PrepareToOpenFile: support for more FileStatus types. Doesn't need to be factored out, though that turned out to work well for testing.\r\n> \r\n> This makes clear that I need to get #2584 in; it's been neglected.\r\n> \r\n> That patch allows for a seek policy to be specified; all the whole file reads ask for sequential IO, so when hive clusters are deployed with the default seek policy set to random, things like distcp and hadoop fs put don't fail. It critically defines a standard set of seek policies, which Azure can implement too.\r\n> I've realized we need one more \"whole-file\", which says \"no need for lazy seek, just do the GET\".\r\n> \r\n> Does anyone want to take that PR up? I keep getting distracted by other high-importance tasks.\r\n> \r\n> Meanwhile the manifest committer can be the first user of this; I will verify that it delivers some speed up. It should: job commit with 1000 tasks will go from one list, 1000 HEAD, 1000 GET to 1 list and the 1000 GETs.\r\n> \r\n> FWIW, this is the code in callthe magic committer patch which becomes safe to use with the filename fix there:\r\n> https://github.com/apache/hadoop/pull/3289/files#diff-3da795bf4618a7e504e21d3c53f9f7a42c52a75fe232906feb4e66f98794d41eR94\r\n> \r\n> ...I'll do the same with the manifest committer; if I do it this week its new tests will help stress the feature.\r\n\r\nHi @steveloughran,\r\n\r\nThanks a lot for taking a look at this change. We will make updates to address the comments provided. \r\n\r\nThere is one comment on the eTag that I would like to check with you. \r\n\r\neTag is a parameter that is sent with every read request to server today. Server ensures that the file is indeed of the expected eTag before proceeding with the read operation. If the file was recreated during the timespan inputStream was created and read is issued, eTag check will fail and so will the read, else read will end up wrongly getting data from the new file. This is the reason we are mandating eTag to be present and in else case fall back to triggering a GetFileStatus. We will changing the log type to debug as per your comment in the else case.\r\n\r\nI did have a look at [#2584](https://github.com/apache/hadoop/pull/2584) which provides the means to pass mandatory and optional keys through OpenFileParameters. Noticed an example where S3 eTag is passed as an optional key (referring to [link](https://github.com/apache/hadoop/pull/2584/files#diff-a4a5b4990d0a979db924e2f338eab5421d9909ccbafee3846bf76b3e816f31d2R49) , for the Hive case where FileStatus is wrapped, is this how eTag gets passed ? And was also wondering how we can get various Hadoop workloads to send it without being aware of which is the current active FileSystem considering the key name is specific to it.\r\n\r\nAlso wanted to check if it would be right to add eTag as a field into the base FileStatus class. Probably as a sub structure within FileStatus so that it can be expanded in future to hold properties that Cloud storage systems commonly depend on.\r\n\r\nOpenFile() change is one of the key performance improvement that we are looking at adopting to. The aspect of FileStatus being passed down in itself reduces HEAD request count by half and look forward to adopting to the new set of read policies too. We will work on understanding how to map various read policies to the current optimizations that the driver has for different read patterns. I think that would translate to a change equivalent to PrepareToOpenFile for ABFS driver. Would it be ok if we make this change once [#2584](https://github.com/apache/hadoop/pull/2584) checks in ? \r\nCurrently we are in a bit of tight schedule and short staffed as we aim to complete on the feature work tracked in [HADOOP-17853](https://issues.apache.org/jira/browse/HADOOP-17853) and another customer requirement that we are in feasibility analysis stage.\r\n",
        "url": "https://github.com/apache/hadoop/pull/2975#issuecomment-907053394"
    },
    {
        "project": "hadoop",
        "id": 872831435,
        "body": "> @tomscut Thanks for contribution.\r\n> I see that getLiveDatanodeStorageReport and getBlocks mostly have the same code. Better to extract them into a new method and it will be more clean.\r\n\r\nThanks @ferhui for your review and advice, I will extract a new method.",
        "url": "https://github.com/apache/hadoop/pull/3140#issuecomment-872831435"
    },
    {
        "project": "apm-agent-java",
        "id": 814643310,
        "body": "We can't really rename them all with `IT` nor consider those using Testcontainers to be integration tests, as some of them are unit tests. For example, the RabbitMQ plugin is a good example of this where common instrumentation is tested with one RabbitMQ version and is then tested with multiple versions in integration tests.\r\n\r\nHowever, I think once we have migrated to Junit5, using appropriate groups/tags to allow running subsets of tests would be relevant.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1748#issuecomment-814643310"
    },
    {
        "project": "spring-framework",
        "id": 949779104,
        "body": "Sorry but I still don't understand the current situation.\r\n\r\n1) You're mentioning a \"non-String collection use case\" but `StringUtils#cleanPath` is not it, right? Is there a code path in Spring showing this behavior?\r\n\r\n2) If we can improve the `StringUtils#cleanPath` performance, I'd like to get more information about the performance issue you've hit - what's calling this method and what's the usage profile (repeated calls, long strings, etc).\r\n\r\nWith those two points I can work on JMH benchmarks and get a clearer picture. Thanks!",
        "url": "https://github.com/spring-projects/spring-framework/pull/27557#issuecomment-949779104"
    },
    {
        "project": "hadoop",
        "id": 934635623,
        "body": "> > On the other hand,Pushing those changes after a major milestone would mean only one Jira and one PR (i.e., HADOOP-XYZ: migrating banned imports to use restrict-imports-enforcer-rule).\r\n> \r\n> IMHO the problem here is that if someone has already pushed a couple of commits with illegal imports, then we will have to take care of such individual modules as well, it's kind of rework right?\r\n\r\nNot really, unless the reviewer proceeds with merging the changes ignoring the checkStyle errors, which it is not supposed to happen.\r\nEven if we move forward with pushing the pom changes, we still have the same issue with guava classes defined in checkstyle.xml\r\n\r\nAnyway, since #3087 and #3049 are already in, then the way to move forward is 1- to proceed with this PR; 2- to file a new jira to migrate `checkStyle.illegalImports` to the parent pom file. For sake of coherence and consistency, this jira has to be merged before any further changes to bannedImports in all pom files.\r\n\r\nThanks @virajjasani for the efforts you put into this. Great job!\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/3503#issuecomment-934635623"
    },
    {
        "project": "druid",
        "id": 1171429202,
        "body": "Build is clean, except for a known flaky IT: [37272.52 (Compile=openjdk8, Run=openjdk8) perfect rollup parallel batch index integration test with Indexer](https://app.travis-ci.com/github/apache/druid/jobs/575188113), reported as [Issue #12692](https://github.com/apache/druid/issues/12692). Ready for review.",
        "url": "https://github.com/apache/druid/pull/12708#issuecomment-1171429202"
    },
    {
        "project": "skywalking",
        "id": 1104686669,
        "body": "In my mind, I prefer to generate this setting from all declaration of handlers, and set this when server start. This is not a setting, this is a expose limitation, which is good. \nI know this was introduced by jetty, but this kind of CVE happenned from time to time for any HTTP server.\n@wankai123 Let's not check it, and accept this as a part of handler registration, and lock it just before server starts.\n",
        "url": "https://github.com/apache/skywalking/pull/8917#issuecomment-1104686669"
    },
    {
        "project": "pulsar",
        "id": 1192298802,
        "body": "> Could you add a unit test to protect this change?\r\n\r\n@BewareMyPower Thanks for your reminder, I looked again, and It is found that we cannot be modify the logic in the `MultiTopicsConsumerImpl.subscribeAsync` method. Because users may use consumer like this:\r\n\r\n``` java\r\n        client.newConsumer().topic(\r\n                        \"persistent://public/dafault/test-topic1-partition-0\",\r\n                        \"persistent://public/dafault/test-topic2-partition-1\"\r\n         );\r\n```\r\nIf force subscription to PartitionTopic inside the method, Then all partitions of `test-topic1` and `test-topic2` will be subscribed. This is a breaking change.\r\n\r\nSo, I revert the original changes.\r\n\r\nThe new changes are: Inside the `PatternTopicsChangedListener.onTopicsAdded` method let it use partitioned name to subscribe.\r\n\r\n@codelipenghui @mattisonchao @Technoboy- @nodece @BewareMyPower Sorry, please help review again.",
        "url": "https://github.com/apache/pulsar/pull/16719#issuecomment-1192298802"
    },
    {
        "project": "flink",
        "id": 1205786492,
        "body": ">It's a separate change--but what do you think about adding warning logs to UnregisteredMetricGroup to emit a log that says a metric may not be reported\r\n\r\nI'm not a fan of warnings if there's nothing the user can do about it, especially when the chance of us fixing these instances are slim.",
        "url": "https://github.com/apache/flink/pull/20449#issuecomment-1205786492"
    },
    {
        "project": "skywalking",
        "id": 885459201,
        "body": "2 things first. \n1. changes.md in the root should be updated.\n2. As you said duplicate case in pure rabbitmq case, it is possible to use this replace the existing plugin rather than adding a new one? ",
        "url": "https://github.com/apache/skywalking/pull/7363#issuecomment-885459201"
    },
    {
        "project": "pulsar",
        "id": 1191438111,
        "body": "> I think this will remove the possibility of having a stable assignment logic. Right now, if you specify the same consumer names, you will be guaranteed to receive the same keys after reconnections.\r\n\r\nOh, good point. Maybe we should try to add more information to the topic stats instead of changing the behavior.",
        "url": "https://github.com/apache/pulsar/pull/16672#issuecomment-1191438111"
    },
    {
        "project": "apm-agent-java",
        "id": 1163055663,
        "body": "> Congrats for pushing this through @SylvainJuge. Apologies for airing dirty laundry here, notwithstanding how badly designed, badly implemented, badly timed, or badly intended my own contribution and work is; suffice to say due to the pushiness and entitlement of @fern-we roping in @kananindzya its unfortunate i never want to contribute to elastic/apm-agent ever again.\r\n\r\nI am really sorry that you've felt offended by what happened here @abdulazizali77 .\r\n\r\nYou initiated this plugin in https://github.com/elastic/apm-agent-java/pull/1932 PR and your commits served as the basis of this PR as we can see in the [commit log](https://github.com/elastic/apm-agent-java/pull/2229/commits), so you still deserve most of the credit for this.\r\n\r\nWhat @kananindzya did (or what I assume the intent is) was just trying to move this forward through another PR, because he is not an Elastic employee he could not directly push nor directly contribute to your PR, if we had to do it again I would have suggested him opening a PR against your own fork/branch instead of opening another one based on your work.\r\n\r\nThere is nothing wrong with your contribution here, even if the code you contributed was later removed or modified, it helped building the current form which we are about to ship and we should all feel grateful for that. Unfortunately, working on agents means that we often have to iterate a lot and often rewrite code often more than once with different approaches before being able to ship something.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2229#issuecomment-1163055663"
    },
    {
        "project": "hadoop",
        "id": 1164984485,
        "body": "Do we really need to introduce this config? Seems like an overkill. \r\nI think 400 Bad Request are supposed to be non retry-able.",
        "url": "https://github.com/apache/hadoop/pull/4483#issuecomment-1164984485"
    },
    {
        "project": "flink",
        "id": 1050853895,
        "body": "After https://github.com/apache/flink/pull/18869 is merged, we can simplify the tests wrt changes in `LongStateHandle`.",
        "url": "https://github.com/apache/flink/pull/18901#issuecomment-1050853895"
    },
    {
        "project": "apm-agent-java",
        "id": 876238536,
        "body": "There is one more thing that can be fixed... in case of an older version there should not be any instrumentation by the `JavalinHandlerLambdaInstrumentation`. With the most recent refactoring of the `getHandlerType()` method, this requires a context, which is not available in that specific instrumentation, as it happens when a new handler is registered, where as the context is only created when a HTTP request is processed.\r\n\r\nWhat would be a proper workaround for this? Using another `MethodHandles.lookup()` in the other instrumentation?",
        "url": "https://github.com/elastic/apm-agent-java/pull/1822#issuecomment-876238536"
    },
    {
        "project": "skywalking",
        "id": 1067981124,
        "body": "For making reviewers easier, here are the key points.\r\n\r\n## New StorageBuilder\r\n```java\r\n/**\r\n * Converter between the give T and K.\r\n *\r\n * @param <T> A storage entity implementation.\r\n */\r\npublic interface StorageBuilder<T extends StorageData> {\r\n    /**\r\n     * Use the given converter to build an OAP entity object.\r\n     *\r\n     * @param converter to transfer data format\r\n     * @return an OAP entity object\r\n     */\r\n    T storage2Entity(Convert2Entity converter);\r\n\r\n    /**\r\n     * Use the given converter to build a database preferred structure.\r\n     *\r\n     * @param entity    to be used\r\n     * @param converter provides the converting logic and hosts the converted value. Use {@link\r\n     *                  Convert2Storage#obtain()} to read the converted data.\r\n     */\r\n    void entity2Storage(T entity, Convert2Storage converter);\r\n}\r\n```\r\n\r\nStorageBuilder wouldn't accept HashMap as a parameter or the return value, now `Convert2Entity` and `Convert2Storage` interfaces open the responsibility of this temporary value holder to the storage plugin implementations.\r\n\r\n## Default HashMapConverter\r\nTo keep all codes are changed smoothly, I introduced `HashMapConverter` as the default converter. @lujiajing1126 Such as BanyanDB plugin could use its native one, and use native structure and get rid of HashMap permanently. \r\n\r\n```java\r\npublic class HashMapConverter {\r\n    /**\r\n     * Stateful Hashmap based converter, build object from a HashMap type source.\r\n     */\r\n    @RequiredArgsConstructor\r\n    public static class ToEntity implements Convert2Entity {\r\n        private final Map<String, Object> source;\r\n\r\n        @Override\r\n        public Object get(final String fieldName) {\r\n            return source.get(fieldName);\r\n        }\r\n    }\r\n\r\n    /**\r\n     * Stateful Hashmap based converter, from object to HashMap.\r\n     */\r\n    public static class ToStorage implements Convert2Storage<Map<String, Object>> {\r\n        private Map<String, Object> source;\r\n\r\n        public ToStorage() {\r\n            source = new HashMap();\r\n        }\r\n\r\n        @Override\r\n        public void accept(final String fieldName, final Object fieldValue) {\r\n            source.put(fieldName, fieldValue);\r\n        }\r\n\r\n        @Override\r\n        public Object get(final String fieldName) {\r\n            return source.get(fieldName);\r\n        }\r\n\r\n        @Override\r\n        public Map<String, Object> obtain() {\r\n            return source;\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis **HashMapConverter** is super easy, just as a wrapper of HashMap.\r\n\r\n____\r\n\r\nAll other code changes are just following these two proposals, and making codes work as usual.\r\n\r\nNotice, there could be more potential changes for JDBC and other storage plugins, such as wrap ResultSet as a new ToEntity, rather than `ResultSet->HashMap->Entity`",
        "url": "https://github.com/apache/skywalking/pull/8684#issuecomment-1067981124"
    },
    {
        "project": "hadoop",
        "id": 1207149152,
        "body": ":broken_heart: **-1 overall**\r\n\r\n\r\n\r\n\r\n\r\n\r\n| Vote | Subsystem | Runtime |  Logfile | Comment |\r\n|:----:|----------:|--------:|:--------:|:-------:|\r\n| +0 :ok: |  reexec  |   0m 58s |  |  Docker mode activated.  |\r\n|||| _ Prechecks _ |\r\n| +1 :green_heart: |  dupname  |   0m  1s |  |  No case conflicting files found.  |\r\n| +0 :ok: |  codespell  |   0m  0s |  |  codespell was not available.  |\r\n| +0 :ok: |  detsecrets  |   0m  0s |  |  detect-secrets was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +0 :ok: |  buf  |   0m  0s |  |  buf was not available.  |\r\n| +1 :green_heart: |  @author  |   0m  0s |  |  The patch does not contain any @author tags.  |\r\n| +1 :green_heart: |  test4tests  |   0m  0s |  |  The patch appears to include 3 new or modified test files.  |\r\n|||| _ trunk Compile Tests _ |\r\n| +0 :ok: |  mvndep  |  15m 25s |  |  Maven dependency ordering for branch  |\r\n| +1 :green_heart: |  mvninstall  |  25m 49s |  |  trunk passed  |\r\n| +1 :green_heart: |  compile  |   4m  1s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  compile  |   3m 29s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  checkstyle  |   1m 32s |  |  trunk passed  |\r\n| +1 :green_heart: |  mvnsite  |   2m 19s |  |  trunk passed  |\r\n| +1 :green_heart: |  javadoc  |   2m  6s |  |  trunk passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 56s |  |  trunk passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 58s |  |  trunk passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 48s |  |  branch has no errors when building and testing our client artifacts.  |\r\n|||| _ Patch Compile Tests _ |\r\n| +0 :ok: |  mvndep  |   0m 31s |  |  Maven dependency ordering for patch  |\r\n| +1 :green_heart: |  mvninstall  |   1m 40s |  |  the patch passed  |\r\n| +1 :green_heart: |  compile  |   3m 49s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  cc  |   3m 49s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 49s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 generated 3 new + 446 unchanged - 0 fixed = 449 total (was 446)  |\r\n| +1 :green_heart: |  compile  |   3m 16s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  cc  |   3m 16s |  |  the patch passed  |\r\n| -1 :x: |  javac  |   3m 16s | [/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/results-compile-javac-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07.txt) |  hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server-jdkPrivateBuild-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 generated 3 new + 370 unchanged - 0 fixed = 373 total (was 370)  |\r\n| +1 :green_heart: |  blanks  |   0m  0s |  |  The patch has no blanks issues.  |\r\n| +1 :green_heart: |  checkstyle  |   1m 11s |  |  hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server: The patch generated 0 new + 11 unchanged - 1 fixed = 11 total (was 12)  |\r\n| +1 :green_heart: |  mvnsite  |   1m 49s |  |  the patch passed  |\r\n| +1 :green_heart: |  javadoc  |   1m 31s |  |  the patch passed with JDK Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1  |\r\n| +1 :green_heart: |  javadoc  |   1m 27s |  |  the patch passed with JDK Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07  |\r\n| +1 :green_heart: |  spotbugs  |   3m 43s |  |  the patch passed  |\r\n| +1 :green_heart: |  shadedclient  |  21m 42s |  |  patch has no errors when building and testing our client artifacts.  |\r\n|||| _ Other Tests _ |\r\n| +1 :green_heart: |  unit  |   3m  5s |  |  hadoop-yarn-server-common in the patch passed.  |\r\n| +1 :green_heart: |  unit  | 102m 57s |  |  hadoop-yarn-server-resourcemanager in the patch passed.  |\r\n| +1 :green_heart: |  asflicense  |   0m 50s |  |  The patch does not generate ASF License warnings.  |\r\n|  |   | 231m 15s |  |  |\r\n\r\n\r\n| Subsystem | Report/Notes |\r\n|----------:|:-------------|\r\n| Docker | ClientAPI=1.41 ServerAPI=1.41 base: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/artifact/out/Dockerfile |\r\n| GITHUB PR | https://github.com/apache/hadoop/pull/4711 |\r\n| Optional Tests | dupname asflicense compile javac javadoc mvninstall mvnsite unit shadedclient spotbugs checkstyle codespell detsecrets cc buflint bufcompat |\r\n| uname | Linux 0fd97407eebb 4.15.0-65-generic #74-Ubuntu SMP Tue Sep 17 17:06:04 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux |\r\n| Build tool | maven |\r\n| Personality | dev-support/bin/hadoop.sh |\r\n| git revision | trunk / 8e63615447d74945f2ec2f11287f1b5f2867a29c |\r\n| Default Java | Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n| Multi-JDK versions | /usr/lib/jvm/java-11-openjdk-amd64:Private Build-11.0.15+10-Ubuntu-0ubuntu0.20.04.1 /usr/lib/jvm/java-8-openjdk-amd64:Private Build-1.8.0_312-8u312-b07-0ubuntu1~20.04-b07 |\r\n|  Test Results | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/testReport/ |\r\n| Max. process+thread count | 949 (vs. ulimit of 5500) |\r\n| modules | C: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager U: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server |\r\n| Console output | https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4711/2/console |\r\n| versions | git=2.25.1 maven=3.6.3 spotbugs=4.2.2 |\r\n| Powered by | Apache Yetus 0.14.0 https://yetus.apache.org |\r\n\r\n\r\nThis message was automatically generated.\r\n\r\n",
        "url": "https://github.com/apache/hadoop/pull/4711#issuecomment-1207149152"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 234259872,
        "body": "@costin @jbaiera, this PR introduces the SlicePartitionDefinition. I had to refactor (cleanup) some parts to make it work. This is still a work in progress and I need to add more tests. In the meanwhile it would be very helpful if you can check the progress. Considering the size of the PR I am available at your convenience for a deep dive. \n",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/812#issuecomment-234259872"
    },
    {
        "project": "spring-framework",
        "id": 596042295,
        "body": "I have pushed the implementation of the new function `getContentAsString()` down to the Resource interface and provided it with a default behavior of `toString()` to avoid having to override in both the Web and MVC modules.  \r\n\r\nThe Concrete implementation was provided at the `AbstractFileResolvingResource` level as any local or jar file should be able to be serialized. It no longer uses Apache Commons and uses the same base raw Java implementation as the Kotlin equivalent.  This does rely on Java version being 7 or higher which I did not consider an issue as the minimum for the project is 8.\r\n\r\nUltimately I expect this to be utilized during Contract Driven and Test Driven Development processes to avoid having to use Raw String variables and to pull developers away from using ResourceUtils directly with either Commons or the overtly verbose `Files.readBytes(File.path, Charset))` and having to handle the exceptions on their own (or forgetting) \r\n\r\nThis is a largely QOL enhancement to the Java developer experience as Kotlin provides a convenience function known as `getResource(path).readText()` \r\n\r\nAn alternative reasoning behind this enhancement is to close the need for access to ResourceUtils outside of the spring framework and allow for moving that Class to package private in a future change.",
        "url": "https://github.com/spring-projects/spring-framework/pull/24651#issuecomment-596042295"
    },
    {
        "project": "apm-agent-java",
        "id": 933473930,
        "body": "According to https://github.com/elastic/apm-agent-java/issues/2163#issuecomment-933268429, this still doesn't fix the issue.\r\n\r\nI think what happens is that because we resolve classes from the agent CL before the target CL, the `Log4j2AppenderAppendAdvice` refers to the agent-bundled types for `org.apache.logging.log4j.core.Appender` and\r\n`org.apache.logging.log4j.core.LogEvent`.\r\n\r\nhttps://github.com/elastic/apm-agent-java/blob/0491135861ae91c649579d3c739b5d6702968f90/apm-agent-plugins/apm-log-shader-plugin/apm-log4j2-plugin/src/main/java/co/elastic/apm/agent/log4j2/Log4j2AppenderAppendAdvice.java#L32-L33\r\n\r\nWe could re-introduce the shading but I'd like to avoid that. As an alternative, we could avoid delegating looking up log4j2 classes to the agent class load within the `IndyPluginClassLoader`. Ideally, plugins shouldn't even have a compile dependency on log4j2. That could be done by creating a separate module that does the logging setup.\r\n\r\nWe'll have to ensure we don't use slf4j types in advice method signatures for similar reasons. We can ensure that in `co.elastic.apm.agent.bci.ElasticApmAgent#validateAdvice`.\r\n\r\n> As a side note TraceMethodInstrumentation, JdkHttpServerInstrumentation, RoutesInstrumentation and Struts2TransactionNameInstrumentation are not TracerAwareInstrumentation. Is that a problem?\r\n\r\nNo, I don't think that's an issue.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2167#issuecomment-933473930"
    },
    {
        "project": "hadoop",
        "id": 509627399,
        "body": "As it's a bigger patch here is a quick guidance what is included:\r\n\r\n```\r\n hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/RpcClient.java            \r\n hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzFs.java\r\n```\r\n\r\nThis is the original fix by @xiaoyuyao It makes the RpcClient compatible with older hadoop versions\r\n\r\n```\r\n hadoop-ozone/dist/src/main/compose/ozone-mr\r\n hadoop-ozone/dist/src/main/compose/ozone-fs\r\n```\r\nozone-mr dir is reorganized to have subdirectories for mapreduce tests (hadoop 2.7, 3.1, 3.2).\r\nSmoketest shell scripts are added\r\n\r\nozone-fs dir is deleted as ozone-mr includes all the functionality and it's better structured.\r\n\r\n```\r\n hadoop-ozone/dist/src/main/compose/testlib.sh                                                  \r\n```\r\n\r\nImproved to pass custom robot test arguments. you case see the usage in ozone-mr/hadoop27/test.sh\r\n\r\n```\r\n hadoop-ozone/dist/src/main/smoketest/ozonefs/hadoopo3fs.robot\r\n```\r\n\r\nTrival fix, the test was wrong until now.\r\n\r\n```\r\n hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneClientAdapterImpl.java\r\n hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/BasicOzoneFileSystem.java\r\n hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/FileStatusAdapter.java\r\n hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/FilteredClassLoader.java\r\n hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneClientAdapter.java\r\n```\r\n\r\nThis is the second part of the fix (on top of the fixes from @xiaoyuyao. As the FileStatus (hadoop3) is not compatible with FileStatus (hadoop2), we need a class which can be shared between hadoop3 and hadoop2 (FileStatusAdapter, includes only simple java types). \r\n\r\n```\r\n hadoop-ozone/ozonefs-lib-legacy/pom.xml\r\n```\r\n\r\nThis is the third part of the fix. The token files should be available for hadoop2 classloader even if they are the cluster is not secure (to serialize/unserialize tokens). The required classes are copied directly to the jar file.",
        "url": "https://github.com/apache/hadoop/pull/1065#issuecomment-509627399"
    },
    {
        "project": "druid",
        "id": 1183687543,
        "body": ">To me this sounds like the basics of the feature are production-ready. There may be various callouts about performance, but it seems that the compatibility story is tight enough that the feature doesn't need the experimental markings at this time.\r\n\r\nI think that is fair :+1:\r\n\r\n>You mentioned being somewhat less certain about the behavior of nested arrays. We should figure out if that part is going to be included in the production-ready feature set, or if we'll call that particular scenario out as an evolving area. What is your intent & recommendation in this area?\r\n\r\nIt is definitely going to be an evolving area (which I think could be said of our array support in general), though there are probably a narrow range use cases that could be used today, mainly where array lengths and element positions are known and have some meaning and query time operations are primarily extracting and operating on individual elements. This is more or less the current limitations of `flattenSpec` with nested arrays I think.\r\n\r\nThere are some lower hanging fruit that would  improve stuff in the near term, some of which might be possible to get in before the next release. The first  supporting wildcards in the subset of the path syntax that we support, which would allow `JSON_QUERY` and `JSON_VALUE` (or something like it.. i'm not sure entirely how the `RETURNING` syntax would work with array types in SQL so need to do some tinkering there) to extract complete arrays. For `JSON_QUERY` these results would still be `COMPLEX<json>` typed , but `JSON_VALUE`* would spit out druid literal array types (`ARRAY<LONG>`, `ARRAY<STRING>`, etc).\r\n\r\nFor nested arrays of JSON objects extracted by `JSON_QUERY`, i think we will want a way to convert a `COMLEX<json>` into an `ARRAY<COMPLEX<json>>` so that they too can take part in array operations, _especially_ once we add a native `UNNEST` function to transform arrays into tables, which would be the path to exploding out these nested objects and performing operations on their contents.\r\n\r\nAt some point after that, I intend to introduce the option to begin storing literal arrays in nested `ARRAY` typed columns instead of them broken out into separate columns for individual elements like they currently exist (so that array operations don't have to decompress a bunch of separate columns to do stuff). \r\n\r\nI guess I'm getting a bit into the weeds, but my point I guess is that I think this feature will evolve along-side and should help us improve array support in general, so am hyped to get it there.",
        "url": "https://github.com/apache/druid/pull/12753#issuecomment-1183687543"
    },
    {
        "project": "apm-agent-java",
        "id": 861339262,
        "body": "I thought that having the \"log once\" logger disable all log levels once it has already been used would have helped reuse it.\r\n\r\nHowever, when trying to use it with `co.elastic.apm.agent.servlet.ServletVersionInstrumentation` that does not work as two log statements are issued, and that might introduce a small concurrency bug as \"checking the logger state\" and \"issuing log statement\" are not done in an atomic operation.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1863#issuecomment-861339262"
    },
    {
        "project": "spring-boot",
        "id": 1102197745,
        "body": "@chanhyeong tanks for the PR but I believe that the link is correct. The `features` at the beginning is an old artifact from a previous version but handled properly as we rewrite anchors.\r\n\r\nDid you notice a version of the Spring Boot docs where the link isn't working? If that's the case, please share a link to it and we can investigate. \r\n\r\nIrrespective of this, I don't think this change is correct.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30674#issuecomment-1102197745"
    },
    {
        "project": "spring-boot",
        "id": 1191780882,
        "body": "@wilkinsona I believe this should now be ready meaning I don't think my observation about this being blocked by #31703 was valid.\r\n\r\nI've taken a closer look at `FilterSecurityInterceptor` vs `AuthorizationFilter` and the latter should have _once per request_ semantics by default as it extends `OncePerRequestFilter`. However it also filters all dispatch types by default, which I disabled (note the second commit). That uncovered one issue with tests but now everything should build cleanly.",
        "url": "https://github.com/spring-projects/spring-boot/pull/31255#issuecomment-1191780882"
    },
    {
        "project": "spring-framework",
        "id": 712408461,
        "body": "It looks reasonable to me @sbrannen . The only concern I'd have is that we do allowable SpEL compilation of inline lists (not inline maps yet):\r\n\r\nIn InlineList:\r\n```\r\n\tpublic boolean isCompilable() {\r\n\t\treturn isConstant();\r\n\t}\r\n```\r\n\r\nSo maybe a few tests verifying compiled expressions using inline lists are still OK with the constant computation extended to include OpMinus nodes. I *think* it'll be ok from browsing the code, but worth testing.",
        "url": "https://github.com/spring-projects/spring-framework/pull/25921#issuecomment-712408461"
    },
    {
        "project": "hadoop",
        "id": 1154627715,
        "body": "Hi @ashutoshcipher thank you for reviewing the PR! I have tried to address your comments below:\r\n\r\n## I am thinking could there be a case where the block can show up in both liveReplicas and decommissioning, which could lead to any unnecessarily call to invalidateCorruptReplicas()\r\n\r\nI am not sure its possible for a block replica to be reported as both a liveReplica & a decommissioningReplica at the same time. Its my understanding that a replica on decommissioning datanode is counted as a decommissioningReplica & a non-corrupt replica on a live datanode is counted as a liveReplica. So a block replica on a decommissioning node will only be counted towards decommissioningReplica count & not liveReplica count. In my experience I have not seen the behavior you are mentioning before; let me know if you have a reference JIRA.\r\n\r\nIf the case you described is possible then in theory numUsableReplica would be greater than it should be. In the typical case where \"dfs.namenode.replication.min = 1\" this makes no difference because even if there is only 1 non-corrupt block replica on 1 decommissioning node then the corrupt blocks are invalidated regardless of wether or not numUsableReplica=1 or numUsableReplica=2 (due to double counting of replica as liveReplica & decommissioningReplica). In the case where \"dfs.namenode.replication.min > 1\" there could arguably be a difference because the corrupt replicas would not be invalidated if numUsableReplica=1 but they will be invalidated if numUsableReplica=2 (due to double counting of replica as liveReplica & decommissioningReplica).\r\n\r\nI think if this scenario is possible the correct fix would be to ensure that each block replica is only counted once towards either liveReplicas or decommissioningReplicas. Let me know if there is a JIRA for this issue & I can potentially look into the bug fix separately.\r\n\r\n## Edge case coming to my mind is when the we are considering the block on decommissioning node as useable but the very next moment, the node is decommissioned.\r\n\r\nFair point, I had considered this & mentioned this edge case in the PR description:\r\n\r\n```\r\nThe only perceived risk here would be that the corrupt blocks are invalidated at around the same time the decommissioning and entering maintenance nodes are decommissioned. This could in theory bring the overall number of replicas below the \"dfs.namenode.replication.min\" (i.e. to 0 replicas in the worst case). This is however not an actual risk because the decommissioning and entering maintenance nodes will not finish decommissioning until their is a sufficient number of liveReplicas; so there is no possibility that the decommissioning and entering maintenance nodes will be decommissioned prematurely.\r\n```\r\n\r\nAny replicas on decommissioning nodes will not be decommissioned until there are more liveReplicas than the replication factor for the block. Its only possible for decommissioningReplicas to be decommissioned at the same time corruptReplicas are invalidated if there are sufficient liveReplicas to satisfy the replication factor; in this case, because of the liveReplicas its safe to eliminate both the decommissioningReplicas & the corruptReplicas. If there is not a sufficient number of liveReplicas then the decommissioningReplicas will not be decommissioned but the corruptReplicas will be invalidated; the block will not be lost because the decommissioningReplicas will still exist.\r\n\r\nOne case you could argue is that if:\r\n- corruptReplicas > 1\r\n- decommissioningReplicas = 1\r\n\r\nBy invalidating the corruptReplicas we are exposing the block to a risk of loss should the decommissioningReplica be unexpectedly terminated/failed at around the same time the corruptReplicas are invalidated. This is true, but this same risk already exists where:\r\n- corruptReplicas > 1\r\n- liveReplicas = 1\r\nBy invalidating the corruptReplicas we are exposing the block to a risk of loss should the liveReplica be unexpectedly terminated/failed at around the same time the corruptReplicas are invalidated\r\n\r\nThus far, I am unable to think of any scenario where this change introduces an increased risk of data loss. In fact, this change helps improve block redundancy (reducing risk of data loss) in scenarios where a block cannot be sufficiently replicated because corruptReplicas cannot be invalidated.",
        "url": "https://github.com/apache/hadoop/pull/4410#issuecomment-1154627715"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 1191900013,
        "body": "Failed on address in use - is that possible on the CI boxes?",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1980#issuecomment-1191900013"
    },
    {
        "project": "pulsar",
        "id": 1063580105,
        "body": "> @Technoboy- I little suggestion for the future: when you cherry pick a patch from someone else it is a good practice to start by cherry picking the original patch and try to keep the original author, otherwise it seems that you are the original author, in this case it is @mattisonchao\r\n> \r\n> I am looking here https://github.com/apache/pulsar/pull/14618/commits\r\n> \r\n> this is the original commit [4f1e39b](https://github.com/apache/pulsar/commit/4f1e39b6921ea401b8c27f17a041d06d85f8abf8)\r\n> \r\n> it is fine to amend it and adapt it to the target branch, but my point is that in the git history @mattisonchao should be listed as author of the patch (whenever it is possible).\r\n\r\nYes, I see. Thanks.  But there are a lot of conflicts when cherry-picking. So I have to rewrite it according to the original one. We can add the author info when merging.  (I'm not for any commits record here, but for speeding up the release.)",
        "url": "https://github.com/apache/pulsar/pull/14618#issuecomment-1063580105"
    },
    {
        "project": "spring-boot",
        "id": 897723450,
        "body": "> Thanks for the suggestion, but I'm not sure that I understand the benefit of being able to exclude certain file extensions. Can you please expand a bit on what you would use it for and why it would be useful?\r\n\r\nie: `spring.config.exclude-file-extensions=xml,json,yaml,properties`\r\nmeans we skip the check file exist of  these file extensions, cause  the project do not have xml/json/yaml/properties as spring boot config file.\r\n\r\nwith the PR, switch logging (logging.level.org.springframework.boot.context.config=TRACE\uff09\r\nYou will see the different: we do not have these file extensions, so there no need for check whether if the exists. -- reduce unnecessary invoke\r\n",
        "url": "https://github.com/spring-projects/spring-boot/pull/27626#issuecomment-897723450"
    },
    {
        "project": "spring-framework",
        "id": 630989483,
        "body": "Shortening `r2dbc.connectionfactory` to `r2dbc.connection` sounds like a fine option as well. I wonder what to do with the very light `r2dbc.support` package though, we could also try to get rid of that one completely, moving `R2dbcExceptionTranslator` to `r2dbc.connection` (where it fits usage-wise).\r\n\r\nAs for the SQL state exception parsing, I'm rather strongly aiming to avoid this here. Over there in JDBC land, it is an old and hard-coded arrangement of very specific vendor codes, largely outlived by the JDBC 4 exception subclass support for a long time already. Since this is just about throwing more specific exception subclasses, it's not too bad if a particular driver does not distinguish much at all. I do not want our code to make up for a lack of sophistication in the drivers here... again. The appropriate target for a user to request finer-grained exception translation is the driver vendor.\r\n\r\nCollapsing `R2dbcExceptionTranslator` into an interface plus single default implementation might therefore be worthwhile since other benefits of the equivalent JDBC separation - JDBC 4 runtime dependencies (for the `SQLException` subclasses) vs JDBC 2/3 compatibility, or the customizable error code files - don't really show here and are also outlived on the JDBC side already. For R2DBC, we could arguably have a single default implementation checking the exception subclasses and leave the rest up to the drivers. Even further, we could turn it into a translation utility like with our `JmsUtils.convertJmsAccessException` and Hibernate `SessionFactoryUtils.convertHibernateAccessException` methods, with no SPI interface and no configuration options for users, just doing internal exception subclass adaptation.\r\n\r\nOur JDBC support was very maintenance intensive over the years, in particular due to its many vendor-specific checks (also for parameter type determination, value extraction, stored procedure handling). We have a clean plate to start with here, let's keep it as streamlined as possible.",
        "url": "https://github.com/spring-projects/spring-framework/pull/25065#issuecomment-630989483"
    },
    {
        "project": "druid",
        "id": 1183687543,
        "body": ">To me this sounds like the basics of the feature are production-ready. There may be various callouts about performance, but it seems that the compatibility story is tight enough that the feature doesn't need the experimental markings at this time.\r\n\r\nI think that is fair :+1:\r\n\r\n>You mentioned being somewhat less certain about the behavior of nested arrays. We should figure out if that part is going to be included in the production-ready feature set, or if we'll call that particular scenario out as an evolving area. What is your intent & recommendation in this area?\r\n\r\nIt is definitely going to be an evolving area (which I think could be said of our array support in general), though there are probably a narrow range use cases that could be used today, mainly where array lengths and element positions are known and have some meaning and query time operations are primarily extracting and operating on individual elements. This is more or less the current limitations of `flattenSpec` with nested arrays I think.\r\n\r\nThere are some lower hanging fruit that would  improve stuff in the near term, some of which might be possible to get in before the next release. The first  supporting wildcards in the subset of the path syntax that we support, which would allow `JSON_QUERY` and `JSON_VALUE` (or something like it.. i'm not sure entirely how the `RETURNING` syntax would work with array types in SQL so need to do some tinkering there) to extract complete arrays. For `JSON_QUERY` these results would still be `COMPLEX<json>` typed , but `JSON_VALUE`* would spit out druid literal array types (`ARRAY<LONG>`, `ARRAY<STRING>`, etc).\r\n\r\nFor nested arrays of JSON objects extracted by `JSON_QUERY`, i think we will want a way to convert a `COMLEX<json>` into an `ARRAY<COMPLEX<json>>` so that they too can take part in array operations, _especially_ once we add a native `UNNEST` function to transform arrays into tables, which would be the path to exploding out these nested objects and performing operations on their contents.\r\n\r\nAt some point after that, I intend to introduce the option to begin storing literal arrays in nested `ARRAY` typed columns instead of them broken out into separate columns for individual elements like they currently exist (so that array operations don't have to decompress a bunch of separate columns to do stuff). \r\n\r\nI guess I'm getting a bit into the weeds, but my point I guess is that I think this feature will evolve along-side and should help us improve array support in general, so am hyped to get it there.",
        "url": "https://github.com/apache/druid/pull/12753#issuecomment-1183687543"
    },
    {
        "project": "spring-framework",
        "id": 490017215,
        "body": "@rwinch As Stephane suggested, can we decompose this PR a bit? Separating the nohttp tool bit from the doc changes seems worthwhile indeed. Also, it's not entirely clear to me either which example URLs we should really be using now.",
        "url": "https://github.com/spring-projects/spring-framework/pull/22839#issuecomment-490017215"
    },
    {
        "project": "flink",
        "id": 1194861729,
        "body": "@wuchong The CI is passed now. Could you please help merge so that I can move on the other issues.",
        "url": "https://github.com/apache/flink/pull/20247#issuecomment-1194861729"
    },
    {
        "project": "spring-framework",
        "id": 1188728445,
        "body": "The typo fix commit is merged since 2020, the fork is not up to date, so only the RSocket link fix is relevant, so I bring back the original title.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28817#issuecomment-1188728445"
    },
    {
        "project": "pulsar",
        "id": 1201740346,
        "body": "Interesting changes. I went to https://github.com/tisonkun/pulsar/issues/new/choose to test and noticed that the PIP template has a discussion about flaky tests and not a PIP. Once you re-review the descriptions I will look in greater detail.",
        "url": "https://github.com/apache/pulsar/pull/16902#issuecomment-1201740346"
    },
    {
        "project": "spring-boot",
        "id": 1122022925,
        "body": "Thanks for sharing your thoughts, @onobc.\r\n\r\n> and not try to consolidate/reconcile for example w/ the\r\n`spring.kafka.consumer.auto-commit-interval` and `spring.reactor.kafka.receiver.commit-interval`.\r\n\r\nI'm not too comfortable with that. Once Boot surfaces the properties, IDEs will offer auto-completion for them which increases the chances of two similarly named properties causing confusion for users and being difficult to use as a result. If the two properties serve the same purpose, I think they should be consolidated into a single property. If the two properties serve different purposes, I think they should be renamed. In either case, I suspect some changes in Spring Kafka and/or Reactor Kafka will be necessary so that the property or properties map onto similar settings.",
        "url": "https://github.com/spring-projects/spring-boot/pull/30567#issuecomment-1122022925"
    },
    {
        "project": "apm-agent-java",
        "id": 1208383511,
        "body": "\n## :broken_heart: Build Failed\n\n\n<!-- BUILD BADGES-->\n> _the below badges are clickable and redirect to their specific view in the CI or DOCS_\n[![Pipeline View](https://img.shields.io/badge/pipeline-pipeline%20-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2736/detail/PR-2736/2//pipeline) [![Test View](https://img.shields.io/badge/test-test-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2736/detail/PR-2736/2//tests) [![Changes](https://img.shields.io/badge/changes-changes-green)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2736/detail/PR-2736/2//changes) [![Artifacts](https://img.shields.io/badge/artifacts-artifacts-yellow)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2736/detail/PR-2736/2//artifacts) [![preview](https://img.shields.io/badge/docs-preview-yellowgreen)](http://apm-agent-java_2736.docs-preview.app.elstc.co/diff)  [![preview](https://img.shields.io/badge/elastic-observability-blue)](https://ci-stats.elastic.co/app/apm/services/apm-ci/transactions/view?rangeFrom=2022-08-09T08:29:15.508Z&rangeTo=2022-08-09T08:49:15.508Z&transactionName=BUILD+apm-agent-java%2Fapm-agent-java-mbp%2FPR-%7Bnumber%7D&transactionType=job&latencyAggregationType=avg&traceId=1c6ab77ad2a894bf78fee44dfcba7f78&transactionId=7615e89736852bdf)\n\n<!-- BUILD SUMMARY-->\n<details><summary>Expand to view the summary</summary>\n<p>\n\n#### Build stats\n\n\n\n\n* Start Time: 2022-08-09T08:39:15.508+0000\n\n\n* Duration: 6 min 12 sec\n\n\n\n\n\n</p>\n</details>\n\n<!-- TEST RESULTS IF ANY-->\n\n\n<!-- STEPS ERRORS IF ANY -->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### Steps errors [![5](https://img.shields.io/badge/5%20-red)](https://apm-ci.elastic.co/blue/organizations/jenkins/apm-agent-java%2Fapm-agent-java-mbp%2FPR-2736/detail/PR-2736/2//pipeline)\n  <details><summary>Expand to view the steps failures</summary>\n  <p>\n\n\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 1 min 46 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2736/runs/2/steps/135/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 0 min 15 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2736/runs/2/steps/139/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 0 min 15 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2736/runs/2/steps/143/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 0 min 15 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2736/runs/2/steps/147/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  \n  \n  \n##### `mvn install`\n<ul>\n<li>Took 0 min 15 sec . View more details <a href=\"https://apm-ci.elastic.co//blue/rest/organizations/jenkins/pipelines/apm-agent-java/pipelines/apm-agent-java-mbp/pipelines/PR-2736/runs/2/steps/151/log/?start=0\">here</a></li>\n<li>Description: <code>./mvnw clean install -DskipTests=true -Dmaven.javadoc.skip=true</code></l1>\n</ul>\n  \n  </p>\n  </details>\n\n\n\n## :grey_exclamation: Flaky test report\nNo test was executed to be analysed.\n\n\n\n## :robot: GitHub comments\n\nTo re-run your PR in the CI, just comment with:\n\n\n\n- `/test` : Re-trigger the build.\n\n\n- `run benchmark tests` : Run the benchmark tests.\n\n\n- `run jdk compatibility tests` : Run the JDK Compatibility tests.\n\n\n- `run integration tests` : Run the Agent Integration tests.\n\n\n- `run end-to-end tests` : Run the APM-ITs.\n\n\n- `run windows tests` : Build & tests on windows.\n\n\n- `run` `elasticsearch-ci/docs` : Re-trigger the docs validation. (use unformatted text in the comment!)\n\n\n\n\n<!--COMMENT_GENERATED_WITH_ID_comment.id-->",
        "url": "https://github.com/elastic/apm-agent-java/pull/2736#issuecomment-1208383511"
    },
    {
        "project": "spring-security",
        "id": 637766351,
        "body": "Thanks for the PR @bedla. However, a Draft PR has recently been submitted for this #8588. I haven't had time to review it as of yet.\r\n\r\nAlso, I'm seeing 44 modified files in this PR which would make it very difficult and time consuming to review. It's recommended to put forth a Draft PR with minimal changes as a first step so we can get on the same page with proposed changes. I would still be willing to review this PR but the changes need to be reduced significantly so we can proceed with initial draft review.",
        "url": "https://github.com/spring-projects/spring-security/pull/8624#issuecomment-637766351"
    },
    {
        "project": "pulsar",
        "id": 284087328,
        "body": "First commit solved the immediate problem - 072fe7e296474291fa72cd75ec8026bb695a63d2\r\n\r\nSecond commit restructures the code to solve both the immediate problem and the problem @rdhabalia had tried to fix - c1088ac4a2377a535f2503ffee2fd17331d3e1d9",
        "url": "https://github.com/apache/pulsar/pull/275#issuecomment-284087328"
    },
    {
        "project": "hadoop",
        "id": 1207218385,
        "body": "@goiri Thank you very much for helping to review the code, can you help merge to trunk branch? I will follow up with YARN-11227.",
        "url": "https://github.com/apache/hadoop/pull/4695#issuecomment-1207218385"
    },
    {
        "project": "apm-agent-java",
        "id": 1001533729,
        "body": "Let me think about it for a bit and see if and how it is related to the fix we need to do anyway (we shouldn't set an agent CL as described in #2350 ).\r\nMaybe we will end up extending the API as you propose eventually or as an additional workaround. \r\nMy goal is to find a way for it to work without requiring you to specify a CL and definitely without changing the context class loader.",
        "url": "https://github.com/elastic/apm-agent-java/pull/2369#issuecomment-1001533729"
    },
    {
        "project": "druid",
        "id": 190468500,
        "body": "@rpashuto we have some backlog preventing us from reviewing this in detail\n\nDo you know if it is possible to actually restructure this code as a module to Druid? If so, it'll be much faster review process\n",
        "url": "https://github.com/apache/druid/pull/2354#issuecomment-190468500"
    },
    {
        "project": "spring-security",
        "id": 718902631,
        "body": "Hello @rwinch. \r\n\r\nIt's my first PR on Spring Security, so i've to adjust my work to this project test philosophy...\r\nIf i understand your logic, i'd \"duplicate\" 9 of existing test methods (`matches`, `matchesLengthChecked`, `notMatches`, `encodeSamePasswordMultipleTimesDiffers`, `passivity` ...) to test the \"custom salt length\" initial configuration (ie `encoderSalt16` instance) in isolated methods (see below) ? \r\n\r\nMy approach is to group tests by aims like before (ex: `matches` tests whether a raw password matches with the encode result of this raw password, `notMatches` tests a raw password doesn't matches with a different password encode result...) and those unitary test purposes are checked on different initial configurations (encoder instances).\r\nTo be more complete on initial conditions, i'd like to add some other password encoder instances to the `encoders` array : one instance with _no secret_, another with _a custom hash size_ (but, i didn't because it's not related to my current pull request subject. I think i will submit another PR for this).\r\nWith my approach, no need to change (except to complete `zipSatisfy` tests) or add tests methods to tests those other configurations, we simply have to add them to the `encoders` array. This reduce duplicate code, make test purpose clear and same for all configurations. But the counter parts are a little more changes in test history and if a test fails for only one initial configuration, the entire test methods fails. I think this is not a big problem for precise tests that are proof of working for a code change and/or non-regression guards for future changes.\r\n\r\nWhat do you think about this ?\r\n\r\nIf you prefer seperate methods, i plan to add those methods, is that OK ? :\r\n* `matchesWhenCustomSaltLengthThenSuccess`\r\n* `matchesLengthCheckedWhenCustomSaltLengthThenSuccess`\r\n* `notMatchesWhenCustomSaltLengthThenSuccess`\r\n* `encodeSamePasswordMultipleTimesWhenCustomSaltLengthThenDiffers`\r\n* `passivityWhenCustomSaltLengthThenSuccess`\r\n* `encodeAndMatchWhenBase64AndCustomSaltLengthThenSuccess`\r\n* `encodeWhenBase64AndCustomSaltLengthThenSuccess`\r\n* `matchWhenBase64AndCustomSaltLengthThenSuccess`\r\n* `encodeAndMatchWhenSha256AndCustomSaltLengthThenSuccess`\r\n",
        "url": "https://github.com/spring-projects/spring-security/pull/9147#issuecomment-718902631"
    },
    {
        "project": "apm-agent-java",
        "id": 1149218160,
        "body": "@SylvainJuge This is blocking us hard, to the point we're considering diverting to another tracing solution. Is there anything our teams can do to help expedite this improvement? Thanks!",
        "url": "https://github.com/elastic/apm-agent-java/pull/2229#issuecomment-1149218160"
    },
    {
        "project": "apm-agent-java",
        "id": 595098537,
        "body": " > About distributed tracing don't work, i think i forget to change the way to start a transaction in the MessageListener Wrapper , i will fix it and add a test for it.\r\n\r\nExcellent, thanks! Try to use the Kafka module tests as reference as to what we want to test. It will help you cover all sorts of scenarios and make sure your distributed tracing works as expected.\r\n\r\nRegarding the Lambdas - take a look at [the JMS `MessageListener` handling](https://github.com/elastic/apm-agent-java/blob/96b7d60e4edcb39242ab5d7922111d52fd9c1023/apm-agent-plugins/apm-jms-plugin/src/main/java/co/elastic/apm/agent/jms/JmsInstrumentationHelperImpl.java#L134-L143) - it identifies Lambdas and wraps only them when they are registered, thus enabling straightforward instrumentation of the `onMessage` interface method. See if you can do the same. \r\nLater, we may consider reworking things and enable Lambdas instrumentation, in which case we will be able to just cancel this wrapping.",
        "url": "https://github.com/elastic/apm-agent-java/pull/1057#issuecomment-595098537"
    },
    {
        "project": "pulsar",
        "id": 1119355922,
        "body": "> LGTM, great addition @nodece.\r\n> \r\n> Minor question: is `tests/certificate-authority/jks/cert-signed-client.pem` supposed to be an empty file?\r\n\r\n@michaeljmarshall Thanks for your report, it should be not empty.\r\n\r\nGenerate the JKS script has been refactored.",
        "url": "https://github.com/apache/pulsar/pull/15456#issuecomment-1119355922"
    },
    {
        "project": "spring-framework",
        "id": 1150179805,
        "body": "Hmm, my observation that `PathMatchConfigurer::isUseTrailingSlashMatch` deprecation was accidental is based on its usage in `WebMvcConfigurationSupport` and the fact that there's no apparent replacement.\r\n\r\nSo you're saying it was deprecated with the intent to reduce the visibility to package private (and not replacing the method with something else)? In that case my PR is indeed invalid. But if you're reducing the visibility I think you still likely need to drop to deprecation.",
        "url": "https://github.com/spring-projects/spring-framework/pull/28540#issuecomment-1150179805"
    },
    {
        "project": "spring-boot",
        "id": 1161627638,
        "body": "Thanks @StefanBratanov for the changes, the PR looks quite nice now!\r\n\r\n> Hi Moritz, sorry for the delay. I did make a refactor based on the comments, which i agree with. I wasn't sure about handling the `WebClient` async call, but now i think it makes sense and is cleaner. I also changed the `ZipkinRestTemplateSender` async implementation to use a `CompletableFuture` since it was blocking before. I also added tests for both sync and async in both implementations.\r\n\r\nNo need to be sorry :)\r\n\r\nI don't think we need that `CompletableFuture` handling. The `UrlConnectionSender` from zipkin implements the enqueue call without additional threading, so we should be fine here without introducing multi-threading.\r\n\r\n> I am not sure about initializing the `WebClient` in `ZipkinConfigurations`. I was using the `ReactorClientHttpConnector` which is based on` Netty`, so that i can honor the read and connect timeouts set in `ZipkinProperties`. What do you think is a better approach for initializing the `WebClient`? \r\n\r\nI have to double check if there's really no way to set timeouts without tying us directly on the implementation.\r\n\r\n> Also i couldn't find a mock server similar to `MockRestServiceServer` for `WebClient`, so kept my old implementation. If there is one, I am happy to change it.\r\n\r\nYes, I was wrong, there's no such support in the Spring Framework. The recommendation is to use the [OkHttp MockServer](https://docs.spring.io/spring-framework/docs/6.0.0-SNAPSHOT/reference/html/web-reactive.html#webflux-client-testing), like you did.\r\n",
        "url": "https://github.com/spring-projects/spring-boot/pull/30792#issuecomment-1161627638"
    },
    {
        "project": "pulsar",
        "id": 1199001532,
        "body": "This PR has conflict with https://github.com/apache/pulsar/pull/16831. So I will rebase to branch-2.8 after one of them is merged.",
        "url": "https://github.com/apache/pulsar/pull/16867#issuecomment-1199001532"
    },
    {
        "project": "elasticsearch-hadoop",
        "id": 822787502,
        "body": "I noticed a minor issue with the build with local repo vs. not local.  Can you add the following (verify by running ./gradlew clean && ./gradlew clean -PlocalRepo)\r\n\r\n```diff\r\ndiff --git a/settings.gradle b/settings.gradle\r\nindex 259e6e31..e47115ed 100644\r\n--- a/settings.gradle\r\n+++ b/settings.gradle\r\n@@ -1,3 +1,9 @@\r\n+pluginManagement {\r\n+    plugins {\r\n+        id 'com.github.johnrengelman.shadow' version \"6.1.0\"\r\n+    }\r\n+}\r\n+\r\n rootProject.name = \"elasticsearch-hadoop\"\r\n \r\n include 'thirdparty'\r\n@@ -37,3 +43,4 @@ include 'test:fixtures:minikdc'\r\n \r\n include 'qa'\r\n include 'qa:kerberos'\r\n+\r\ndiff --git a/thirdparty/build.gradle b/thirdparty/build.gradle\r\nindex 03e491b2..c0717fd0 100644\r\n--- a/thirdparty/build.gradle\r\n+++ b/thirdparty/build.gradle\r\n@@ -1,7 +1,7 @@\r\n import org.elasticsearch.hadoop.gradle.BuildPlugin\r\n \r\n plugins {\r\n-    id 'com.github.johnrengelman.shadow' version \"6.1.0\"\r\n+    id 'com.github.johnrengelman.shadow'\r\n     id 'es.hadoop.build'\r\n }\r\n```",
        "url": "https://github.com/elastic/elasticsearch-hadoop/pull/1636#issuecomment-822787502"
    },
    {
        "project": "flink",
        "id": 1039411151,
        "body": "Thanks for the second review @zentol. I addressed/responded to your comments. I reorganized the commits and squashed the individual commits together after addressing changes. Additionally, the branch got rebased",
        "url": "https://github.com/apache/flink/pull/18692#issuecomment-1039411151"
    },
    {
        "project": "flink",
        "id": 1067723796,
        "body": "Thanks for all the efforts on this PR @zhoulii, but unfortunately this PR doesn't fully solve the problem and introduces new problems.\r\n\r\nCurrently, the solution discussed in FLINK-26548 encounters the following problem: Setting the parallelism of the actual source directly will cause the parallelism of the multiInput to be configured, which will break the forward property and cause the following error:\r\n`org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[1124]-1669 parallelism: -1, downstream operation: MultipleInput[1149]-1721 parallelism: 1 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global.`\r\n\r\nConsidering that the legacy file sources will be replaced by new source(FLIP-27), and there are some known issues: FLINK-26576, I think we should choose to support new sources only, and add this to limitations of adaptive batch scheduler. BTW, I think we should migrate the `StreamExecutionEnvironment#readFile` `StreamExecutionEnvironment#createInput` to new source as soon as possible.\r\n\r\nWDYT @zhoulii @zhuzhurk ?",
        "url": "https://github.com/apache/flink/pull/19040#issuecomment-1067723796"
    },
    {
        "project": "spring-framework",
        "id": 575222675,
        "body": "Thanks for the pull request. I've updated it a bit, renaming the property from `proxyPing` to `handlePing`.",
        "url": "https://github.com/spring-projects/spring-framework/pull/24367#issuecomment-575222675"
    },
    {
        "project": "hadoop",
        "id": 918179680,
        "body": "Thanks for the review fixes. +1 from my side.\r\n\r\nThe hasEmptyPart could be simplified with the newly added iterator, and there are some checkStyle warnings (longer than 100 lines).",
        "url": "https://github.com/apache/hadoop/pull/3342#issuecomment-918179680"
    },
    {
        "project": "spring-security",
        "id": 936494674,
        "body": "@jzheaux - TL;DR: I don't think anything in this PR should backported as the two changes about ID and listing style are only a refactoring that produces the same output as before: \r\n\r\n1. Asciidoctor understands both the AsciiDoc and Markdown syntax for listings, and it generates the same output. \r\n2. the section IDs work with implicit section IDs in 5.5.x (and they would work with Antora as well). Again, it is just a refactoring towards AsciiDoc best practices, as it guards you from broken references when headings change. It generates the output as before.\r\n\r\nStill, if you insist, I'm happy to split this PR, or (preferred) submit another PR for 5.5.x with only the ID and listing changes. \r\n\r\n",
        "url": "https://github.com/spring-projects/spring-security/pull/10345#issuecomment-936494674"
    },
    {
        "project": "spring-framework",
        "id": 654231030,
        "body": "> I think I'll just rename `SystemArchitecture` to `SharedPointcuts`, `CommonPointcuts`, or something similar.\r\n> \r\n> What do you think about that?\r\n\r\nI think that is a particularly good idea, much better than mine to rename the pointcut methods themselves. `SharedPointcuts` would reflect the wording in the manual. Actually, the corresponding sub-chapter's headline is even \"shared common pointcut definitions\" which sounds kind of redundant, so maybe better not `SharedCommonPointcuts`. Even `SystemArchitectureSharedPointcuts` would be acceptable, covering both the domain language and the AOP concept. But maybe that would be kind of too verbose. Either way, the intent will be conveyed well with both of your suggestions.\r\n\r\nThank you very much for the swift and constructive reaction. \ud83d\ude00",
        "url": "https://github.com/spring-projects/spring-framework/pull/25357#issuecomment-654231030"
    },
    {
        "project": "spring-security",
        "id": 942567555,
        "body": "> @jzheaux Do you want the same change on main (with 5.6 instead of 5.5) or would you do it on your own inside your team?\r\n\r\nOK, I see that you've done some changes (split & getLast -> Pattern.match) so I'll assume that you'll merge this into main on you own :-)",
        "url": "https://github.com/spring-projects/spring-security/pull/10348#issuecomment-942567555"
    },
    {
        "project": "spring-boot",
        "id": 360600978,
        "body": "@rajadilipkolli @philwebb thank you for attention to this issues! But this is false positive static code analysis warning.  Possible resource leak already fixed in [PR 11624](https://github.com/spring-projects/spring-boot/pull/11624) \r\n\r\nNeed to redesign this classes to avoid this sonarcloud warnings.\r\n\r\nLog4J2LoggingSystem.java is not applicable - stream should be in open state after return of ConfigurationSource \r\n\r\nJarWriter.java - output stream created in construnctor and used in another object methods\r\n\r\nSocketTargetServerConnection.java - network channel should be in open state after method return",
        "url": "https://github.com/spring-projects/spring-boot/pull/11779#issuecomment-360600978"
    },
    {
        "project": "flink",
        "id": 1066696748,
        "body": "@zentol, I analyzed the problem. `OperationTimeoutException` definitely happens at the client side (Cassandra driver) under load in all the tests but the timeouts raised seem to be not enough. So I reduced the load by avoiding \"IF EXIST\" in CQL requests and more important I replaced `drop keyspace` by `drop table`. Indeed, each `Before` updates the tableID and it is called even in case or retrials. So, we need to drop only the created table for this test (with the same tableID) and not the whole keyspace. That will reduce the overall load on  cassandra (so I hope no more timeouts) but also fix the `keyspace does not exist` flaky issue.\r\n\r\nFinally I put `testRetrialDropTable` back to check that everything still works in case of retrial (drop the single table instead of keyspace etc...)\r\n\r\nI did 10 local runs of the whole CassandraITCase with no error.",
        "url": "https://github.com/apache/flink/pull/19059#issuecomment-1066696748"
    },
    {
        "project": "spring-security",
        "id": 487240941,
        "body": "Alternate approach to **avoid double parsing** is [NimbusJwtMultiTenantDecoder.java](https://github.com/gburboz/spring-security/blob/feature/6779-rs-multitenant-v2/oauth2/oauth2-jose/src/main/java/org/springframework/security/oauth2/jwt/NimbusJwtMultiTenantDecoder.java) with refactored [NimbusJwtDecoder.java](https://github.com/gburboz/spring-security/blob/feature/6779-rs-multitenant-v2/oauth2/oauth2-jose/src/main/java/org/springframework/security/oauth2/jwt/NimbusJwtDecoder.java)\r\n\r\nFurthermore we should be able to specify validator and converter beans for respective decoder in config (either by name or type). \r\n\r\n```yaml\r\nspring.security.oauth2.resourceserver:\r\n  multi-tenant-jwt:\r\n    -\r\n      issuer-uri: \"https://mockwebserver.com/\"\r\n      jwk-set-uri: ${mockwebserver.url}/.well-known/jwks.json\r\n      converter-bean-name: \"mockwebserverJwtConverter\"\r\n      validator-bean-name: \"mockwebserverJwtValidator\"\r\n    -\r\n      issuer-uri: \"https://some-domain.com/\"\r\n      public-key: \"${some-domain.public-key}\"\r\n      converter-bean-name: \"someDomainJwtConverter\"\r\n      validator-bean-name: \"someDomainJwtValidator\"\r\n\r\n```",
        "url": "https://github.com/spring-projects/spring-security/pull/6817#issuecomment-487240941"
    },
    {
        "project": "flink",
        "id": 1178478120,
        "body": "This PR includes UI changes regarding the [Completed Jobs Information Enhancement FLIP](https://cwiki.apache.org/confluence/display/FLINK/FLIP-241%3A+Completed+Jobs+Information+Enhancement). Could @simplejason help review this? Thanks!",
        "url": "https://github.com/apache/flink/pull/20209#issuecomment-1178478120"
    },
    {
        "project": "druid",
        "id": 1182687157,
        "body": "Hi @ektravel , Could you paste the key information of the original issue in this PR? I think people out of Imply are not able to see that issue by your link.",
        "url": "https://github.com/apache/druid/pull/12772#issuecomment-1182687157"
    },
    {
        "project": "hadoop",
        "id": 1196683343,
        "body": "> UserGroupInformation",
        "url": "https://github.com/apache/hadoop/pull/4625#issuecomment-1196683343"
    },
    {
        "project": "pulsar",
        "id": 1155196275,
        "body": "> Is this change ABI compatible so that existing user created Pulsar Functions implementations can be run without recompiling when upgrading to a version that includes this change?\r\n\r\n@lhotari I think it is since we only add a method to the `Context` API. \r\n`SinkRecord` is sometimes used outside of the package (and probably shouldn't) in tests but I think extracting some methods to a super class is OK for binary compat. Or is it not ?\r\nDo you see other things that could break ?",
        "url": "https://github.com/apache/pulsar/pull/16041#issuecomment-1155196275"
    }
]